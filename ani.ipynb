{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c820b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ Installation de {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e09e163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopher/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ CONFIGURATION ULTRA-OPTIMIS√âE ACTIV√âE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "packages = ['psutil', 'matplotlib', 'seaborn', 'pandas', 'numpy']\n",
    "for pkg in packages:\n",
    "    install_if_missing(pkg)\n",
    "\n",
    "import tensorflow as tf\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üöÄ CONFIGURATION ULTRA-OPTIMIS√âE ACTIV√âE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a6b23dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_count = multiprocessing.cpu_count()\n",
    "total_memory = psutil.virtual_memory().total / (1024**3)\n",
    "available_memory = psutil.virtual_memory().available / (1024**3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8722db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª RESSOURCES SYST√àME:\n",
      "   CPU: 14 threads\n",
      "   RAM totale: 36.0 GB\n",
      "   RAM disponible: 14.9 GB\n",
      "\n",
      "‚ö° OPTIMISATIONS TENSORFLOW:\n"
     ]
    }
   ],
   "source": [
    "print(f\"üíª RESSOURCES SYST√àME:\")\n",
    "print(f\"   CPU: {cpu_count} threads\")\n",
    "print(f\"   RAM totale: {total_memory:.1f} GB\")\n",
    "print(f\"   RAM disponible: {available_memory:.1f} GB\")\n",
    "\n",
    "# Configuration TensorFlow ultra-optimis√©e\n",
    "print(f\"\\n‚ö° OPTIMISATIONS TENSORFLOW:\")\n",
    "\n",
    "# Configuration threads pour utilisation maximale\n",
    "tf.config.threading.set_intra_op_parallelism_threads(cpu_count)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(cpu_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "169840b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Variables d'environnement optimales\n",
    "os.environ['OMP_NUM_THREADS'] = str(cpu_count)\n",
    "os.environ['TF_NUM_INTEROP_THREADS'] = str(cpu_count)\n",
    "os.environ['TF_NUM_INTRAOP_THREADS'] = str(cpu_count)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4041386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üíª Mode CPU optimis√©\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"   üéÆ GPU d√©tect√©s: {len(gpus)}\")\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"   ‚úì Croissance m√©moire GPU activ√©e\")\n",
    "    except:\n",
    "        print(f\"   ‚ö†Ô∏è Configuration GPU partielle\")\n",
    "else:\n",
    "    print(f\"   üíª Mode CPU optimis√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af355180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì XLA JIT activ√©\n",
      "   ‚úì Mixed Precision FP16 activ√©e\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    print(f\"   ‚úì XLA JIT activ√©\")\n",
    "except:\n",
    "    print(f\"   ‚ö†Ô∏è XLA non disponible\")\n",
    "\n",
    "try:\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(f\"   ‚úì Mixed Precision FP16 activ√©e\")\n",
    "except:\n",
    "    print(f\"   ‚ö†Ô∏è Mixed Precision non support√©e\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "396eb539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Dataset AUTOTUNE configur√©\n",
      "\n",
      "‚úÖ CONFIGURATION TERMIN√âE\n",
      "ÔøΩ Utilisation pr√©vue: CPU 14 threads, RAM ~11GB\n",
      "üöÄ Syst√®me optimis√© pour performances maximales!\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "print(f\"   ‚úì Dataset AUTOTUNE configur√©\")\n",
    "\n",
    "print(f\"\\n‚úÖ CONFIGURATION TERMIN√âE\")\n",
    "print(f\"ÔøΩ Utilisation pr√©vue: CPU {cpu_count} threads, RAM ~{int(available_memory*0.8)}GB\")\n",
    "print(\"üöÄ Syst√®me optimis√© pour performances maximales!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70576440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des packages n√©cessaires avec optimisations\n",
    "!pip install tensorflow tensorflow-addons scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d3edfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå∏ Cr√©ation du mod√®le Shirayuki ultra-optimis√©...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Embedding, MultiHeadAttention, Dropout, LayerNormalization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "print(\"üå∏ Cr√©ation du mod√®le Shirayuki ultra-optimis√©...\")\n",
    "\n",
    "class SimpleTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim//num_heads, dropout=rate)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"gelu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        attn_output = self.att(x, x, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class ShirayukiTransformer(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=8, ff_dim=512, maxlen=128, num_layers=4, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, embed_dim, mask_zero=True)\n",
    "        self.pos_embedding = Embedding(maxlen, embed_dim)\n",
    "\n",
    "        self.encoder_layers = [SimpleTransformerBlock(embed_dim, num_heads, ff_dim, rate)\n",
    "                              for _ in range(num_layers)]\n",
    "        self.decoder_layers = [SimpleTransformerBlock(embed_dim, num_heads, ff_dim, rate)\n",
    "                              for _ in range(num_layers)]\n",
    "\n",
    "        self.final_layer = Dense(vocab_size, dtype='float32')\n",
    "        self.dropout = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        if isinstance(inputs, tuple):\n",
    "            input_ids, target_ids = inputs\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "            target_ids = None\n",
    "\n",
    "        # Encoder\n",
    "        encoder_output = self.encode(input_ids, training)\n",
    "\n",
    "        if target_ids is not None:\n",
    "            # Decoder avec teacher forcing\n",
    "            decoder_output = self.decode(target_ids, encoder_output, training)\n",
    "            return self.final_layer(decoder_output)\n",
    "        else:\n",
    "            return encoder_output\n",
    "\n",
    "    def encode(self, input_ids, training=False):\n",
    "        seq_len = tf.shape(input_ids)[1]\n",
    "        x = self.embedding(input_ids)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "\n",
    "        positions = tf.range(seq_len)[None, :]\n",
    "        x += self.pos_embedding(positions)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "\n",
    "    def decode(self, target_ids, encoder_output, training=False):\n",
    "        seq_len = tf.shape(target_ids)[1]\n",
    "        x = self.embedding(target_ids)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "\n",
    "        positions = tf.range(seq_len)[None, :]\n",
    "        x += self.pos_embedding(positions)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37aea19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shirayuki_data(file_path):\n",
    "    print(f\"üìä Chargement des donn√©es...\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        inputs = df['guy'].astype(str).tolist()\n",
    "        outputs = df['girl'].astype(str).tolist()\n",
    "        print(f\"‚úÖ Fichier CSV charg√©: {len(inputs)} conversations\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Fichier CSV non trouv√©, cr√©ation d'un dataset de d√©monstration...\")\n",
    "\n",
    "    # Nettoyage simple\n",
    "    clean_pairs = []\n",
    "    for inp, out in zip(inputs, outputs):\n",
    "        if inp and out and len(inp.strip()) > 0 and len(out.strip()) > 0:\n",
    "            clean_pairs.append((inp.strip(), out.strip()))\n",
    "\n",
    "    print(f\"üìä Conversations valides: {len(clean_pairs)}\")\n",
    "    return clean_pairs\n",
    "\n",
    "# Cr√©ation du tokenizer simplifi√©\n",
    "def create_simple_tokenizer(conversations, vocab_size=8192, max_length=64):\n",
    "    print(\"üîß Cr√©ation du tokenizer...\")\n",
    "\n",
    "    from tensorflow.keras.utils import text_dataset_from_directory\n",
    "    from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "    # Extraction des textes\n",
    "    all_texts = []\n",
    "    for inp, out in conversations:\n",
    "        all_texts.append(inp)\n",
    "        all_texts.append(\"[START] \" + out + \" [END]\")\n",
    "\n",
    "    # Tokenizer optimis√©\n",
    "    tokenizer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_sequence_length=max_length,\n",
    "        standardize='lower_and_strip_punctuation',\n",
    "        split='whitespace'\n",
    "    )\n",
    "\n",
    "    tokenizer.adapt(all_texts)\n",
    "\n",
    "    # Pr√©paration des donn√©es\n",
    "    inputs = [pair[0] for pair in conversations]\n",
    "    outputs = [\"[START] \" + pair[1] + \" [END]\" for pair in conversations]\n",
    "\n",
    "    input_ids = tokenizer(inputs)\n",
    "    output_ids = tokenizer(outputs)\n",
    "\n",
    "    # Teacher forcing\n",
    "    decoder_input = output_ids[:, :-1]\n",
    "    decoder_target = output_ids[:, 1:]\n",
    "\n",
    "    print(f\"‚úÖ Tokenizer cr√©√©: {tokenizer.vocabulary_size()} tokens, longueur {max_length}\")\n",
    "    return tokenizer, input_ids, decoder_input, decoder_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49dfe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configuration du mod√®le...\n",
      "üìä Param√®tres:\n",
      "   Vocab: 8192 tokens\n",
      "   Longueur max: 64\n",
      "   Dimensions: 256\n",
      "   Couches: 4\n",
      "   Batch size: 32\n",
      "üìä Chargement des donn√©es...\n",
      "‚úÖ Fichier CSV charg√©: 4363 conversations\n",
      "üìä Conversations valides: 4362\n",
      "üîß Cr√©ation du tokenizer...\n",
      "‚úÖ Tokenizer cr√©√©: 6038 tokens, longueur 64\n"
     ]
    }
   ],
   "source": [
    "# Configuration optimale\n",
    "print(\"‚öôÔ∏è Configuration du mod√®le...\")\n",
    "vocab_size = 8192\n",
    "max_length = 64\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "ff_dim = 512\n",
    "num_layers = 4\n",
    "batch_size = min(32, max(8, int(available_memory * 4)))\n",
    "\n",
    "\n",
    "print(f\"üìä Param√®tres:\")\n",
    "print(f\"   Vocab: {vocab_size} tokens\")\n",
    "print(f\"   Longueur max: {max_length}\")\n",
    "print(f\"   Dimensions: {embed_dim}\")\n",
    "print(f\"   Couches: {num_layers}\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "\n",
    "conversations = load_shirayuki_data('/Users/christopher/Documents/IA/ani/conversation_dataset_ShirayukiV3.csv')\n",
    "\n",
    "tokenizer, input_ids, decoder_input, decoder_target = create_simple_tokenizer(\n",
    "    conversations, vocab_size, max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f035f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Cr√©ation du dataset...\n",
      "üå∏ Cr√©ation du mod√®le Shirayuki...\n",
      "üß™ Test du mod√®le...\n",
      "   Taille du batch de test: (32, 64)\n",
      "   Entr√©es: [[  35   32    2 ...    0    0    0]\n",
      " [ 235   85  191 ...    0    0    0]\n",
      " [2500    0    0 ...    0    0    0]\n",
      " ...\n",
      " [ 596  123    0 ...    0    0    0]\n",
      " [   5  142   64 ...    0    0    0]\n",
      " [  38   10    6 ...    0    0    0]]\n",
      "   Cibles: [[1114  797   68 ...    0    0    0]\n",
      " [  33   76   21 ...    0    0    0]\n",
      " [  32    2   13 ...    0    0    0]\n",
      " ...\n",
      " [  19    8  837 ...    0    0    0]\n",
      " [3700    8   17 ...    0    0    0]\n",
      " [  82  448   33 ...    0    0    0]]\n",
      "‚ùå Erreur de test: Exception encountered when calling ShirayukiTransformer.call().\n",
      "\n",
      "\u001b[1mcannot compute Mul as input #1(zero-based) was expected to be a half tensor but is a float tensor [Op:Mul] name: \u001b[0m\n",
      "\n",
      "Arguments received by ShirayukiTransformer.call():\n",
      "  ‚Ä¢ inputs=('tf.Tensor(shape=(32, 64), dtype=int64)', 'tf.Tensor(shape=(32, 63), dtype=int64)')\n",
      "  ‚Ä¢ training=False\n",
      "\n",
      "üéâ MOD√àLE SHIRAYUKI PR√äT!\n",
      "üöÄ Ex√©cutez la cellule suivante pour l'entra√Ænement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopher/Library/Python/3.9/lib/python/site-packages/keras/src/layers/layer.py:1474: UserWarning: Layer 'shirayuki_transformer_2' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.''\n",
      "  warnings.warn(\n",
      "/Users/christopher/Library/Python/3.9/lib/python/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'shirayuki_transformer_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# üöÄ MOD√àLE SHIRAYUKI ULTRA-SIMPLIFI√â ET ROBUSTE (CORRIG√â)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Embedding, MultiHeadAttention, Dropout, LayerNormalization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "print(\"üå∏ Cr√©ation du mod√®le Shirayuki ultra-optimis√©...\")\n",
    "\n",
    "# D√©sactiver mixed precision pour √©viter les conflits\n",
    "tf.keras.mixed_precision.set_global_policy('float32')\n",
    "\n",
    "# Classes optimis√©es simplifi√©es avec types coh√©rents\n",
    "class SimpleTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim//num_heads, dropout=rate)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"gelu\", dtype='float32'),\n",
    "            Dense(embed_dim, dtype='float32'),\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6, dtype='float32')\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6, dtype='float32')\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        attn_output = self.att(x, x, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class ShirayukiTransformer(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=8, ff_dim=512, maxlen=128, num_layers=4, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, embed_dim, mask_zero=True, dtype='float32')\n",
    "        self.pos_embedding = Embedding(maxlen, embed_dim, dtype='float32')\n",
    "\n",
    "        self.encoder_layers = [SimpleTransformerBlock(embed_dim, num_heads, ff_dim, rate)\n",
    "                              for _ in range(num_layers)]\n",
    "        self.decoder_layers = [SimpleTransformerBlock(embed_dim, num_heads, ff_dim, rate)\n",
    "                              for _ in range(num_layers)]\n",
    "\n",
    "        self.final_layer = Dense(vocab_size, dtype='float32')\n",
    "        self.dropout = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        if isinstance(inputs, tuple):\n",
    "            input_ids, target_ids = inputs\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "            target_ids = None\n",
    "\n",
    "        # Encoder\n",
    "        encoder_output = self.encode(input_ids, training)\n",
    "\n",
    "        if target_ids is not None:\n",
    "            # Decoder avec teacher forcing\n",
    "            decoder_output = self.decode(target_ids, encoder_output, training)\n",
    "            return self.final_layer(decoder_output)\n",
    "        else:\n",
    "            return encoder_output\n",
    "\n",
    "    def encode(self, input_ids, training=False):\n",
    "        seq_len = tf.shape(input_ids)[1]\n",
    "        x = self.embedding(input_ids)\n",
    "        x = tf.cast(x, tf.float32)  # Force float32\n",
    "        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "\n",
    "        positions = tf.range(seq_len)[None, :]\n",
    "        pos_emb = self.pos_embedding(positions)\n",
    "        pos_emb = tf.cast(pos_emb, tf.float32)  # Force float32\n",
    "        x += pos_emb\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "\n",
    "    def decode(self, target_ids, encoder_output, training=False):\n",
    "        seq_len = tf.shape(target_ids)[1]\n",
    "        x = self.embedding(target_ids)\n",
    "        x = tf.cast(x, tf.float32)  # Force float32\n",
    "        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "\n",
    "        positions = tf.range(seq_len)[None, :]\n",
    "        pos_emb = self.pos_embedding(positions)\n",
    "        pos_emb = tf.cast(pos_emb, tf.float32)  # Force float32\n",
    "        x += pos_emb\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "\n",
    "# Fonction de chargement de donn√©es robuste\n",
    "def load_shirayuki_data(file_path):\n",
    "    print(f\"üìä Chargement des donn√©es...\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        inputs = df['guy'].astype(str).tolist()\n",
    "        outputs = df['girl'].astype(str).tolist()\n",
    "        print(f\"‚úÖ Fichier CSV charg√©: {len(inputs)} conversations\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Fichier CSV non trouv√©, cr√©ation d'un dataset de d√©monstration...\")\n",
    "        # Dataset de demo tsundere\n",
    "        demo_conversations = [\n",
    "            (\"Bonjour Shirayuki\", \"H-H√© ! Ne me parle pas si soudainement ! *rougit*\"),\n",
    "            (\"Comment √ßa va ?\", \"√áa va bien... pas que √ßa t'int√©resse ! Hmph !\"),\n",
    "            (\"Tu es mignonne\", \"Q-Quoi ?! Ne dis pas des choses comme √ßa ! *devient rouge*\"),\n",
    "            (\"Je t'aime\", \"C-Ce n'est pas comme si... si j'√©tais contente ! Baka !\"),\n",
    "            (\"Tu veux sortir ?\", \"P-Peut-√™tre... si tu insistes vraiment...\"),\n",
    "            (\"Bonne nuit\", \"Bonne nuit... et ne r√™ve pas de moi ! *d√©tourne le regard*\"),\n",
    "            (\"Tu me manques\", \"Tu... tu me manques aussi... mais juste un peu !\"),\n",
    "            (\"Merci\", \"C-C'est normal ! Ne me remercie pas ! *embarrass√©e*\"),\n",
    "            (\"Tu es belle\", \"Arr√™te de dire n'importe quoi ! Mais... merci...\"),\n",
    "            (\"Veux-tu √™tre mon amie ?\", \"On... on est d√©j√† amies ! Idiot ! *sourit secr√®tement*\")\n",
    "        ] * 20  # 200 exemples\n",
    "\n",
    "        inputs = [conv[0] for conv in demo_conversations]\n",
    "        outputs = [conv[1] for conv in demo_conversations]\n",
    "        print(f\"‚úÖ Dataset de d√©monstration cr√©√©: {len(inputs)} conversations\")\n",
    "\n",
    "    # Nettoyage simple\n",
    "    clean_pairs = []\n",
    "    for inp, out in zip(inputs, outputs):\n",
    "        if inp and out and len(inp.strip()) > 0 and len(out.strip()) > 0:\n",
    "            clean_pairs.append((inp.strip(), out.strip()))\n",
    "\n",
    "    print(f\"üìä Conversations valides: {len(clean_pairs)}\")\n",
    "    return clean_pairs\n",
    "\n",
    "# Cr√©ation du tokenizer simplifi√©\n",
    "def create_simple_tokenizer(conversations, vocab_size=8192, max_length=64):\n",
    "    print(\"\udd27 Cr√©ation du tokenizer...\")\n",
    "\n",
    "    from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "    # Extraction des textes\n",
    "    all_texts = []\n",
    "    for inp, out in conversations:\n",
    "        all_texts.append(inp)\n",
    "        all_texts.append(\"[START] \" + out + \" [END]\")\n",
    "\n",
    "    # Tokenizer optimis√©\n",
    "    tokenizer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_sequence_length=max_length,\n",
    "        standardize='lower_and_strip_punctuation',\n",
    "        split='whitespace'\n",
    "    )\n",
    "\n",
    "    tokenizer.adapt(all_texts)\n",
    "\n",
    "    # Pr√©paration des donn√©es\n",
    "    inputs = [pair[0] for pair in conversations]\n",
    "    outputs = [\"[START] \" + pair[1] + \" [END]\" for pair in conversations]\n",
    "\n",
    "    input_ids = tokenizer(inputs)\n",
    "    output_ids = tokenizer(outputs)\n",
    "\n",
    "    # Teacher forcing\n",
    "    decoder_input = output_ids[:, :-1]\n",
    "    decoder_target = output_ids[:, 1:]\n",
    "\n",
    "    print(f\"‚úÖ Tokenizer cr√©√©: {tokenizer.vocabulary_size()} tokens, longueur {max_length}\")\n",
    "    return tokenizer, input_ids, decoder_input, decoder_target\n",
    "\n",
    "# Configuration optimale\n",
    "print(\"‚öôÔ∏è Configuration du mod√®le...\")\n",
    "vocab_size = 8192\n",
    "max_length = 64\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "ff_dim = 512\n",
    "num_layers = 4\n",
    "batch_size = min(32, max(8, int(available_memory * 4)))\n",
    "\n",
    "print(f\"üìä Param√®tres:\")\n",
    "print(f\"   Vocab: {vocab_size} tokens\")\n",
    "print(f\"   Longueur max: {max_length}\")\n",
    "print(f\"   Dimensions: {embed_dim}\")\n",
    "print(f\"   Couches: {num_layers}\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "\n",
    "# Chargement des donn√©es\n",
    "conversations = load_shirayuki_data('/Users/christopher/Documents/IA/ani/conversation_dataset_ShirayukiV3.csv')\n",
    "conversation_pairs = conversations  # Variable pour compatibilit√©\n",
    "\n",
    "# Cr√©ation du tokenizer et des donn√©es\n",
    "tokenizer, input_ids, decoder_input, decoder_target = create_simple_tokenizer(\n",
    "    conversations, vocab_size, max_length\n",
    ")\n",
    "\n",
    "# Cr√©ation du dataset\n",
    "print(\"\ud83düì¶ Cr√©ation du dataset...\")\n",
    "dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    'encoder_input': input_ids,\n",
    "    'decoder_input': decoder_input,\n",
    "    'decoder_target': decoder_target\n",
    "})\n",
    "\n",
    "def prepare_batch(batch):\n",
    "    return ((batch['encoder_input'], batch['decoder_input']), batch['decoder_target'])\n",
    "\n",
    "dataset = (dataset\n",
    "    .map(prepare_batch, num_parallel_calls=AUTOTUNE)\n",
    "    .shuffle(1000)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(AUTOTUNE))\n",
    "\n",
    "# Cr√©ation du mod√®le\n",
    "print(\"üå∏ Cr√©ation du mod√®le Shirayuki...\")\n",
    "model = ShirayukiTransformer(\n",
    "    vocab_size=tokenizer.vocabulary_size(),\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ff_dim=ff_dim,\n",
    "    maxlen=max_length,\n",
    "    num_layers=num_layers\n",
    ")\n",
    "\n",
    "# Compilation optimis√©e\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Test du mod√®le\n",
    "print(\"üß™ Test du mod√®le...\")\n",
    "try:\n",
    "    test_batch = next(iter(dataset.take(1)))\n",
    "    print(f\"   Taille du batch de test: {test_batch[0][0].shape}\")\n",
    "    output = model(test_batch[0])\n",
    "    print(f\"‚úÖ Test r√©ussi! Shape de sortie: {output.shape}\")\n",
    "    print(f\"üìä Param√®tres du mod√®le: {model.count_params():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur de test: {e}\")\n",
    "\n",
    "print(\"\\nüéâ MOD√àLE SHIRAYUKI PR√äT!\")\n",
    "print(\"üöÄ Ex√©cutez la cellule suivante pour l'entra√Ænement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcba87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• D√©marrage de l'entra√Ænement avec utilisation maximale des ressources!\n",
      "======================================================================\n",
      "üìä Configuration:\n",
      "   Epochs: 15\n",
      "   Batch size: 32\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'conversation_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Batch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Steps par epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(conversation_pairs)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   CPU threads: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcpu_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   M√©moire utilis√©e: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(available_memory\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0.8\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conversation_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "# \udd25 ENTRA√éNEMENT SHIRAYUKI ULTRA-OPTIMIS√â (CORRIG√â)\n",
    "print(\"üî• D√©marrage de l'entra√Ænement avec utilisation maximale des ressources!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configuration d'entra√Ænement\n",
    "epochs = 15\n",
    "steps_per_epoch = len(conversation_pairs) // batch_size\n",
    "\n",
    "print(f\"üìä Configuration:\")\n",
    "print(f\"   Epochs: {epochs}\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Steps par epoch: {steps_per_epoch}\")\n",
    "print(f\"   CPU threads: {cpu_count}\")\n",
    "print(f\"   M√©moire utilis√©e: {int(available_memory * 0.8)} GB\")\n",
    "print(f\"   Dataset: {len(conversation_pairs)} conversations\")\n",
    "\n",
    "# Callbacks optimis√©s\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.LambdaCallback(\n",
    "        on_epoch_end=lambda epoch, logs: print(f\"üå∏ Epoch {epoch+1}/{epochs} - Loss: {logs['loss']:.4f} - Accuracy: {logs['accuracy']:.4f}\")\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\nüöÄ Lancement de l'entra√Ænement...\")\n",
    "print(\"üí° Utilisation de teacher forcing pour un apprentissage optimal\")\n",
    "\n",
    "try:\n",
    "    # Entra√Ænement avec gestion d'erreurs\n",
    "    history = model.fit(\n",
    "        dataset,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\nüéâ ENTRA√éNEMENT TERMIN√â!\")\n",
    "    print(f\"üìà Loss finale: {history.history['loss'][-1]:.4f}\")\n",
    "    print(f\"üìà Accuracy finale: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "    # Test de g√©n√©ration simple\n",
    "    print(\"\\nüß™ Test de g√©n√©ration:\")\n",
    "    test_input = \"Bonjour Shirayuki\"\n",
    "    test_tokens = tokenizer([test_input])\n",
    "    print(f\"Input: {test_input}\")\n",
    "    print(\"Shirayuki va r√©pondre...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur d'entra√Ænement: {e}\")\n",
    "    print(\"\udca1 Tentative avec param√®tres r√©duits...\")\n",
    "\n",
    "    # Fallback avec param√®tres r√©duits\n",
    "    try:\n",
    "        smaller_dataset = dataset.take(min(100, steps_per_epoch))\n",
    "        history = model.fit(\n",
    "            smaller_dataset,\n",
    "            epochs=min(5, epochs),\n",
    "            verbose=1\n",
    "        )\n",
    "        print(\"‚úÖ Entra√Ænement de secours r√©ussi!\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Erreur critique: {e2}\")\n",
    "\n",
    "print(\"\\nüå∏ Mod√®le Shirayuki pr√™t pour la conversation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c92db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTRA√éNEMENT ULTRA-OPTIMIS√â AVEC MONITORING TEMPS R√âEL\n",
    "print(\"üöÄ Configuration avanc√©e pour utilisation maximale des ressources\")\n",
    "\n",
    "# G√©n√©rateur Shirayuki ultra-optimis√©\n",
    "class UltraShirayukiGenerator:\n",
    "    \"\"\"G√©n√©rateur ultra-optimis√© pour conversations tsundere avec monitoring\"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, max_length=128):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.generation_cache = {}  # Cache pour optimiser les g√©n√©rations r√©p√©t√©es\n",
    "\n",
    "    @tf.function(jit_compile=True)\n",
    "    def _generate_step(self, input_ids, temperature, top_k, top_p):\n",
    "        \"\"\"√âtape de g√©n√©ration compil√©e avec XLA\"\"\"\n",
    "        encoder_output = self.model.encode(input_ids, training=False)\n",
    "\n",
    "        # D√©but de s√©quence\n",
    "        start_token = self.tokenizer(['[START]'])[0, 0:1]\n",
    "        decoder_input = start_token\n",
    "\n",
    "        generated_ids = []\n",
    "\n",
    "        for _ in tf.range(self.max_length - 1):\n",
    "            # Pr√©diction du token suivant\n",
    "            logits = self.model.decode(decoder_input, encoder_output, training=False)\n",
    "            logits = self.model.output_layer(logits)\n",
    "\n",
    "            # Sampling optimis√©\n",
    "            next_token = self._sample_token(logits[:, -1, :], temperature, top_k, top_p)\n",
    "\n",
    "            # Ajout du token g√©n√©r√©\n",
    "            generated_ids.append(next_token)\n",
    "            decoder_input = tf.concat([decoder_input, next_token], axis=1)\n",
    "\n",
    "            # Arr√™t si token de fin\n",
    "            if tf.reduce_any(tf.equal(next_token, self.tokenizer(['[END]'])[0, 0])):\n",
    "                break\n",
    "\n",
    "        return tf.concat(generated_ids, axis=1)\n",
    "\n",
    "    @tf.function(jit_compile=True)\n",
    "    def _sample_token(self, logits, temperature, top_k, top_p):\n",
    "        \"\"\"Sampling optimis√© avec nucleus sampling\"\"\"\n",
    "        logits = logits / temperature\n",
    "\n",
    "        # Top-k filtering\n",
    "        if top_k > 0:\n",
    "            top_k_logits, top_k_indices = tf.nn.top_k(logits, k=top_k)\n",
    "            logits = tf.where(\n",
    "                tf.reduce_any(tf.equal(tf.expand_dims(tf.range(tf.shape(logits)[-1]), 0),\n",
    "                                      tf.expand_dims(top_k_indices, -1)), axis=1),\n",
    "                logits,\n",
    "                tf.fill(tf.shape(logits), -1e9)\n",
    "            )\n",
    "\n",
    "        # Top-p (nucleus) filtering\n",
    "        if top_p < 1.0:\n",
    "            sorted_logits = tf.sort(logits, direction='DESCENDING')\n",
    "            sorted_probs = tf.nn.softmax(sorted_logits)\n",
    "            cumulative_probs = tf.cumsum(sorted_probs, axis=-1)\n",
    "\n",
    "            # Masque pour les tokens √† garder\n",
    "            keep_mask = cumulative_probs <= top_p\n",
    "            keep_mask = tf.concat([tf.ones_like(keep_mask[:, :1]), keep_mask[:, :-1]], axis=-1)\n",
    "\n",
    "            # Application du masque\n",
    "            filtered_logits = tf.where(keep_mask, sorted_logits, -1e9)\n",
    "            logits = tf.gather(filtered_logits, tf.argsort(tf.argsort(logits, direction='DESCENDING')),\n",
    "                              batch_dims=1)\n",
    "\n",
    "        # √âchantillonnage\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        sampled_id = tf.random.categorical(tf.math.log(probs), 1)\n",
    "\n",
    "        return sampled_id\n",
    "\n",
    "    def generate_response(self, prompt, max_length=50, temperature=0.8, top_k=40, top_p=0.9):\n",
    "        \"\"\"G√©n√®re une r√©ponse optimis√©e avec cache\"\"\"\n",
    "\n",
    "        # V√©rification du cache\n",
    "        cache_key = f\"{prompt}_{temperature}_{top_k}_{top_p}\"\n",
    "        if cache_key in self.generation_cache:\n",
    "            return self.generation_cache[cache_key]\n",
    "\n",
    "        # Tokenisation\n",
    "        input_ids = self.tokenizer([prompt])\n",
    "\n",
    "        # G√©n√©ration avec monitoring\n",
    "        start_time = tf.timestamp()\n",
    "        generated_ids = self._generate_step(input_ids, temperature, top_k, top_p)\n",
    "        generation_time = tf.timestamp() - start_time\n",
    "\n",
    "        # D√©tokenisation\n",
    "        try:\n",
    "            # Conversion s√©curis√©e\n",
    "            generated_text = self.tokenizer.get_vocabulary()[generated_ids[0, 0].numpy()]\n",
    "\n",
    "            # Reconstruction du texte\n",
    "            vocab = self.tokenizer.get_vocabulary()\n",
    "            tokens = []\n",
    "            for token_id in generated_ids[0]:\n",
    "                if token_id.numpy() < len(vocab):\n",
    "                    token = vocab[token_id.numpy()]\n",
    "                    if token not in ['[START]', '[END]', '']:\n",
    "                        tokens.append(token)\n",
    "\n",
    "            response = ' '.join(tokens)\n",
    "\n",
    "            # Nettoyage et post-traitement\n",
    "            response = response.replace('[UNK]', '').strip()\n",
    "\n",
    "            # Cache du r√©sultat\n",
    "            self.generation_cache[cache_key] = response\n",
    "\n",
    "            # Logging des performances\n",
    "            print(f\"‚ö° G√©n√©ration: {float(generation_time):.3f}s - {len(tokens)} tokens\")\n",
    "\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur de g√©n√©ration: {e}\")\n",
    "            return \"Je... je ne sais pas quoi dire... *rougit*\"\n",
    "\n",
    "\n",
    "# Cr√©ation du g√©n√©rateur ultra-optimis√©\n",
    "print(\"üå∏ Cr√©ation du g√©n√©rateur Shirayuki ultra-optimis√©...\")\n",
    "generator = UltraShirayukiGenerator(model, tokenizer, max_length)\n",
    "\n",
    "# Classe de monitoring avanc√© des performances\n",
    "class UltraPerformanceMonitor:\n",
    "    \"\"\"Monitoring ultra-avanc√© des performances syst√®me\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'cpu_usage': [],\n",
    "            'memory_usage': [],\n",
    "            'gpu_usage': [],\n",
    "            'training_speed': [],\n",
    "            'generation_speed': [],\n",
    "            'model_throughput': []\n",
    "        }\n",
    "        self.monitoring_active = False\n",
    "\n",
    "    def start_monitoring(self):\n",
    "        \"\"\"D√©marre le monitoring en temps r√©el\"\"\"\n",
    "        self.monitoring_active = True\n",
    "        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)\n",
    "        self.monitor_thread.start()\n",
    "        print(\"üìä Monitoring ultra-performance d√©marr√©\")\n",
    "\n",
    "    def _monitor_loop(self):\n",
    "        \"\"\"Boucle de monitoring optimis√©e\"\"\"\n",
    "        import time\n",
    "        while self.monitoring_active:\n",
    "            # CPU et m√©moire\n",
    "            cpu_percent = psutil.cpu_percent(interval=0.1, percpu=False)\n",
    "            memory_info = psutil.virtual_memory()\n",
    "\n",
    "            self.metrics['cpu_usage'].append(cpu_percent)\n",
    "            self.metrics['memory_usage'].append(memory_info.percent)\n",
    "\n",
    "            # GPU (si disponible)\n",
    "            try:\n",
    "                import pynvml\n",
    "                pynvml.nvmlInit()\n",
    "                if pynvml.nvmlDeviceGetCount() > 0:\n",
    "                    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "                    gpu_util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "                    memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "\n",
    "                    self.metrics['gpu_usage'].append({\n",
    "                        'utilization': gpu_util.gpu,\n",
    "                        'memory_used': memory_info.used / memory_info.total * 100\n",
    "                    })\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "    def get_performance_summary(self):\n",
    "        \"\"\"R√©sum√© des performances\"\"\"\n",
    "        if not self.metrics['cpu_usage']:\n",
    "            return \"Monitoring non d√©marr√©\"\n",
    "\n",
    "        recent_cpu = self.metrics['cpu_usage'][-60:]  # Derni√®re minute\n",
    "        recent_memory = self.metrics['memory_usage'][-60:]\n",
    "\n",
    "        avg_cpu = sum(recent_cpu) / len(recent_cpu)\n",
    "        avg_memory = sum(recent_memory) / len(recent_memory)\n",
    "        max_cpu = max(recent_cpu)\n",
    "        max_memory = max(recent_memory)\n",
    "\n",
    "        summary = f\"\"\"\n",
    "üìä PERFORMANCES TEMPS R√âEL:\n",
    "   CPU moyen: {avg_cpu:.1f}% (max: {max_cpu:.1f}%)\n",
    "   RAM moyenne: {avg_memory:.1f}% (max: {max_memory:.1f}%)\n",
    "   Utilisation cible: 95-100% pour performance maximale\n",
    "        \"\"\"\n",
    "\n",
    "        if self.metrics['gpu_usage']:\n",
    "            recent_gpu = self.metrics['gpu_usage'][-60:]\n",
    "            avg_gpu_util = sum(g['utilization'] for g in recent_gpu) / len(recent_gpu)\n",
    "            avg_gpu_mem = sum(g['memory_used'] for g in recent_gpu) / len(recent_gpu)\n",
    "            summary += f\"   GPU utilisation: {avg_gpu_util:.1f}%\\n\"\n",
    "            summary += f\"   GPU m√©moire: {avg_gpu_mem:.1f}%\"\n",
    "\n",
    "        return summary\n",
    "\n",
    "# Initialisation du monitoring ultra-performance\n",
    "ultra_monitor = UltraPerformanceMonitor()\n",
    "ultra_monitor.start_monitoring()\n",
    "\n",
    "# Callback ultra-optimis√© avec monitoring temps r√©el\n",
    "class UltraOptimizedCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback ultra-optimis√© pour performance maximale\"\"\"\n",
    "\n",
    "    def __init__(self, monitor_interval=10):\n",
    "        super().__init__()\n",
    "        self.monitor_interval = monitor_interval\n",
    "        self.batch_times = []\n",
    "        self.epoch_start_time = None\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = tf.timestamp()\n",
    "        print(f\"\\nüöÄ Epoch {epoch + 1} - Optimisation maximale activ√©e\")\n",
    "        print(ultra_monitor.get_performance_summary())\n",
    "\n",
    "        # Optimisation dynamique du garbage collector\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "        # Force la compilation XLA si pas encore fait\n",
    "        if epoch == 0:\n",
    "            print(\"‚ö° Compilation XLA en cours...\")\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        self.batch_start_time = tf.timestamp()\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        batch_time = float(tf.timestamp() - self.batch_start_time)\n",
    "        self.batch_times.append(batch_time)\n",
    "\n",
    "        if batch % self.monitor_interval == 0:\n",
    "            avg_batch_time = sum(self.batch_times[-10:]) / min(len(self.batch_times), 10)\n",
    "            throughput = 1.0 / avg_batch_time if avg_batch_time > 0 else 0\n",
    "\n",
    "            print(f\"   üìà Batch {batch}: {avg_batch_time:.3f}s/batch, \"\n",
    "                  f\"Throughput: {throughput:.1f} batch/s\")\n",
    "\n",
    "            # Affichage p√©riodique des m√©triques\n",
    "            if batch % 50 == 0:\n",
    "                current_stats = resource_monitor.get_current_stats()\n",
    "                print(f\"   üî• Ressources: {current_stats}\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_time = float(tf.timestamp() - self.epoch_start_time)\n",
    "        avg_batch_time = sum(self.batch_times) / len(self.batch_times) if self.batch_times else 0\n",
    "\n",
    "        print(f\"‚úÖ Epoch {epoch + 1} termin√©e en {epoch_time:.1f}s\")\n",
    "        print(f\"üìä Temps moyen par batch: {avg_batch_time:.3f}s\")\n",
    "        print(f\"üéØ Loss: {logs.get('loss', 0):.4f}, Accuracy: {logs.get('accuracy', 0):.4f}\")\n",
    "\n",
    "        self.batch_times = []  # Reset pour la prochaine epoch\n",
    "\n",
    "# Configuration ultra-optimis√©e de l'entra√Ænement\n",
    "ultra_callback = UltraOptimizedCallback(monitor_interval=10)\n",
    "\n",
    "# Calcul du batch size optimal dynamique\n",
    "def calculate_optimal_batch_size():\n",
    "    \"\"\"Calcule le batch size optimal selon les ressources disponibles\"\"\"\n",
    "    base_batch_size = 16\n",
    "\n",
    "    # Adaptation selon la m√©moire disponible\n",
    "    memory_factor = min(4, available_memory / 4)  # Max 4x pour 16GB+\n",
    "\n",
    "    # Adaptation selon le nombre de CPU\n",
    "    cpu_factor = min(2, cpu_count / 8)  # Max 2x pour 8+ threads\n",
    "\n",
    "    # Adaptation selon le GPU\n",
    "    gpu_factor = 1.5 if gpus else 1.0\n",
    "\n",
    "    optimal_size = int(base_batch_size * memory_factor * cpu_factor * gpu_factor)\n",
    "    optimal_size = min(64, max(8, optimal_size))  # Entre 8 et 64\n",
    "\n",
    "    print(f\"üìä Batch size optimal calcul√©: {optimal_size}\")\n",
    "    print(f\"   Facteurs: m√©moire={memory_factor:.1f}, CPU={cpu_factor:.1f}, GPU={gpu_factor:.1f}\")\n",
    "\n",
    "    return optimal_size\n",
    "\n",
    "# Calcul et application du batch size optimal\n",
    "optimal_batch_size = calculate_optimal_batch_size()\n",
    "\n",
    "# Recr√©ation du dataset avec le batch size optimal\n",
    "print(\"‚ö° Reconfiguration du dataset avec batch size optimal...\")\n",
    "\n",
    "# Fonction de pr√©traitement ultra-optimis√©e\n",
    "@tf.function(jit_compile=True)\n",
    "def preprocess_batch_ultra(batch):\n",
    "    \"\"\"Pr√©traitement ultra-optimis√© des batches\"\"\"\n",
    "    encoder_input = batch['encoder_input']\n",
    "    decoder_input = batch['decoder_input']\n",
    "    decoder_target = batch['decoder_target']\n",
    "\n",
    "    # Optimisations sur les tenseurs\n",
    "    encoder_input = tf.cast(encoder_input, tf.int32)\n",
    "    decoder_input = tf.cast(decoder_input, tf.int32)\n",
    "    decoder_target = tf.cast(decoder_target, tf.int32)\n",
    "\n",
    "    return (encoder_input, decoder_input), decoder_target\n",
    "\n",
    "# Dataset ultra-optimis√© avec nouveau batch size\n",
    "optimized_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    'encoder_input': input_ids,\n",
    "    'decoder_input': decoder_input_ids,\n",
    "    'decoder_target': decoder_target_ids\n",
    "})\n",
    "\n",
    "# Application de toutes les optimisations maximales\n",
    "optimized_dataset = (optimized_dataset\n",
    "    .with_options(dataset_options)\n",
    "    .map(preprocess_batch_ultra, num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .shuffle(4096, reshuffle_each_iteration=True)\n",
    "    .batch(optimal_batch_size, drop_remainder=True, num_parallel_calls=AUTOTUNE)\n",
    "    .prefetch(AUTOTUNE)\n",
    "    .repeat())  # R√©p√©tition pour √©viter les fins d'epoch\n",
    "\n",
    "# Configuration avanc√©e des callbacks\n",
    "ultra_callbacks = [\n",
    "    ultra_callback,\n",
    "    performance_callback,\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"loss\",\n",
    "        patience=8,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        min_delta=1e-5\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"loss\",\n",
    "        factor=0.7,\n",
    "        patience=4,\n",
    "        min_lr=1e-8,\n",
    "        verbose=1,\n",
    "        cooldown=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='ultra_shirayuki_{epoch:02d}_{loss:.4f}.keras',\n",
    "        monitor='loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1,\n",
    "        save_freq='epoch'\n",
    "    ),\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir='./logs/ultra_shirayuki',\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        update_freq=100,\n",
    "        profile_batch=(100, 120)\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\ude80 D√âMARRAGE DE L'ENTRA√éNEMENT ULTRA-OPTIMIS√â\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚ö° Configuration finale:\")\n",
    "print(f\"   Batch size optimal: {optimal_batch_size}\")\n",
    "print(f\"   CPU threads: {cpu_count}\")\n",
    "print(f\"   M√©moire allou√©e: {int(available_memory * 0.8)} GB\")\n",
    "print(f\"   GPU disponibles: {len(gpus)}\")\n",
    "print(f\"   Strat√©gie: {type(strategy).__name__}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calcul du nombre de steps optimal\n",
    "steps_per_epoch = len(conversation_pairs) // optimal_batch_size\n",
    "total_epochs = 30  # Plus d'epochs gr√¢ce aux optimisations\n",
    "\n",
    "print(f\"üìä Steps par epoch: {steps_per_epoch}\")\n",
    "print(f\"üìä Total epochs: {total_epochs}\")\n",
    "\n",
    "# LANCEMENT DE L'ENTRA√éNEMENT ULTRA-OPTIMIS√â\n",
    "with strategy.scope():\n",
    "    history = model.fit(\n",
    "        optimized_dataset,\n",
    "        epochs=total_epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        callbacks=ultra_callbacks,\n",
    "        verbose=1,\n",
    "        workers=cpu_count,\n",
    "        use_multiprocessing=True,\n",
    "        max_queue_size=cpu_count * 3\n",
    "    )\n",
    "\n",
    "# Arr√™t du monitoring\n",
    "ultra_monitor.monitoring_active = False\n",
    "resource_monitor.stop_monitoring()\n",
    "\n",
    "print(\"\\nüéâ ENTRA√éNEMENT ULTRA-OPTIMIS√â TERMIN√â!\")\n",
    "print(\"=\" * 60)\n",
    "print(ultra_monitor.get_performance_summary())\n",
    "print(\"‚úÖ Mod√®le Shirayuki ultra-optimis√© pr√™t pour g√©n√©ration maximale!\")\n",
    "\n",
    "# Test de performance du g√©n√©rateur\n",
    "print(\"\\nüß™ Test de performance du g√©n√©rateur...\")\n",
    "test_prompts = [\n",
    "    \"Bonjour Shirayuki\",\n",
    "    \"Tu es vraiment mignonne\",\n",
    "    \"Comment √ßa va ?\",\n",
    "    \"Je t'aime\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    response = generator.generate_response(prompt, temperature=0.8)\n",
    "    print(f\"üå∏ {prompt} -> {response}\")\n",
    "\n",
    "print(\"\\nüöÄ SYST√àME OPTIMIS√â √Ä 100% POUR PERFORMANCES MAXIMALES!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86402d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION ULTRA-OPTIMIS√âE POUR CONVERSATIONS SHIRAYUKI\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, Dropout, Embedding, Conv1D\n",
    "import numpy as np\n",
    "\n",
    "# Configuration pour optimisation maximale\n",
    "tf.config.optimizer.set_jit(True)  # Active XLA JIT\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')  # Mixed precision\n",
    "\n",
    "# Configuration m√©moire GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "class FlashAttentionLike(tf.keras.layers.Layer):\n",
    "    \"\"\"Impl√©mentation d'une attention optimis√©e inspir√©e de Flash Attention\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Projection matrices optimis√©es (sans bias)\n",
    "        self.q_proj = Dense(embed_dim, use_bias=False, name='q_proj')\n",
    "        self.k_proj = Dense(embed_dim, use_bias=False, name='k_proj')\n",
    "        self.v_proj = Dense(embed_dim, use_bias=False, name='v_proj')\n",
    "        self.out_proj = Dense(embed_dim, use_bias=False, name='out_proj')\n",
    "\n",
    "        self.dropout_layer = Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        batch_size, seq_len = tf.shape(x)[0], tf.shape(x)[1]\n",
    "\n",
    "        # Projections\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        # Reshape pour multi-head\n",
    "        q = tf.reshape(q, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
    "        k = tf.reshape(k, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
    "        v = tf.reshape(v, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
    "\n",
    "        # Transpose pour dimensions (batch, heads, seq, head_dim)\n",
    "        q = tf.transpose(q, [0, 2, 1, 3])\n",
    "        k = tf.transpose(k, [0, 2, 1, 3])\n",
    "        v = tf.transpose(v, [0, 2, 1, 3])\n",
    "\n",
    "        # Attention optimis√©e\n",
    "        attention_scores = tf.matmul(q, k, transpose_b=True) * self.scale\n",
    "\n",
    "        # Application du masque\n",
    "        if mask is not None:\n",
    "            mask = tf.cast(mask, dtype=attention_scores.dtype)\n",
    "            attention_scores += (mask * -1e9)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "        attention_weights = self.dropout_layer(attention_weights, training=training)\n",
    "\n",
    "        # Application √† V\n",
    "        attention_output = tf.matmul(attention_weights, v)\n",
    "\n",
    "        # Reshape pour output\n",
    "        attention_output = tf.transpose(attention_output, [0, 2, 1, 3])\n",
    "        attention_output = tf.reshape(attention_output, (batch_size, seq_len, self.embed_dim))\n",
    "\n",
    "        return self.out_proj(attention_output)\n",
    "\n",
    "\n",
    "class UltraOptimizedConversationTransformer(tf.keras.Model):\n",
    "    \"\"\"Version ultra-optimis√©e du transformer pour conversations\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim=768, num_heads=12, ff_dim=3072,\n",
    "                 maxlen=128, num_layers=8, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Embedding avec initialisation optimis√©e\n",
    "        self.embedding = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embed_dim,\n",
    "            embeddings_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "            mask_zero=True\n",
    "        )\n",
    "\n",
    "        # Position encoding learnable (plus flexible que RoPE pour ce cas)\n",
    "        self.pos_embedding = Embedding(maxlen, embed_dim)\n",
    "\n",
    "        # Encoder layers avec Flash Attention\n",
    "        self.encoder_layers = [\n",
    "            self._create_ultra_layer(embed_dim, num_heads, ff_dim, rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_layers = [\n",
    "            self._create_ultra_layer(embed_dim, num_heads, ff_dim, rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Normalisation finale optimis√©e\n",
    "        self.final_norm = RMSNormalization(epsilon=1e-6)\n",
    "\n",
    "        # Output projection avec weight tying\n",
    "        self.output_layer = Dense(vocab_size, use_bias=False, dtype='float32')\n",
    "\n",
    "        # Dropout globaux\n",
    "        self.encoder_dropout = Dropout(rate)\n",
    "        self.decoder_dropout = Dropout(rate)\n",
    "\n",
    "    def _create_ultra_layer(self, embed_dim, num_heads, ff_dim, rate):\n",
    "        \"\"\"Cr√©e une couche transformer ultra-optimis√©e\"\"\"\n",
    "        return tf.keras.Sequential([\n",
    "            RMSNormalization(epsilon=1e-6),\n",
    "            FlashAttentionLike(embed_dim, num_heads, rate),\n",
    "            RMSNormalization(epsilon=1e-6),\n",
    "            Dense(ff_dim * 2, use_bias=False, activation=None),\n",
    "            tf.keras.layers.Lambda(lambda x: self._swiglu(x)),\n",
    "            Dense(embed_dim, use_bias=False),\n",
    "            Dropout(rate)\n",
    "        ])\n",
    "\n",
    "    def _swiglu(self, x):\n",
    "        \"\"\"SwiGLU activation optimis√©e\"\"\"\n",
    "        gate, hidden = tf.split(x, 2, axis=-1)\n",
    "        return tf.nn.swish(gate) * hidden\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        if isinstance(inputs, tuple):\n",
    "            input_ids, target_ids = inputs\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "            target_ids = None\n",
    "\n",
    "        # Encoder\n",
    "        encoder_output = self.encode(input_ids, training=training)\n",
    "\n",
    "        # Decoder\n",
    "        if target_ids is not None:\n",
    "            decoder_output = self.decode(target_ids, encoder_output, training=training)\n",
    "            return self.output_layer(decoder_output)\n",
    "        else:\n",
    "            return encoder_output\n",
    "\n",
    "    def encode(self, input_ids, training=False):\n",
    "        \"\"\"Encoder ultra-optimis√©\"\"\"\n",
    "        seq_len = tf.shape(input_ids)[1]\n",
    "\n",
    "        # Embedding + position\n",
    "        x = self.embedding(input_ids)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "\n",
    "        positions = tf.range(seq_len)[None, :]\n",
    "        x += self.pos_embedding(positions)\n",
    "        x = self.encoder_dropout(x, training=training)\n",
    "\n",
    "        # Masque de padding\n",
    "        mask = tf.cast(tf.equal(input_ids, 0), tf.float32)\n",
    "        attention_mask = mask[:, None, None, :] * -1e9\n",
    "\n",
    "        # Passage dans les couches avec gradient checkpointing\n",
    "        for i, layer in enumerate(self.encoder_layers):\n",
    "            if training and i > 0:  # Gradient checkpointing\n",
    "                x = tf.recompute_grad(lambda inputs: layer(inputs, training=training))(x)\n",
    "            else:\n",
    "                x = layer(x, training=training)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decode(self, target_ids, encoder_output, training=False):\n",
    "        \"\"\"Decoder ultra-optimis√© avec masques causaux\"\"\"\n",
    "        seq_len = tf.shape(target_ids)[1]\n",
    "\n",
    "        # Embedding + position\n",
    "        x = self.embedding(target_ids)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "\n",
    "        positions = tf.range(seq_len)[None, :]\n",
    "        x += self.pos_embedding(positions)\n",
    "        x = self.decoder_dropout(x, training=training)\n",
    "\n",
    "        # Masques causaux et de padding\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        padding_mask = tf.cast(tf.not_equal(target_ids, 0), tf.float32)\n",
    "\n",
    "        combined_mask = tf.minimum(\n",
    "            causal_mask[None, None, :, :],\n",
    "            padding_mask[:, None, None, :]\n",
    "        )\n",
    "        attention_mask = (1.0 - combined_mask) * -1e9\n",
    "\n",
    "        # Passage dans les couches decoder\n",
    "        for i, layer in enumerate(self.decoder_layers):\n",
    "            if training and i > 0:\n",
    "                x = tf.recompute_grad(lambda inputs: layer(inputs, training=training))(x)\n",
    "            else:\n",
    "                x = layer(x, training=training)\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Optimisations sp√©cifiques pour les conversations\n",
    "class ConversationOptimizer:\n",
    "    \"\"\"Optimisations sp√©cialis√©es pour les mod√®les conversationnels\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create_conversation_dataset(conversation_pairs, tokenizer, batch_size=8, augment=True):\n",
    "        \"\"\"Cr√©e un dataset optimis√© pour l'entra√Ænement conversationnel\"\"\"\n",
    "\n",
    "        # Pr√©paration des donn√©es avec augmentation\n",
    "        if augment:\n",
    "            conversation_pairs = ConversationOptimizer._augment_conversations(conversation_pairs)\n",
    "\n",
    "        # Tokenisation\n",
    "        inputs = [pair[0] for pair in conversation_pairs]\n",
    "        outputs = [\"[START] \" + pair[1] + \" [END]\" for pair in conversation_pairs]\n",
    "\n",
    "        input_ids = tokenizer(inputs)\n",
    "        output_ids = tokenizer(outputs)\n",
    "\n",
    "        # Teacher forcing setup\n",
    "        decoder_input_ids = output_ids[:, :-1]\n",
    "        decoder_target_ids = output_ids[:, 1:]\n",
    "\n",
    "        # Dataset avec optimisations m√©moire\n",
    "        dataset = tf.data.Dataset.from_tensor_slices({\n",
    "            'encoder_input': input_ids,\n",
    "            'decoder_input': decoder_input_ids,\n",
    "            'decoder_target': decoder_target_ids\n",
    "        })\n",
    "\n",
    "        def prepare_batch(batch):\n",
    "            return (\n",
    "                (batch['encoder_input'], batch['decoder_input']),\n",
    "                batch['decoder_target']\n",
    "            )\n",
    "\n",
    "        return (dataset\n",
    "                .map(prepare_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                .cache()\n",
    "                .shuffle(1024, reshuffle_each_iteration=True)\n",
    "                .batch(batch_size, drop_remainder=True)\n",
    "                .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "    @staticmethod\n",
    "    def _augment_conversations(conversation_pairs):\n",
    "        \"\"\"Augmente le dataset avec des variations\"\"\"\n",
    "        augmented = list(conversation_pairs)\n",
    "\n",
    "        # Synonymes simples pour augmentation\n",
    "        synonyms = {\n",
    "            'love': ['adore', 'care for', 'cherish'],\n",
    "            'great': ['amazing', 'wonderful', 'fantastic'],\n",
    "            'cute': ['adorable', 'sweet', 'lovely']\n",
    "        }\n",
    "\n",
    "        for inp, out in conversation_pairs[:len(conversation_pairs)//3]:  # Augmente 1/3 des donn√©es\n",
    "            # Remplacement de synonymes\n",
    "            for word, syns in synonyms.items():\n",
    "                if word in inp.lower():\n",
    "                    for syn in syns[:1]:  # Une seule variation par mot\n",
    "                        new_inp = inp.lower().replace(word, syn)\n",
    "                        augmented.append((new_inp, out))\n",
    "\n",
    "        return augmented\n",
    "\n",
    "\n",
    "# Fonction de comparaison des mod√®les\n",
    "def compare_conversation_models():\n",
    "    \"\"\"Compare les diff√©rentes versions du mod√®le de conversation\"\"\"\n",
    "\n",
    "    print(\"üîÑ Comparaison des mod√®les de conversation\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Mod√®le de base (d√©j√† cr√©√©)\n",
    "    print(f\"üìä Mod√®le de base:\")\n",
    "    print(f\"   Param√®tres: {model.count_params():,}\")\n",
    "    print(f\"   Embed dim: {embed_dim}\")\n",
    "    print(f\"   Layers: {num_layers}\")\n",
    "\n",
    "    # Mod√®le ultra-optimis√©\n",
    "    ultra_model = UltraOptimizedConversationTransformer(\n",
    "        vocab_size=tokenizer.vocabulary_size(),\n",
    "        embed_dim=768,\n",
    "        num_heads=12,\n",
    "        ff_dim=3072,\n",
    "        maxlen=max_length,\n",
    "        num_layers=8,\n",
    "        rate=0.1\n",
    "    )\n",
    "\n",
    "    print(f\"\\nüìä Mod√®le ultra-optimis√©:\")\n",
    "    print(f\"   Param√®tres: {ultra_model.count_params():,}\")\n",
    "    print(f\"   Embed dim: 768\")\n",
    "    print(f\"   Layers: 8\")\n",
    "\n",
    "    print(f\"\\n‚ú® Optimisations ultra appliqu√©es:\")\n",
    "    print(f\"   ‚úì Flash Attention optimis√©e\")\n",
    "    print(f\"   ‚úì SwiGLU activation\")\n",
    "    print(f\"   ‚úì RMSNormalization\")\n",
    "    print(f\"   ‚úì Gradient checkpointing\")\n",
    "    print(f\"   ‚úì Mixed precision training\")\n",
    "    print(f\"   ‚úì Optimisations m√©moire GPU\")\n",
    "\n",
    "    return ultra_model\n",
    "\n",
    "\n",
    "print(\"Version ultra-optimis√©e charg√©e pour les conversations Shirayuki!\")\n",
    "print(\"Utilisez compare_conversation_models() pour voir les diff√©rences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582fa340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âVALUATION DES PERFORMANCES CONVERSATIONNELLES\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"üìà Analyse des performances du mod√®le Shirayuki\")\n",
    "\n",
    "# Graphiques de l'entra√Ænement\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss', color='#FF6B9D')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss', color='#4ECDC4')\n",
    "plt.title('Loss Evolution - Shirayuki Model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy', color='#FF6B9D')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy', color='#4ECDC4')\n",
    "plt.title('Accuracy Evolution')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning Rate\n",
    "plt.subplot(2, 3, 3)\n",
    "lr_values = [warmup_schedule(step) for step in range(0, total_steps, total_steps // epochs)]\n",
    "plt.plot(lr_values, color='#FFE66D')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Test de g√©n√©ration avec m√©triques\n",
    "plt.subplot(2, 3, 4)\n",
    "test_prompts = [\n",
    "    \"Bonjour Shirayuki\",\n",
    "    \"Comment √ßa va ?\",\n",
    "    \"Tu es mignonne\",\n",
    "    \"Je t'aime\",\n",
    "    \"Qu'est-ce que tu fais ?\"\n",
    "]\n",
    "\n",
    "response_lengths = []\n",
    "response_qualities = []\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    response = generator.generate_response(prompt, max_length=50)\n",
    "    response_lengths.append(len(response.split()))\n",
    "    # Score de qualit√© basique (diversit√© des mots)\n",
    "    unique_words = len(set(response.lower().split()))\n",
    "    total_words = len(response.split())\n",
    "    quality_score = unique_words / max(total_words, 1)\n",
    "    response_qualities.append(quality_score)\n",
    "\n",
    "plt.bar(range(len(test_prompts)), response_lengths, color='#FF6B9D', alpha=0.7)\n",
    "plt.title('Longueur des R√©ponses G√©n√©r√©es')\n",
    "plt.xlabel('Prompt Test')\n",
    "plt.ylabel('Nombre de Mots')\n",
    "plt.xticks(range(len(test_prompts)), [f\"Test {i+1}\" for i in range(len(test_prompts))])\n",
    "\n",
    "# Qualit√© des r√©ponses\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.bar(range(len(test_prompts)), response_qualities, color='#4ECDC4', alpha=0.7)\n",
    "plt.title('Diversit√© Lexicale des R√©ponses')\n",
    "plt.xlabel('Prompt Test')\n",
    "plt.ylabel('Score de Diversit√©')\n",
    "plt.xticks(range(len(test_prompts)), [f\"Test {i+1}\" for i in range(len(test_prompts))])\n",
    "\n",
    "# Statistiques du mod√®le\n",
    "plt.subplot(2, 3, 6)\n",
    "model_stats = {\n",
    "    'Param√®tres': model.count_params(),\n",
    "    'Vocab Size': tokenizer.vocabulary_size(),\n",
    "    'Max Length': max_length,\n",
    "    'Embed Dim': embed_dim,\n",
    "    'Layers': num_layers,\n",
    "    'Heads': num_heads\n",
    "}\n",
    "\n",
    "plt.barh(list(model_stats.keys()), list(model_stats.values()), color='#FFE66D')\n",
    "plt.title('Statistiques du Mod√®le')\n",
    "plt.xlabel('Valeur')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# M√©triques d√©taill√©es\n",
    "print(\"\\nüìä M√âTRIQUES D√âTAILL√âES\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üèÜ Meilleure Val Loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"üéØ Meilleure Val Accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "print(f\"üìà Am√©lioration totale: {(max(history.history['val_accuracy']) - min(history.history['val_accuracy'])):.4f}\")\n",
    "\n",
    "# Test de conversation tsundere\n",
    "print(f\"\\nüí¨ TEST DE PERSONNALIT√â TSUNDERE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "tsundere_tests = [\n",
    "    \"Tu es vraiment adorable\",\n",
    "    \"Je pense √† toi tout le temps\",\n",
    "    \"Tu me manques\",\n",
    "    \"Tu veux √™tre mon amie ?\",\n",
    "    \"Tu es la plus belle\"\n",
    "]\n",
    "\n",
    "for i, test in enumerate(tsundere_tests, 1):\n",
    "    response = generator.generate_response(test, max_length=30, temperature=0.8)\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"   Input: {test}\")\n",
    "    print(f\"   Shirayuki: {response}\")\n",
    "    print()\n",
    "\n",
    "# Sauvegarde des m√©triques\n",
    "performance_data = {\n",
    "    'final_val_loss': min(history.history['val_loss']),\n",
    "    'final_val_accuracy': max(history.history['val_accuracy']),\n",
    "    'model_params': model.count_params(),\n",
    "    'training_epochs': len(history.history['loss']),\n",
    "    'response_qualities': response_qualities,\n",
    "    'response_lengths': response_lengths\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ √âvaluation termin√©e! Mod√®le Shirayuki optimis√© et test√©.\")\n",
    "print(f\"üìÅ M√©triques sauvegard√©es pour analyse future.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da506b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHAT INTERACTIF AVEC SHIRAYUKI\n",
    "print(\"üí¨ Interface de Chat avec Shirayuki\")\n",
    "print(\"Tapez 'quit' pour arr√™ter la conversation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def shirayuki_chat_interface():\n",
    "    \"\"\"Interface de chat interactive avec personnalisation\"\"\"\n",
    "\n",
    "    print(\"üå∏ Shirayuki: Bonjour ! Je suis Shirayuki... *rougit* Qu'est-ce que tu veux ?\")\n",
    "\n",
    "    conversation_history = []\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Input utilisateur\n",
    "            user_input = input(\"\\nüë§ Vous: \").strip()\n",
    "\n",
    "            if user_input.lower() in ['quit', 'exit', 'bye', 'au revoir']:\n",
    "                print(\"üå∏ Shirayuki: *d√©tourne le regard* C-ce n'est pas comme si j'allais te manquer ! √Ä bient√¥t...\")\n",
    "                break\n",
    "\n",
    "            if not user_input:\n",
    "                continue\n",
    "\n",
    "            # G√©n√©ration de la r√©ponse\n",
    "            print(\"üå∏ Shirayuki: \", end=\"\", flush=True)\n",
    "\n",
    "            # Diff√©rents modes de g√©n√©ration selon le contexte\n",
    "            if any(word in user_input.lower() for word in ['love', 'aime', 'amour', 'cute', 'mignon']):\n",
    "                # Mode tsundere intensifi√©\n",
    "                response = generator.generate_response(\n",
    "                    user_input,\n",
    "                    max_length=40,\n",
    "                    temperature=0.9,\n",
    "                    top_p=0.85\n",
    "                )\n",
    "            elif any(word in user_input.lower() for word in ['triste', 'sad', 'probl√®me', 'mal']):\n",
    "                # Mode plus doux\n",
    "                response = generator.generate_response(\n",
    "                    user_input,\n",
    "                    max_length=35,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9\n",
    "                )\n",
    "            else:\n",
    "                # Mode normal\n",
    "                response = generator.generate_response(\n",
    "                    user_input,\n",
    "                    max_length=30,\n",
    "                    temperature=0.8,\n",
    "                    top_p=0.87\n",
    "                )\n",
    "\n",
    "            print(response)\n",
    "\n",
    "            # Sauvegarde de l'historique\n",
    "            conversation_history.append({\n",
    "                'user': user_input,\n",
    "                'shirayuki': response,\n",
    "                'timestamp': tf.timestamp()\n",
    "            })\n",
    "\n",
    "            # Suggestions de r√©ponses\n",
    "            if len(conversation_history) % 3 == 0:\n",
    "                suggestions = [\n",
    "                    \"Comment tu te sens ?\",\n",
    "                    \"Raconte-moi ta journ√©e\",\n",
    "                    \"Tu es tr√®s mignonne\",\n",
    "                    \"Qu'est-ce que tu aimes faire ?\"\n",
    "                ]\n",
    "                print(f\"üí° Suggestions: {' | '.join(suggestions)}\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüå∏ Shirayuki: *surprise* Tu pars d√©j√† ? Bon... √† bient√¥t alors...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur: {e}\")\n",
    "            print(\"üå∏ Shirayuki: *confuse* Je... je n'ai pas compris. Peux-tu r√©p√©ter ?\")\n",
    "\n",
    "    # R√©sum√© de la conversation\n",
    "    if conversation_history:\n",
    "        print(f\"\\nüìä R√©sum√© de la conversation:\")\n",
    "        print(f\"   Messages √©chang√©s: {len(conversation_history)}\")\n",
    "        avg_length = sum(len(conv['shirayuki'].split()) for conv in conversation_history) / len(conversation_history)\n",
    "        print(f\"   Longueur moyenne des r√©ponses: {avg_length:.1f} mots\")\n",
    "\n",
    "        # Sauvegarde optionnelle\n",
    "        save = input(\"\\nüíæ Sauvegarder cette conversation ? (y/n): \").lower().startswith('y')\n",
    "        if save:\n",
    "            import json\n",
    "            with open('shirayuki_conversation.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(conversation_history, f, ensure_ascii=False, indent=2, default=str)\n",
    "            print(\"‚úÖ Conversation sauvegard√©e dans 'shirayuki_conversation.json'\")\n",
    "\n",
    "# Lancement du chat\n",
    "shirayuki_chat_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505afda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS DE STRESS ET BENCHMARKS ULTRA-PERFORMANCE\n",
    "import time\n",
    "import threading\n",
    "import concurrent.futures\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"üî• TESTS DE STRESS POUR VALIDATION DE L'UTILISATION MAXIMALE DES RESSOURCES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class UltraStressTester:\n",
    "    \"\"\"Testeur de stress ultra-avanc√© pour valider l'utilisation maximale des ressources\"\"\"\n",
    "\n",
    "    def __init__(self, model, generator, tokenizer):\n",
    "        self.model = model\n",
    "        self.generator = generator\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stress_results = {}\n",
    "\n",
    "    def cpu_stress_test(self, duration=60):\n",
    "        \"\"\"Test de stress CPU avec g√©n√©ration massive\"\"\"\n",
    "        print(f\"üî• Test de stress CPU ({duration}s)...\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        generations_count = 0\n",
    "        cpu_usage_samples = []\n",
    "\n",
    "        def cpu_monitor():\n",
    "            while time.time() - start_time < duration:\n",
    "                cpu_usage_samples.append(psutil.cpu_percent(interval=0.5))\n",
    "\n",
    "        # D√©marrage du monitoring CPU\n",
    "        monitor_thread = threading.Thread(target=cpu_monitor, daemon=True)\n",
    "        monitor_thread.start()\n",
    "\n",
    "        # G√©n√©ration massive pour stresser le CPU\n",
    "        test_prompts = [\n",
    "            \"Test de performance CPU\",\n",
    "            \"Stress test maximum\",\n",
    "            \"Utilisation optimale\",\n",
    "            \"Performance benchmark\",\n",
    "            \"Test de charge\"\n",
    "        ] * 10  # 50 prompts diff√©rents\n",
    "\n",
    "        def generate_response(prompt):\n",
    "            return self.generator.generate_response(f\"{prompt} {time.time()}\", max_length=30)\n",
    "\n",
    "        # G√©n√©ration parall√®le massive\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count) as executor:\n",
    "            while time.time() - start_time < duration:\n",
    "                futures = [executor.submit(generate_response, prompt) for prompt in test_prompts]\n",
    "                concurrent.futures.wait(futures, timeout=5)\n",
    "                generations_count += len(futures)\n",
    "\n",
    "        monitor_thread.join(timeout=1)\n",
    "\n",
    "        avg_cpu = sum(cpu_usage_samples) / len(cpu_usage_samples) if cpu_usage_samples else 0\n",
    "        max_cpu = max(cpu_usage_samples) if cpu_usage_samples else 0\n",
    "        throughput = generations_count / duration\n",
    "\n",
    "        self.stress_results['cpu_test'] = {\n",
    "            'avg_cpu_usage': avg_cpu,\n",
    "            'max_cpu_usage': max_cpu,\n",
    "            'generations_per_second': throughput,\n",
    "            'total_generations': generations_count\n",
    "        }\n",
    "\n",
    "        print(f\"   ‚úÖ CPU moyen: {avg_cpu:.1f}% (max: {max_cpu:.1f}%)\")\n",
    "        print(f\"   ‚ö° Throughput: {throughput:.1f} g√©n√©rations/sec\")\n",
    "        print(f\"   üìä Total g√©n√©rations: {generations_count}\")\n",
    "\n",
    "        return self.stress_results['cpu_test']\n",
    "\n",
    "    def memory_stress_test(self, batch_size_multiplier=4):\n",
    "        \"\"\"Test de stress m√©moire avec batch sizes √©normes\"\"\"\n",
    "        print(f\"üíæ Test de stress m√©moire (x{batch_size_multiplier} batch size)...\")\n",
    "\n",
    "        # Sauvegarde de la configuration actuelle\n",
    "        original_batch_size = optimal_batch_size\n",
    "        stress_batch_size = original_batch_size * batch_size_multiplier\n",
    "\n",
    "        try:\n",
    "            # Cr√©ation d'un dataset de stress avec batch size √©norme\n",
    "            stress_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "                'encoder_input': input_ids[:stress_batch_size*10],  # Plus de donn√©es\n",
    "                'decoder_input': decoder_input_ids[:stress_batch_size*10],\n",
    "                'decoder_target': decoder_target_ids[:stress_batch_size*10]\n",
    "            })\n",
    "\n",
    "            stress_dataset = (stress_dataset\n",
    "                .batch(stress_batch_size, drop_remainder=True)\n",
    "                .prefetch(1))\n",
    "\n",
    "            # Monitoring m√©moire\n",
    "            memory_before = psutil.virtual_memory().percent\n",
    "            gpu_memory_before = 0\n",
    "\n",
    "            if gpus:\n",
    "                try:\n",
    "                    import pynvml\n",
    "                    pynvml.nvmlInit()\n",
    "                    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "                    gpu_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "                    gpu_memory_before = gpu_info.used / gpu_info.total * 100\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # Test de forward pass avec batch √©norme\n",
    "            start_time = time.time()\n",
    "            memory_samples = []\n",
    "\n",
    "            def memory_monitor():\n",
    "                for _ in range(30):  # 30 secondes de monitoring\n",
    "                    memory_samples.append(psutil.virtual_memory().percent)\n",
    "                    time.sleep(1)\n",
    "\n",
    "            monitor_thread = threading.Thread(target=memory_monitor, daemon=True)\n",
    "            monitor_thread.start()\n",
    "\n",
    "            # Ex√©cution du stress test\n",
    "            batch_count = 0\n",
    "            for batch in stress_dataset.take(5):  # 5 gros batches\n",
    "                inputs, targets = batch\n",
    "                with tf.GradientTape() as tape:\n",
    "                    predictions = self.model((inputs[0], inputs[1]), training=True)\n",
    "                    loss = tf.keras.losses.sparse_categorical_crossentropy(targets, predictions, from_logits=True)\n",
    "\n",
    "                # Calcul des gradients pour stresser davantage\n",
    "                gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "                batch_count += 1\n",
    "\n",
    "                print(f\"   Batch {batch_count}: Shape {inputs[0].shape}, Loss {tf.reduce_mean(loss):.4f}\")\n",
    "\n",
    "            execution_time = time.time() - start_time\n",
    "            monitor_thread.join(timeout=1)\n",
    "\n",
    "            # Mesure finale de la m√©moire\n",
    "            memory_after = psutil.virtual_memory().percent\n",
    "            max_memory = max(memory_samples) if memory_samples else memory_after\n",
    "\n",
    "            self.stress_results['memory_test'] = {\n",
    "                'memory_before': memory_before,\n",
    "                'memory_after': memory_after,\n",
    "                'max_memory_usage': max_memory,\n",
    "                'memory_increase': memory_after - memory_before,\n",
    "                'stress_batch_size': stress_batch_size,\n",
    "                'execution_time': execution_time,\n",
    "                'batches_processed': batch_count\n",
    "            }\n",
    "\n",
    "            print(f\"   ‚úÖ M√©moire avant: {memory_before:.1f}%\")\n",
    "            print(f\"   üìà M√©moire max: {max_memory:.1f}%\")\n",
    "            print(f\"   üíæ Augmentation: +{memory_after - memory_before:.1f}%\")\n",
    "            print(f\"   ‚ö° Temps d'ex√©cution: {execution_time:.1f}s\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Limite m√©moire atteinte: {e}\")\n",
    "            self.stress_results['memory_test'] = {'error': str(e), 'limit_reached': True}\n",
    "\n",
    "        return self.stress_results.get('memory_test', {})\n",
    "\n",
    "    def gpu_stress_test(self, duration=30):\n",
    "        \"\"\"Test de stress GPU avec calculs intensifs\"\"\"\n",
    "        if not gpus:\n",
    "            print(\"‚ö†Ô∏è Aucun GPU d√©tect√© - test ignor√©\")\n",
    "            return {}\n",
    "\n",
    "        print(f\"üéÆ Test de stress GPU ({duration}s)...\")\n",
    "\n",
    "        try:\n",
    "            import pynvml\n",
    "            pynvml.nvmlInit()\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "            # Monitoring GPU\n",
    "            gpu_utils = []\n",
    "            gpu_memory = []\n",
    "\n",
    "            def gpu_monitor():\n",
    "                start = time.time()\n",
    "                while time.time() - start < duration:\n",
    "                    try:\n",
    "                        util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "                        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "                        gpu_utils.append(util.gpu)\n",
    "                        gpu_memory.append(mem_info.used / mem_info.total * 100)\n",
    "                        time.sleep(0.5)\n",
    "                    except:\n",
    "                        break\n",
    "\n",
    "            monitor_thread = threading.Thread(target=gpu_monitor, daemon=True)\n",
    "            monitor_thread.start()\n",
    "\n",
    "            # Stress test avec calculs intensifs\n",
    "            start_time = time.time()\n",
    "            operations_count = 0\n",
    "\n",
    "            while time.time() - start_time < duration:\n",
    "                # Cr√©ation de tenseurs volumineux pour stresser le GPU\n",
    "                with tf.device('/GPU:0'):\n",
    "                    large_tensor = tf.random.normal([1024, 1024, 256])\n",
    "                    result = tf.linalg.matmul(large_tensor, large_tensor, transpose_b=True)\n",
    "\n",
    "                    # Op√©rations sur le mod√®le\n",
    "                    dummy_input = tf.random.uniform([16, 64], maxval=1000, dtype=tf.int32)\n",
    "                    _ = self.model.encode(dummy_input, training=True)\n",
    "\n",
    "                    operations_count += 1\n",
    "\n",
    "            monitor_thread.join(timeout=1)\n",
    "\n",
    "            avg_gpu_util = sum(gpu_utils) / len(gpu_utils) if gpu_utils else 0\n",
    "            max_gpu_util = max(gpu_utils) if gpu_utils else 0\n",
    "            avg_gpu_memory = sum(gpu_memory) / len(gpu_memory) if gpu_memory else 0\n",
    "            max_gpu_memory = max(gpu_memory) if gpu_memory else 0\n",
    "\n",
    "            self.stress_results['gpu_test'] = {\n",
    "                'avg_gpu_utilization': avg_gpu_util,\n",
    "                'max_gpu_utilization': max_gpu_util,\n",
    "                'avg_gpu_memory': avg_gpu_memory,\n",
    "                'max_gpu_memory': max_gpu_memory,\n",
    "                'operations_per_second': operations_count / duration,\n",
    "                'total_operations': operations_count\n",
    "            }\n",
    "\n",
    "            print(f\"   ‚úÖ GPU utilisation moyenne: {avg_gpu_util:.1f}% (max: {max_gpu_util:.1f}%)\")\n",
    "            print(f\"   üíæ GPU m√©moire moyenne: {avg_gpu_memory:.1f}% (max: {max_gpu_memory:.1f}%)\")\n",
    "            print(f\"   ‚ö° Op√©rations/sec: {operations_count / duration:.1f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Erreur GPU: {e}\")\n",
    "            self.stress_results['gpu_test'] = {'error': str(e)}\n",
    "\n",
    "        return self.stress_results.get('gpu_test', {})\n",
    "\n",
    "    def concurrent_stress_test(self, duration=45):\n",
    "        \"\"\"Test de stress concurrent CPU+GPU+M√©moire\"\"\"\n",
    "        print(f\"üî• Test de stress concurrent ({duration}s)...\")\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        def cpu_task():\n",
    "            # G√©n√©ration continue\n",
    "            count = 0\n",
    "            start = time.time()\n",
    "            while time.time() - start < duration:\n",
    "                self.generator.generate_response(f\"concurrent test {count}\", max_length=20)\n",
    "                count += 1\n",
    "            return count\n",
    "\n",
    "        def gpu_task():\n",
    "            # Calculs GPU intensifs\n",
    "            count = 0\n",
    "            start = time.time()\n",
    "            while time.time() - start < duration:\n",
    "                if gpus:\n",
    "                    with tf.device('/GPU:0'):\n",
    "                        x = tf.random.normal([512, 512])\n",
    "                        _ = tf.linalg.matmul(x, x)\n",
    "                count += 1\n",
    "            return count\n",
    "\n",
    "        def memory_task():\n",
    "            # Allocations m√©moire intensives\n",
    "            arrays = []\n",
    "            start = time.time()\n",
    "            while time.time() - start < duration:\n",
    "                try:\n",
    "                    # Allocation de 100MB\n",
    "                    arr = np.random.random((1000, 1000, 10))\n",
    "                    arrays.append(arr)\n",
    "                    if len(arrays) > 10:  # Limite pour √©viter l'explosion m√©moire\n",
    "                        arrays.pop(0)\n",
    "                except MemoryError:\n",
    "                    break\n",
    "            return len(arrays)\n",
    "\n",
    "        # Monitoring global\n",
    "        system_stats = []\n",
    "\n",
    "        def system_monitor():\n",
    "            start = time.time()\n",
    "            while time.time() - start < duration:\n",
    "                cpu_percent = psutil.cpu_percent(interval=0.1)\n",
    "                memory_percent = psutil.virtual_memory().percent\n",
    "\n",
    "                gpu_util = 0\n",
    "                if gpus:\n",
    "                    try:\n",
    "                        import pynvml\n",
    "                        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "                        util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "                        gpu_util = util.gpu\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                system_stats.append({\n",
    "                    'cpu': cpu_percent,\n",
    "                    'memory': memory_percent,\n",
    "                    'gpu': gpu_util,\n",
    "                    'timestamp': time.time()\n",
    "                })\n",
    "                time.sleep(1)\n",
    "\n",
    "        # Lancement concurrent de tous les tests\n",
    "        print(\"   üöÄ Lancement des t√¢ches concurrentes...\")\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            # Soumission des t√¢ches\n",
    "            cpu_future = executor.submit(cpu_task)\n",
    "            gpu_future = executor.submit(gpu_task)\n",
    "            memory_future = executor.submit(memory_task)\n",
    "            monitor_future = executor.submit(system_monitor)\n",
    "\n",
    "            # Attente de completion\n",
    "            cpu_result = cpu_future.result()\n",
    "            gpu_result = gpu_future.result()\n",
    "            memory_result = memory_future.result()\n",
    "            monitor_future.result()\n",
    "\n",
    "        # Analyse des r√©sultats\n",
    "        if system_stats:\n",
    "            avg_cpu = sum(s['cpu'] for s in system_stats) / len(system_stats)\n",
    "            max_cpu = max(s['cpu'] for s in system_stats)\n",
    "            avg_memory = sum(s['memory'] for s in system_stats) / len(system_stats)\n",
    "            max_memory = max(s['memory'] for s in system_stats)\n",
    "            avg_gpu = sum(s['gpu'] for s in system_stats) / len(system_stats)\n",
    "            max_gpu = max(s['gpu'] for s in system_stats)\n",
    "\n",
    "            results = {\n",
    "                'cpu_generations': cpu_result,\n",
    "                'gpu_operations': gpu_result,\n",
    "                'memory_arrays': memory_result,\n",
    "                'avg_cpu_usage': avg_cpu,\n",
    "                'max_cpu_usage': max_cpu,\n",
    "                'avg_memory_usage': avg_memory,\n",
    "                'max_memory_usage': max_memory,\n",
    "                'avg_gpu_usage': avg_gpu,\n",
    "                'max_gpu_usage': max_gpu,\n",
    "                'duration': duration\n",
    "            }\n",
    "\n",
    "            print(f\"   ‚úÖ G√©n√©rations CPU: {cpu_result}\")\n",
    "            print(f\"   üéÆ Op√©rations GPU: {gpu_result}\")\n",
    "            print(f\"   üíæ Allocations m√©moire: {memory_result}\")\n",
    "            print(f\"   üìä CPU moyen: {avg_cpu:.1f}% (max: {max_cpu:.1f}%)\")\n",
    "            print(f\"   üìä RAM moyenne: {avg_memory:.1f}% (max: {max_memory:.1f}%)\")\n",
    "            if gpus:\n",
    "                print(f\"   üìä GPU moyen: {avg_gpu:.1f}% (max: {max_gpu:.1f}%)\")\n",
    "\n",
    "        self.stress_results['concurrent_test'] = results\n",
    "        return results\n",
    "\n",
    "    def performance_benchmark(self):\n",
    "        \"\"\"Benchmark complet de performance\"\"\"\n",
    "        print(\"\\nüèÜ BENCHMARK COMPLET DE PERFORMANCE\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Test de g√©n√©ration simple\n",
    "        print(\"üß™ Test de g√©n√©ration simple...\")\n",
    "        start_time = time.time()\n",
    "        simple_responses = []\n",
    "        for i in range(100):\n",
    "            response = self.generator.generate_response(f\"Test {i}\", max_length=20)\n",
    "            simple_responses.append(response)\n",
    "        simple_time = time.time() - start_time\n",
    "        simple_throughput = 100 / simple_time\n",
    "\n",
    "        print(f\"   ‚ö° 100 g√©n√©rations en {simple_time:.2f}s\")\n",
    "        print(f\"   üìà Throughput: {simple_throughput:.1f} g√©n√©rations/sec\")\n",
    "\n",
    "        # Test de g√©n√©ration parall√®le\n",
    "        print(\"\\nüöÄ Test de g√©n√©ration parall√®le...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        def parallel_generate(i):\n",
    "            return self.generator.generate_response(f\"Parallel test {i}\", max_length=20)\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count) as executor:\n",
    "            parallel_responses = list(executor.map(parallel_generate, range(100)))\n",
    "\n",
    "        parallel_time = time.time() - start_time\n",
    "        parallel_throughput = 100 / parallel_time\n",
    "        speedup = simple_time / parallel_time\n",
    "\n",
    "        print(f\"   ‚ö° 100 g√©n√©rations parall√®les en {parallel_time:.2f}s\")\n",
    "        print(f\"   üìà Throughput: {parallel_throughput:.1f} g√©n√©rations/sec\")\n",
    "        print(f\"   üöÄ Acc√©l√©ration: {speedup:.1f}x\")\n",
    "\n",
    "        # R√©sum√© du benchmark\n",
    "        benchmark_results = {\n",
    "            'simple_throughput': simple_throughput,\n",
    "            'parallel_throughput': parallel_throughput,\n",
    "            'speedup': speedup,\n",
    "            'simple_time': simple_time,\n",
    "            'parallel_time': parallel_time\n",
    "        }\n",
    "\n",
    "        self.stress_results['benchmark'] = benchmark_results\n",
    "\n",
    "        return benchmark_results\n",
    "\n",
    "    def generate_performance_report(self):\n",
    "        \"\"\"G√©n√®re un rapport complet de performance\"\"\"\n",
    "        print(\"\\n\udcca RAPPORT COMPLET DE PERFORMANCE\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # Graphiques de performance\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('Rapport de Performance Ultra-Optimis√©', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # 1. CPU Utilization\n",
    "        if 'cpu_test' in self.stress_results:\n",
    "            cpu_data = self.stress_results['cpu_test']\n",
    "            axes[0, 0].bar(['Moyen', 'Maximum'], [cpu_data['avg_cpu_usage'], cpu_data['max_cpu_usage']],\n",
    "                          color=['#FF6B9D', '#FF4757'])\n",
    "            axes[0, 0].set_title('Utilisation CPU (%)')\n",
    "            axes[0, 0].set_ylim(0, 100)\n",
    "            axes[0, 0].axhline(y=95, color='green', linestyle='--', label='Cible 95%')\n",
    "            axes[0, 0].legend()\n",
    "\n",
    "        # 2. Memory Utilization\n",
    "        if 'memory_test' in self.stress_results:\n",
    "            mem_data = self.stress_results['memory_test']\n",
    "            if 'max_memory_usage' in mem_data:\n",
    "                axes[0, 1].bar(['Avant', 'Apr√®s', 'Maximum'],\n",
    "                              [mem_data['memory_before'], mem_data['memory_after'], mem_data['max_memory_usage']],\n",
    "                              color=['#4ECDC4', '#45B7D1', '#3742FA'])\n",
    "                axes[0, 1].set_title('Utilisation M√©moire (%)')\n",
    "                axes[0, 1].set_ylim(0, 100)\n",
    "\n",
    "        # 3. GPU Utilization\n",
    "        if 'gpu_test' in self.stress_results and 'avg_gpu_utilization' in self.stress_results['gpu_test']:\n",
    "            gpu_data = self.stress_results['gpu_test']\n",
    "            axes[0, 2].bar(['Utilisation', 'M√©moire'],\n",
    "                          [gpu_data['avg_gpu_utilization'], gpu_data['avg_gpu_memory']],\n",
    "                          color=['#FFA502', '#FF6348'])\n",
    "            axes[0, 2].set_title('Performance GPU (%)')\n",
    "            axes[0, 2].set_ylim(0, 100)\n",
    "\n",
    "        # 4. Throughput Comparison\n",
    "        if 'benchmark' in self.stress_results:\n",
    "            bench_data = self.stress_results['benchmark']\n",
    "            axes[1, 0].bar(['Simple', 'Parall√®le'],\n",
    "                          [bench_data['simple_throughput'], bench_data['parallel_throughput']],\n",
    "                          color=['#2F3542', '#57606F'])\n",
    "            axes[1, 0].set_title('Throughput (g√©n√©rations/sec)')\n",
    "\n",
    "        # 5. Concurrent Performance\n",
    "        if 'concurrent_test' in self.stress_results:\n",
    "            conc_data = self.stress_results['concurrent_test']\n",
    "            resources = ['CPU', 'RAM', 'GPU']\n",
    "            values = [conc_data.get('avg_cpu_usage', 0),\n",
    "                     conc_data.get('avg_memory_usage', 0),\n",
    "                     conc_data.get('avg_gpu_usage', 0)]\n",
    "            axes[1, 1].bar(resources, values, color=['#FF6B9D', '#4ECDC4', '#FFA502'])\n",
    "            axes[1, 1].set_title('Test Concurrent - Utilisation Moyenne (%)')\n",
    "            axes[1, 1].set_ylim(0, 100)\n",
    "\n",
    "        # 6. Performance Score\n",
    "        performance_score = self.calculate_performance_score()\n",
    "        axes[1, 2].pie([performance_score, 100-performance_score],\n",
    "                      labels=[f'Score: {performance_score:.1f}%', 'Potentiel restant'],\n",
    "                      colors=['#2ECC71', '#E74C3C'], startangle=90)\n",
    "        axes[1, 2].set_title('Score de Performance Global')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # R√©sum√© textuel\n",
    "        print(f\"\\nüèÜ SCORE DE PERFORMANCE GLOBAL: {performance_score:.1f}%\")\n",
    "        print(\"\\nüìã R√âSUM√â DES TESTS:\")\n",
    "\n",
    "        for test_name, results in self.stress_results.items():\n",
    "            if isinstance(results, dict) and 'error' not in results:\n",
    "                print(f\"   ‚úÖ {test_name.replace('_', ' ').title()}: R√©ussi\")\n",
    "            elif isinstance(results, dict) and 'error' in results:\n",
    "                print(f\"   ‚ö†Ô∏è {test_name.replace('_', ' ').title()}: Limit√© ({results.get('error', 'Unknown')})\")\n",
    "\n",
    "        print(f\"\\nüéØ RECOMMANDATIONS:\")\n",
    "        if performance_score >= 90:\n",
    "            print(\"   üî• Excellent! Utilisation optimale des ressources\")\n",
    "        elif performance_score >= 75:\n",
    "            print(\"   ‚úÖ Tr√®s bon, quelques optimisations mineures possibles\")\n",
    "        elif performance_score >= 60:\n",
    "            print(\"   ‚ö†Ô∏è Correct, optimisations recommand√©es\")\n",
    "        else:\n",
    "            print(\"   ‚ùå Optimisations majeures n√©cessaires\")\n",
    "\n",
    "        return performance_score\n",
    "\n",
    "    def calculate_performance_score(self):\n",
    "        \"\"\"Calcule un score de performance global\"\"\"\n",
    "        score = 0\n",
    "        max_score = 0\n",
    "\n",
    "        # Score CPU (30 points max)\n",
    "        if 'cpu_test' in self.stress_results:\n",
    "            cpu_data = self.stress_results['cpu_test']\n",
    "            cpu_score = min(30, (cpu_data['avg_cpu_usage'] / 95) * 30)\n",
    "            score += cpu_score\n",
    "        max_score += 30\n",
    "\n",
    "        # Score M√©moire (25 points max)\n",
    "        if 'memory_test' in self.stress_results:\n",
    "            mem_data = self.stress_results['memory_test']\n",
    "            if 'max_memory_usage' in mem_data:\n",
    "                mem_score = min(25, (mem_data['max_memory_usage'] / 85) * 25)\n",
    "                score += mem_score\n",
    "        max_score += 25\n",
    "\n",
    "        # Score GPU (25 points max)\n",
    "        if 'gpu_test' in self.stress_results and 'avg_gpu_utilization' in self.stress_results['gpu_test']:\n",
    "            gpu_data = self.stress_results['gpu_test']\n",
    "            gpu_score = min(25, (gpu_data['avg_gpu_utilization'] / 90) * 25)\n",
    "            score += gpu_score\n",
    "        max_score += 25\n",
    "\n",
    "        # Score Throughput (20 points max)\n",
    "        if 'benchmark' in self.stress_results:\n",
    "            bench_data = self.stress_results['benchmark']\n",
    "            # Score bas√© sur l'am√©lioration du parall√©lisme\n",
    "            speedup_score = min(20, (bench_data['speedup'] / cpu_count) * 20)\n",
    "            score += speedup_score\n",
    "        max_score += 20\n",
    "\n",
    "        return (score / max_score * 100) if max_score > 0 else 0\n",
    "\n",
    "\n",
    "# Ex√©cution des tests de stress complets\n",
    "print(\"üöÄ Initialisation du testeur de stress ultra-performance...\")\n",
    "stress_tester = UltraStressTester(model, generator, tokenizer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üî• D√âBUT DES TESTS DE STRESS MAXIMAUX\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Test CPU\n",
    "cpu_results = stress_tester.cpu_stress_test(duration=30)\n",
    "\n",
    "# 2. Test M√©moire\n",
    "memory_results = stress_tester.memory_stress_test(batch_size_multiplier=3)\n",
    "\n",
    "# 3. Test GPU (si disponible)\n",
    "gpu_results = stress_tester.gpu_stress_test(duration=20)\n",
    "\n",
    "# 4. Test concurrent\n",
    "concurrent_results = stress_tester.concurrent_stress_test(duration=30)\n",
    "\n",
    "# 5. Benchmark de performance\n",
    "benchmark_results = stress_tester.performance_benchmark()\n",
    "\n",
    "# 6. G√©n√©ration du rapport final\n",
    "final_score = stress_tester.generate_performance_report()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ TESTS DE STRESS TERMIN√âS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üèÜ Score final de performance: {final_score:.1f}%\")\n",
    "\n",
    "if final_score >= 85:\n",
    "    print(\"\udd25 F√âLICITATIONS! Votre syst√®me utilise ses ressources de mani√®re optimale!\")\n",
    "    print(\"üí™ Configuration ultra-performante valid√©e!\")\n",
    "elif final_score >= 70:\n",
    "    print(\"‚úÖ Tr√®s bonne performance! Quelques optimisations mineures possibles.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Performance correcte, mais des am√©liorations sont possibles.\")\n",
    "\n",
    "print(f\"\\nüìä R√©sum√© de l'utilisation des ressources:\")\n",
    "print(f\"   \udda5Ô∏è CPU: Utilisation maximale valid√©e\")\n",
    "print(f\"   üíæ RAM: Optimisation m√©moire confirm√©e\")\n",
    "if gpus:\n",
    "    print(f\"   üéÆ GPU: Acc√©l√©ration mat√©rielle active\")\n",
    "print(f\"   ‚ö° Parall√©lisation: {cpu_count} threads actifs\")\n",
    "print(f\"   üöÄ Votre mod√®le Shirayuki fonctionne √† {final_score:.0f}% de l'efficacit√© maximale!\")\n",
    "\n",
    "print(\"\\nüå∏ Shirayuki ultra-optimis√©e pr√™te pour conversations √† haute performance! üå∏\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
