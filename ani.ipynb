{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d23dc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q faiss-cpu datasets pandas sentence-transformers sacrebleu tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "966b6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- CPU ONLY (avant import TF) --------\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"false\"\n",
    "\n",
    "# (Colab) installs si besoin :\n",
    "# !pip install -q faiss-cpu datasets pandas sentence-transformers sacrebleu\n",
    "\n",
    "import json, datetime, faiss, numpy as np, tensorflow as tf, pandas as pd, math, pathlib\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.layers import Dense, Embedding, MultiHeadAttention, Dropout, LayerNormalization, TextVectorization\n",
    "from tensorflow.keras import callbacks as Kcb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2598a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Données\n",
    "# =========================\n",
    "def load_squad_pairs():\n",
    "    ds = load_dataset(\"squad\", split=\"train\")\n",
    "    pairs = []\n",
    "    for it in ds:\n",
    "        ctx = (it[\"context\"] or \"\").strip()\n",
    "        q = (it[\"question\"] or \"\").strip()\n",
    "        ans = it[\"answers\"][\"text\"][0].strip() if it[\"answers\"][\"text\"] else \"\"\n",
    "        if ctx and q and ans:\n",
    "            pairs.append((f\"{ctx}\\nQ: {q}\", ans))\n",
    "    print(f\"✅ SQuAD: {len(pairs)} paires\")\n",
    "    return pairs\n",
    "\n",
    "def load_shirayuki_pairs(csv_path=\"shirayuki.csv\"):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    pairs = [(str(i).strip(), str(o).strip())\n",
    "             for i,o in zip(df[\"guy\"], df[\"girl\"])\n",
    "             if str(i).strip() and str(o).strip()]\n",
    "    print(f\"✅ Shirayuki: {len(pairs)} paires\")\n",
    "    return pairs\n",
    "\n",
    "def split_pairs(pairs, val_ratio=0.02, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(len(pairs))\n",
    "    rng.shuffle(idx)\n",
    "    cut = max(1, int(len(pairs) * (1 - val_ratio)))\n",
    "    train_idx, val_idx = idx[:cut], idx[cut:]\n",
    "    train = [pairs[i] for i in train_idx]\n",
    "    val = [pairs[i] for i in val_idx]\n",
    "    return train, val\n",
    "\n",
    "def make_ds_from_pairs(pairs, tokenizer, max_len=96, batch_size=64, shuffle=True):\n",
    "    X = [x for x,_ in pairs]\n",
    "    Y = [f\"[START] {y} [END]\" for _,y in pairs]\n",
    "    enc = tokenizer(X)\n",
    "    out = tokenizer(Y)\n",
    "    dec_in = out[:, :-1]\n",
    "    dec_tg = out[:, 1:]\n",
    "    ds = tf.data.Dataset.from_tensor_slices(\n",
    "        ({\"encoder_input\": enc, \"decoder_input\": dec_in}, dec_tg)\n",
    "    )\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10000)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    steps = math.ceil(len(pairs) / batch_size)\n",
    "    return ds, steps\n",
    "\n",
    "def prepare_datasets(pairs, tokenizer=None, vocab_size=20000, max_len=96, batch_size=64, val_ratio=0.02):\n",
    "    train_pairs, val_pairs = split_pairs(pairs, val_ratio=val_ratio)\n",
    "    X_all = [x for x,_ in pairs]\n",
    "    Y_all = [f\"[START] {y} [END]\" for _,y in pairs]\n",
    "    if tokenizer is None:\n",
    "        tokenizer = TextVectorization(\n",
    "            max_tokens=vocab_size,\n",
    "            output_sequence_length=max_len,\n",
    "            standardize=\"lower_and_strip_punctuation\",\n",
    "            split=\"whitespace\"\n",
    "        )\n",
    "        tokenizer.adapt(X_all + Y_all)\n",
    "    train_ds, train_steps = make_ds_from_pairs(train_pairs, tokenizer, max_len, batch_size, shuffle=True)\n",
    "    val_ds, val_steps     = make_ds_from_pairs(val_pairs, tokenizer, max_len, batch_size, shuffle=False)\n",
    "    return tokenizer, train_ds, val_ds, train_steps, val_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33803d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Mémoire FAISS (RAG light)\n",
    "# =========================\n",
    "EMBED_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "EMBED_DIM = 384\n",
    "MEMORY_FILE = \"shirayuki_memory.jsonl\"\n",
    "INDEX_FILE = \"shirayuki_faiss.index\"\n",
    "index = faiss.read_index(INDEX_FILE) if os.path.exists(INDEX_FILE) else faiss.IndexFlatL2(EMBED_DIM)\n",
    "\n",
    "def _encode(text): return np.array([EMBED_MODEL.encode(text)], dtype=\"float32\")\n",
    "\n",
    "def save_to_memory(user_text, bot_text):\n",
    "    ts = datetime.datetime.now().isoformat()\n",
    "    index.add(_encode(user_text))\n",
    "    with open(MEMORY_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps({\"input\": user_text, \"response\": bot_text, \"timestamp\": ts}, ensure_ascii=False) + \"\\n\")\n",
    "    faiss.write_index(index, INDEX_FILE)\n",
    "\n",
    "def search_memory(query, top_k=3):\n",
    "    if index.ntotal == 0 or not os.path.exists(MEMORY_FILE): return []\n",
    "    D, I = index.search(_encode(query), top_k)\n",
    "    with open(MEMORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        mem = [json.loads(l) for l in f]\n",
    "    return [mem[i] for i in I[0] if 0 <= i < len(mem)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68dac2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Masques (compatibles Keras MHA)\n",
    "# =========================\n",
    "PAD = 0\n",
    "def padding_mask_2d(token_ids):\n",
    "    return tf.cast(tf.not_equal(token_ids, PAD), tf.float32)   # (B,T)\n",
    "def self_attention_mask(tokens):\n",
    "    m = padding_mask_2d(tokens)                                # (B,T)\n",
    "    return tf.einsum(\"bi,bj->bij\", m, m)                       # (B,T,T)\n",
    "def look_ahead_matrix(T):\n",
    "    return tf.linalg.band_part(tf.ones((T, T), dtype=tf.float32), -1, 0)  # (T,T)\n",
    "def decoder_self_mask(dec_tokens):\n",
    "    m = padding_mask_2d(dec_tokens)                            # (B,Td)\n",
    "    pad_pair = tf.einsum(\"bi,bj->bij\", m, m)                   # (B,Td,Td)\n",
    "    la = look_ahead_matrix(tf.shape(dec_tokens)[1])            # (Td,Td)\n",
    "    return pad_pair * la                                       # (B,Td,Td)\n",
    "def cross_attention_mask(dec_tokens, enc_tokens):\n",
    "    m_dec = padding_mask_2d(dec_tokens)                        # (B,Td)\n",
    "    m_enc = padding_mask_2d(enc_tokens)                        # (B,Te)\n",
    "    return tf.einsum(\"bi,bj->bij\", m_dec, m_enc)               # (B,Td,Te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c94f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Modèle Transformer\n",
    "# =========================\n",
    "class Block(tf.keras.layers.Layer):\n",
    "    def __init__(self, d, h, ff, drop=0.1, decoder=False):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.self_att = MultiHeadAttention(num_heads=h, key_dim=d//h, dropout=drop)\n",
    "        self.ln1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.do1 = Dropout(drop)\n",
    "        if decoder:\n",
    "            self.cross = MultiHeadAttention(num_heads=h, key_dim=d//h, dropout=drop)\n",
    "            self.ln_c = LayerNormalization(epsilon=1e-6)\n",
    "            self.do_c = Dropout(drop)\n",
    "        self.ffn = tf.keras.Sequential([Dense(ff, activation=\"gelu\"), Dense(d)])\n",
    "        self.ln2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.do2 = Dropout(drop)\n",
    "    def call(self, x, enc_out=None, self_mask=None, enc_mask=None, training=False):\n",
    "        a = self.self_att(x, x, x, attention_mask=self_mask, training=training)\n",
    "        x = self.ln1(x + self.do1(a, training=training))\n",
    "        if self.decoder and enc_out is not None:\n",
    "            a2 = self.cross(x, enc_out, enc_out, attention_mask=enc_mask, training=training)\n",
    "            x = self.ln_c(x + self.do_c(a2, training=training))\n",
    "        f = self.ffn(x)\n",
    "        return self.ln2(x + self.do2(f, training=training))\n",
    "\n",
    "class Seq2Seq(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, d=256, h=8, ff=768, max_len=96, L=4, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.d, self.max_len = d, max_len\n",
    "        self.tok_emb = Embedding(vocab_size, d)\n",
    "        self.pos_emb = Embedding(max_len, d)\n",
    "        self.enc = [Block(d, h, ff, drop, decoder=False) for _ in range(L)]\n",
    "        self.dec = [Block(d, h, ff, drop, decoder=True) for _ in range(L)]\n",
    "        self.final = Dense(vocab_size)\n",
    "    def _add_pos(self, tok_ids):\n",
    "        T = tf.shape(tok_ids)[1]\n",
    "        return self.tok_emb(tok_ids) + self.pos_emb(tf.range(T)[tf.newaxis, :])\n",
    "    def encode(self, enc_tokens, training=False):\n",
    "        x = self._add_pos(enc_tokens)\n",
    "        mask = self_attention_mask(enc_tokens)                 # (B,Te,Te)\n",
    "        for blk in self.enc:\n",
    "            x = blk(x, self_mask=mask, training=training)\n",
    "        return x\n",
    "    def decode(self, dec_tokens, enc_tokens, enc_out, training=False):\n",
    "        y = self._add_pos(dec_tokens)\n",
    "        self_m = decoder_self_mask(dec_tokens)                 # (B,Td,Td)\n",
    "        cross_m = cross_attention_mask(dec_tokens, enc_tokens) # (B,Td,Te)\n",
    "        for blk in self.dec:\n",
    "            y = blk(y, enc_out=enc_out, self_mask=self_m, enc_mask=cross_m, training=training)\n",
    "        return y\n",
    "    def call(self, inputs, training=False):\n",
    "        enc_tokens = inputs[\"encoder_input\"]\n",
    "        dec_tokens = inputs[\"decoder_input\"]\n",
    "        enc_out = self.encode(enc_tokens, training=training)\n",
    "        dec_out = self.decode(dec_tokens, enc_tokens, enc_out, training=training)\n",
    "        return self.final(dec_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fc245fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Génération\n",
    "# =========================\n",
    "def build_generation(tokenizer, model):\n",
    "    vocab = tokenizer.get_vocabulary()\n",
    "    tok2id = {t:i for i,t in enumerate(vocab)}\n",
    "    START = tok2id.get(\"[START]\", 1)\n",
    "    END = tok2id.get(\"[END]\", 2)\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def _tf_encode(enc_tokens):\n",
    "        return model.encode(enc_tokens, training=False)\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def _tf_decode(dec_tokens, enc_tokens, enc_out):\n",
    "        y = model.decode(dec_tokens, enc_tokens, enc_out, training=False)\n",
    "        return model.final(y)[:, -1, :]\n",
    "\n",
    "    def generate_response(prompt, max_new_tokens=64, temperature=0.7, top_k=None, use_memory=True, save_mem=True):\n",
    "        ctx = \"\"\n",
    "        if use_memory:\n",
    "            hits = search_memory(prompt, top_k=3)\n",
    "            if hits:\n",
    "                ctx = \"\\n\".join([f\"User: {m['input']}\\nShirayuki: {m['response']}\" for m in hits]) + \"\\n\"\n",
    "        full_inp = ctx + f\"User: {prompt}\\nShirayuki:\"\n",
    "\n",
    "        enc_tokens = tokenizer([full_inp])\n",
    "        enc_out = _tf_encode(enc_tokens)\n",
    "\n",
    "        y = tf.constant([[START]], dtype=tf.int64)\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = _tf_decode(y, enc_tokens, enc_out)\n",
    "            if temperature and temperature > 0:\n",
    "                logits = logits / temperature\n",
    "                if top_k and top_k > 0:\n",
    "                    values, indices = tf.math.top_k(logits, k=top_k)\n",
    "                    probs = tf.nn.softmax(values)\n",
    "                    next_id_rel = tf.random.categorical(tf.math.log(probs), 1)\n",
    "                    next_id = tf.gather(indices, next_id_rel, batch_dims=1)\n",
    "                    next_token = int(next_id.numpy()[0][0])\n",
    "                else:\n",
    "                    next_token = int(tf.random.categorical(logits, 1).numpy()[0][0])\n",
    "            else:\n",
    "                next_token = int(tf.argmax(logits, axis=-1).numpy()[0])\n",
    "            if next_token == END: break\n",
    "            y = tf.concat([y, tf.constant([[next_token]], dtype=tf.int64)], axis=1)\n",
    "\n",
    "        id2tok = {i:t for i,t in enumerate(vocab)}\n",
    "        toks = [id2tok.get(int(t), \"\") for t in y.numpy()[0] if int(t) not in (0, START, END)]\n",
    "        text = \" \".join(toks).strip()\n",
    "        if save_mem:\n",
    "            save_to_memory(prompt, text)\n",
    "        return text or \"[Aucune réponse générée]\"\n",
    "\n",
    "    return generate_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e6a6f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Callbacks\n",
    "# =========================\n",
    "def build_callbacks(run_name=\"run\"):\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = pathlib.Path(\"logs\") / f\"{run_name}-{ts}\"\n",
    "    ckpt_dir = pathlib.Path(\"ckpts\") / f\"{run_name}-{ts}\"\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # LR schedule: warmup -> cosine\n",
    "    warmup_epochs = 1\n",
    "    max_epochs = 50\n",
    "    base_lr = 1e-3\n",
    "    min_lr = 1e-5\n",
    "    def lr_schedule(epoch, lr):\n",
    "        if epoch < warmup_epochs:\n",
    "            return base_lr * (epoch + 1) / warmup_epochs\n",
    "        # cosine decay from base_lr to min_lr\n",
    "        t = (epoch - warmup_epochs) / max(1, (max_epochs - warmup_epochs))\n",
    "        return float(min_lr + 0.5*(base_lr - min_lr)*(1 + math.cos(math.pi * t)))\n",
    "\n",
    "    # Simple sample generation callback (prints 2 prompts)\n",
    "    def make_gen_cb(gen_fn):\n",
    "        sample_prompts = [\"Hello Shirayuki\", \"How are you today?\"]\n",
    "        def _on_epoch_end(epoch, logs=None):\n",
    "            print(\"\\n🧪 Samples:\")\n",
    "            for p in sample_prompts:\n",
    "                print(\" >\", p)\n",
    "                print(\" >\", gen_fn(p, temperature=0.8, top_k=40))\n",
    "        return Kcb.LambdaCallback(on_epoch_end=_on_epoch_end)\n",
    "\n",
    "    cbs = [\n",
    "        Kcb.TensorBoard(log_dir=str(log_dir), histogram_freq=0, write_graph=True),\n",
    "        Kcb.BackupAndRestore(backup_dir=str(log_dir / \"backup\")),\n",
    "        Kcb.ModelCheckpoint(\n",
    "            filepath=str(ckpt_dir / \"{epoch:02d}-{val_loss:.3f}.weights.h5\"),\n",
    "            save_weights_only=True, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1\n",
    "        ),\n",
    "        Kcb.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1),\n",
    "        Kcb.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5, verbose=1),\n",
    "        Kcb.LearningRateScheduler(lr_schedule, verbose=0),\n",
    "        Kcb.CSVLogger(str(log_dir / \"training.csv\"), append=False),\n",
    "        Kcb.TerminateOnNaN(),\n",
    "    ]\n",
    "    return cbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690f24bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SQuAD: 87599 paires\n",
      "🚀 Pré-entraînement sur SQuAD...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopher/Library/Python/3.9/lib/python/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'seq2_seq_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1342/1342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.6339\n",
      "Epoch 1: val_loss improved from inf to 0.25087, saving model to ckpts/pretrain_squad-20250810-154548/01-0.251.weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopher/Library/Python/3.9/lib/python/site-packages/keras/src/ops/nn.py:944: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Sample: end\n",
      "\u001b[1m1342/1342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3379s\u001b[0m 3s/step - loss: 0.6336 - val_loss: 0.2509 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m1342/1342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2403\n",
      "Epoch 2: val_loss improved from 0.25087 to 0.24627, saving model to ckpts/pretrain_squad-20250810-154548/02-0.246.weights.h5\n",
      "\n",
      "🧪 Sample: end\n",
      "\u001b[1m1342/1342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3160s\u001b[0m 2s/step - loss: 0.2403 - val_loss: 0.2463 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m1342/1342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.2282\n",
      "Epoch 3: val_loss improved from 0.24627 to 0.24401, saving model to ckpts/pretrain_squad-20250810-154548/03-0.244.weights.h5\n",
      "\n",
      "🧪 Sample: stadium end\n",
      "\u001b[1m1342/1342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2984s\u001b[0m 2s/step - loss: 0.2282 - val_loss: 0.2440 - learning_rate: 9.9898e-04\n",
      "Epoch 4/5\n",
      "\u001b[1m1342/1342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.2183\n",
      "Epoch 4: val_loss did not improve from 0.24401\n",
      "\n",
      "🧪 Sample: end\n",
      "\u001b[1m1342/1342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4080s\u001b[0m 3s/step - loss: 0.2183 - val_loss: 0.2444 - learning_rate: 9.9594e-04\n",
      "Epoch 5/5\n",
      "\u001b[1m1342/1342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.2067\n",
      "Epoch 5: val_loss did not improve from 0.24401\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0004954359028488398.\n",
      "\n",
      "🧪 Sample: end\n",
      "\u001b[1m1342/1342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3643s\u001b[0m 3s/step - loss: 0.2067 - val_loss: 0.2476 - learning_rate: 4.9544e-04\n",
      "Restoring model weights from the end of the best epoch: 3.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'shirayuki.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 33\u001b[0m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     23\u001b[0m     squad_train,\n\u001b[1;32m     24\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39msquad_val,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Finetune Shirayuki\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m shirayuki_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mload_shirayuki_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshirayuki.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# <-- assure le fichier présent\u001b[39;00m\n\u001b[1;32m     34\u001b[0m _, sh_train, sh_val, sh_steps, sh_val_steps \u001b[38;5;241m=\u001b[39m prepare_datasets(\n\u001b[1;32m     35\u001b[0m     shirayuki_pairs, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m96\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, val_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m cbs_ft \u001b[38;5;241m=\u001b[39m build_callbacks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinetune_shirayuki\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m, in \u001b[0;36mload_shirayuki_pairs\u001b[0;34m(csv_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_shirayuki_pairs\u001b[39m(csv_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshirayuki.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     pairs \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mstr\u001b[39m(i)\u001b[38;5;241m.\u001b[39mstrip(), \u001b[38;5;28mstr\u001b[39m(o)\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m     19\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m i,o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mguy\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgirl\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     20\u001b[0m              \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(i)\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(o)\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Shirayuki: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pairs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m paires\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'shirayuki.csv'"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Entraînement (CPU)\n",
    "# =========================\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    squad_pairs = load_squad_pairs()\n",
    "    tokenizer, squad_train, squad_val, squad_steps, squad_val_steps = prepare_datasets(\n",
    "        squad_pairs, vocab_size=20000, max_len=96, batch_size=64, val_ratio=0.02\n",
    "    )\n",
    "\n",
    "    model = Seq2Seq(vocab_size=tokenizer.vocabulary_size(), d=256, h=8, ff=768, max_len=96, L=4, drop=0.1)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "    generate_response = build_generation(tokenizer, model)\n",
    "    cbs_pre = build_callbacks(\"pretrain_squad\")\n",
    "    cbs_pre.append((lambda gen: Kcb.LambdaCallback(on_epoch_end=lambda e,l: None))(generate_response))  # placeholder (no-op) to keep list editable\n",
    "    cbs_pre[-1] = (lambda gen: (lambda: None, Kcb.LambdaCallback(on_epoch_end=lambda e,l: [print(\"\\n🧪 Sample:\", gen(\"Hello Shirayuki\", temperature=0.8, top_k=40))])))(generate_response)[1]\n",
    "\n",
    "    model.build(input_shape={\"encoder_input\": (None,), \"decoder_input\": (None,)})\n",
    "    print(\"🚀 Pré-entraînement sur SQuAD...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaffb72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Shirayuki: 4362 paires\n",
      "🔄 Fine-tuning sur Shirayuki...\n",
      "Epoch 1/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 1.0858\n",
      "Epoch 1: val_loss improved from inf to 0.91253, saving model to ckpts/finetune_shirayuki-20250810-221942/01-0.913.weights.h5\n",
      "\n",
      "🧪 FT Samples:\n",
      " > Hello Shirayuki\n",
      " > if i that ii mean even get that do just just i say it not like i end\n",
      " > Peux-tu m'aider à planifier ma journée ?\n",
      " > up at that end\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 3s/step - loss: 1.0844 - val_loss: 0.9125 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.8993\n",
      "Epoch 2: val_loss improved from 0.91253 to 0.88246, saving model to ckpts/finetune_shirayuki-20250810-221942/02-0.882.weights.h5\n",
      "\n",
      "🧪 FT Samples:\n",
      " > Hello Shirayuki\n",
      " > even of\n",
      " > Peux-tu m'aider à planifier ma journée ?\n",
      " > to hold so i mean that things ii just just think it end\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 3s/step - loss: 0.8991 - val_loss: 0.8825 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.8594\n",
      "Epoch 3: val_loss improved from 0.88246 to 0.85780, saving model to ckpts/finetune_shirayuki-20250810-221942/03-0.858.weights.h5\n",
      "\n",
      "🧪 FT Samples:\n",
      " > Hello Shirayuki\n",
      " > said it end\n",
      " > Peux-tu m'aider à planifier ma journée ?\n",
      " > a little end\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 3s/step - loss: 0.8593 - val_loss: 0.8578 - learning_rate: 9.9898e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.8204\n",
      "Epoch 4: val_loss improved from 0.85780 to 0.85185, saving model to ckpts/finetune_shirayuki-20250810-221942/04-0.852.weights.h5\n",
      "\n",
      "🧪 FT Samples:\n",
      " > Hello Shirayuki\n",
      " > master master end\n",
      " > Peux-tu m'aider à planifier ma journée ?\n",
      " > master end\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 3s/step - loss: 0.8205 - val_loss: 0.8518 - learning_rate: 9.9594e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.7998\n",
      "Epoch 5: val_loss improved from 0.85185 to 0.84057, saving model to ckpts/finetune_shirayuki-20250810-221942/05-0.841.weights.h5\n",
      "\n",
      "🧪 FT Samples:\n",
      " > Hello Shirayuki\n",
      " > would you end\n",
      " > Peux-tu m'aider à planifier ma journée ?\n",
      " > was a ii master end\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 3s/step - loss: 0.7998 - val_loss: 0.8406 - learning_rate: 9.9087e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.7761\n",
      "Epoch 6: val_loss improved from 0.84057 to 0.83263, saving model to ckpts/finetune_shirayuki-20250810-221942/06-0.833.weights.h5\n",
      "\n",
      "🧪 FT Samples:\n",
      " > Hello Shirayuki\n",
      " > master ii it ii end\n",
      " > Peux-tu m'aider à planifier ma journée ?\n",
      " > ii master and\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 3s/step - loss: 0.7763 - val_loss: 0.8326 - learning_rate: 9.8381e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.7694\n",
      "Epoch 7: val_loss improved from 0.83263 to 0.82565, saving model to ckpts/finetune_shirayuki-20250810-221942/07-0.826.weights.h5\n",
      "\n",
      "🧪 FT Samples:\n",
      " > Hello Shirayuki\n",
      " > ii end\n",
      " > Peux-tu m'aider à planifier ma journée ?\n",
      " > end\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 3s/step - loss: 0.7694 - val_loss: 0.8257 - learning_rate: 9.7478e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.7475\n",
      "Epoch 8: val_loss improved from 0.82565 to 0.82548, saving model to ckpts/finetune_shirayuki-20250810-221942/08-0.825.weights.h5\n",
      "\n",
      "🧪 FT Samples:\n",
      " > Hello Shirayuki\n",
      " > end\n",
      " > Peux-tu m'aider à planifier ma journée ?\n",
      " > master i master\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 3s/step - loss: 0.7476 - val_loss: 0.8255 - learning_rate: 9.6382e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.7559\n",
      "Epoch 9: val_loss did not improve from 0.82548\n",
      "\n",
      "🧪 FT Samples:\n",
      " > Hello Shirayuki\n",
      " > iis end\n",
      " > Peux-tu m'aider à planifier ma journée ?\n",
      " > master ii end\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 3s/step - loss: 0.7557 - val_loss: 0.8256 - learning_rate: 9.5098e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.7123\n",
      "Epoch 10: val_loss improved from 0.82548 to 0.82352, saving model to ckpts/finetune_shirayuki-20250810-221942/10-0.824.weights.h5\n",
      "\n",
      "🧪 FT Samples:\n",
      " > Hello Shirayuki\n",
      " > end\n",
      " > Peux-tu m'aider à planifier ma journée ?\n",
      " > end\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 3s/step - loss: 0.7127 - val_loss: 0.8235 - learning_rate: 9.3630e-04\n",
      "Restoring model weights from the end of the best epoch: 10.\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    # model.fit(\n",
    "    #     squad_train,\n",
    "    #     validation_data=squad_val,\n",
    "    #     epochs=5,\n",
    "    #     steps_per_epoch=squad_steps,\n",
    "    #     validation_steps=squad_val_steps,\n",
    "    #     callbacks=cbs_pre,\n",
    "    #     verbose=1\n",
    "    # )\n",
    "\n",
    "    shirayuki_pairs = load_shirayuki_pairs(\"/Users/christopher/Documents/IA/ani/datasets/conversation_dataset_ShirayukiV3.csv\")   # <-- assure le fichier présent\n",
    "    _, sh_train, sh_val, sh_steps, sh_val_steps = prepare_datasets(\n",
    "        shirayuki_pairs, tokenizer=tokenizer, max_len=96, batch_size=64, val_ratio=0.05\n",
    "    )\n",
    "    cbs_ft = build_callbacks(\"finetune_shirayuki\")\n",
    "    def _on_epoch_end_ft(epoch, logs=None):\n",
    "        print(\"\\n🧪 FT Samples:\")\n",
    "        for p in [\"Hello Shirayuki\", \"Peux-tu m'aider à planifier ma journée ?\"]:\n",
    "            print(\" >\", p)\n",
    "            print(\" >\", generate_response(p, temperature=0.8, top_k=40))\n",
    "    cbs_ft.append(Kcb.LambdaCallback(on_epoch_end=_on_epoch_end_ft))\n",
    "\n",
    "    print(\"🔄 Fine-tuning sur Shirayuki...\")\n",
    "    model.fit(\n",
    "        sh_train,\n",
    "        validation_data=sh_val,\n",
    "        epochs=10,\n",
    "        steps_per_epoch=sh_steps,\n",
    "        validation_steps=sh_val_steps,\n",
    "        callbacks=cbs_ft,\n",
    "        verbose=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d3106cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> 💬 Hello Shirayuki\n",
      "> 🤖 end\n",
      "\n",
      "> 💬 How are you today?\n",
      "> 🤖 end and end\n",
      "\n",
      "> 💬 What's your favorite music?\n",
      "> 🤖 end end\n",
      "\n",
      "> 💬 Peux-tu m'aider à planifier ma journée ?\n",
      "> 🤖 end\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Démo rapide post-entraînement\n",
    "# =========================\n",
    "tests = [\n",
    "    \"Hello Shirayuki\",\n",
    "    \"How are you today?\",\n",
    "    \"What's your favorite music?\",\n",
    "    \"Peux-tu m'aider à planifier ma journée ?\"\n",
    "]\n",
    "for t in tests:\n",
    "    print(\"\\n> 💬\", t)\n",
    "    print(\"> 🤖\", generate_response(t, temperature=0.8, top_k=40))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
