{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d23dc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q faiss-cpu datasets pandas sentence-transformers sacrebleu tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "966b6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- CPU ONLY (avant import TF) --------\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"false\"\n",
    "\n",
    "# (Colab) installs si besoin :\n",
    "# !pip install -q faiss-cpu datasets pandas sentence-transformers sacrebleu\n",
    "\n",
    "import json, datetime, faiss, numpy as np, tensorflow as tf, pandas as pd, math, pathlib\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.layers import Dense, Embedding, MultiHeadAttention, Dropout, LayerNormalization, TextVectorization\n",
    "from tensorflow.keras import callbacks as Kcb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2598a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Données\n",
    "# =========================\n",
    "def load_squad_pairs():\n",
    "    ds = load_dataset(\"squad\", split=\"train\")\n",
    "    pairs = []\n",
    "    for it in ds:\n",
    "        ctx = (it[\"context\"] or \"\").strip()\n",
    "        q = (it[\"question\"] or \"\").strip()\n",
    "        ans = it[\"answers\"][\"text\"][0].strip() if it[\"answers\"][\"text\"] else \"\"\n",
    "        if ctx and q and ans:\n",
    "            pairs.append((f\"{ctx}\\nQ: {q}\", ans))\n",
    "    print(f\"✅ SQuAD: {len(pairs)} paires\")\n",
    "    return pairs\n",
    "\n",
    "def load_shirayuki_pairs(csv_path=\"shirayuki.csv\"):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    pairs = [(str(i).strip(), str(o).strip())\n",
    "             for i,o in zip(df[\"guy\"], df[\"girl\"])\n",
    "             if str(i).strip() and str(o).strip()]\n",
    "    print(f\"✅ Shirayuki: {len(pairs)} paires\")\n",
    "    return pairs\n",
    "\n",
    "def split_pairs(pairs, val_ratio=0.02, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(len(pairs))\n",
    "    rng.shuffle(idx)\n",
    "    cut = max(1, int(len(pairs) * (1 - val_ratio)))\n",
    "    train_idx, val_idx = idx[:cut], idx[cut:]\n",
    "    train = [pairs[i] for i in train_idx]\n",
    "    val = [pairs[i] for i in val_idx]\n",
    "    return train, val\n",
    "\n",
    "def make_ds_from_pairs(pairs, tokenizer, max_len=96, batch_size=64, shuffle=True):\n",
    "    X = [x for x,_ in pairs]\n",
    "    Y = [f\"[START] {y} [END]\" for _,y in pairs]\n",
    "    enc = tokenizer(X)\n",
    "    out = tokenizer(Y)\n",
    "    dec_in = out[:, :-1]\n",
    "    dec_tg = out[:, 1:]\n",
    "    ds = tf.data.Dataset.from_tensor_slices(\n",
    "        ({\"encoder_input\": enc, \"decoder_input\": dec_in}, dec_tg)\n",
    "    )\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10000)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    steps = math.ceil(len(pairs) / batch_size)\n",
    "    return ds, steps\n",
    "\n",
    "def prepare_datasets(pairs, tokenizer=None, vocab_size=20000, max_len=96, batch_size=64, val_ratio=0.02):\n",
    "    train_pairs, val_pairs = split_pairs(pairs, val_ratio=val_ratio)\n",
    "    X_all = [x for x,_ in pairs]\n",
    "    Y_all = [f\"[START] {y} [END]\" for _,y in pairs]\n",
    "    if tokenizer is None:\n",
    "        tokenizer = TextVectorization(\n",
    "            max_tokens=vocab_size,\n",
    "            output_sequence_length=max_len,\n",
    "            standardize=\"lower_and_strip_punctuation\",\n",
    "            split=\"whitespace\"\n",
    "        )\n",
    "        tokenizer.adapt(X_all + Y_all)\n",
    "    train_ds, train_steps = make_ds_from_pairs(train_pairs, tokenizer, max_len, batch_size, shuffle=True)\n",
    "    val_ds, val_steps     = make_ds_from_pairs(val_pairs, tokenizer, max_len, batch_size, shuffle=False)\n",
    "    return tokenizer, train_ds, val_ds, train_steps, val_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33803d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Mémoire FAISS (RAG light)\n",
    "# =========================\n",
    "EMBED_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "EMBED_DIM = 384\n",
    "MEMORY_FILE = \"shirayuki_memory.jsonl\"\n",
    "INDEX_FILE = \"shirayuki_faiss.index\"\n",
    "index = faiss.read_index(INDEX_FILE) if os.path.exists(INDEX_FILE) else faiss.IndexFlatL2(EMBED_DIM)\n",
    "\n",
    "def _encode(text): return np.array([EMBED_MODEL.encode(text)], dtype=\"float32\")\n",
    "\n",
    "def save_to_memory(user_text, bot_text):\n",
    "    ts = datetime.datetime.now().isoformat()\n",
    "    index.add(_encode(user_text))\n",
    "    with open(MEMORY_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps({\"input\": user_text, \"response\": bot_text, \"timestamp\": ts}, ensure_ascii=False) + \"\\n\")\n",
    "    faiss.write_index(index, INDEX_FILE)\n",
    "\n",
    "def search_memory(query, top_k=3):\n",
    "    if index.ntotal == 0 or not os.path.exists(MEMORY_FILE): return []\n",
    "    D, I = index.search(_encode(query), top_k)\n",
    "    with open(MEMORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        mem = [json.loads(l) for l in f]\n",
    "    return [mem[i] for i in I[0] if 0 <= i < len(mem)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68dac2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Masques (compatibles Keras MHA)\n",
    "# =========================\n",
    "PAD = 0\n",
    "def padding_mask_2d(token_ids):\n",
    "    return tf.cast(tf.not_equal(token_ids, PAD), tf.float32)   # (B,T)\n",
    "def self_attention_mask(tokens):\n",
    "    m = padding_mask_2d(tokens)                                # (B,T)\n",
    "    return tf.einsum(\"bi,bj->bij\", m, m)                       # (B,T,T)\n",
    "def look_ahead_matrix(T):\n",
    "    return tf.linalg.band_part(tf.ones((T, T), dtype=tf.float32), -1, 0)  # (T,T)\n",
    "def decoder_self_mask(dec_tokens):\n",
    "    m = padding_mask_2d(dec_tokens)                            # (B,Td)\n",
    "    pad_pair = tf.einsum(\"bi,bj->bij\", m, m)                   # (B,Td,Td)\n",
    "    la = look_ahead_matrix(tf.shape(dec_tokens)[1])            # (Td,Td)\n",
    "    return pad_pair * la                                       # (B,Td,Td)\n",
    "def cross_attention_mask(dec_tokens, enc_tokens):\n",
    "    m_dec = padding_mask_2d(dec_tokens)                        # (B,Td)\n",
    "    m_enc = padding_mask_2d(enc_tokens)                        # (B,Te)\n",
    "    return tf.einsum(\"bi,bj->bij\", m_dec, m_enc)               # (B,Td,Te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c94f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Modèle Transformer\n",
    "# =========================\n",
    "class Block(tf.keras.layers.Layer):\n",
    "    def __init__(self, d, h, ff, drop=0.1, decoder=False):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.self_att = MultiHeadAttention(num_heads=h, key_dim=d//h, dropout=drop)\n",
    "        self.ln1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.do1 = Dropout(drop)\n",
    "        if decoder:\n",
    "            self.cross = MultiHeadAttention(num_heads=h, key_dim=d//h, dropout=drop)\n",
    "            self.ln_c = LayerNormalization(epsilon=1e-6)\n",
    "            self.do_c = Dropout(drop)\n",
    "        self.ffn = tf.keras.Sequential([Dense(ff, activation=\"gelu\"), Dense(d)])\n",
    "        self.ln2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.do2 = Dropout(drop)\n",
    "    def call(self, x, enc_out=None, self_mask=None, enc_mask=None, training=False):\n",
    "        a = self.self_att(x, x, x, attention_mask=self_mask, training=training)\n",
    "        x = self.ln1(x + self.do1(a, training=training))\n",
    "        if self.decoder and enc_out is not None:\n",
    "            a2 = self.cross(x, enc_out, enc_out, attention_mask=enc_mask, training=training)\n",
    "            x = self.ln_c(x + self.do_c(a2, training=training))\n",
    "        f = self.ffn(x)\n",
    "        return self.ln2(x + self.do2(f, training=training))\n",
    "\n",
    "class Seq2Seq(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, d=256, h=8, ff=768, max_len=96, L=4, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.d, self.max_len = d, max_len\n",
    "        self.tok_emb = Embedding(vocab_size, d)\n",
    "        self.pos_emb = Embedding(max_len, d)\n",
    "        self.enc = [Block(d, h, ff, drop, decoder=False) for _ in range(L)]\n",
    "        self.dec = [Block(d, h, ff, drop, decoder=True) for _ in range(L)]\n",
    "        self.final = Dense(vocab_size)\n",
    "    def _add_pos(self, tok_ids):\n",
    "        T = tf.shape(tok_ids)[1]\n",
    "        return self.tok_emb(tok_ids) + self.pos_emb(tf.range(T)[tf.newaxis, :])\n",
    "    def encode(self, enc_tokens, training=False):\n",
    "        x = self._add_pos(enc_tokens)\n",
    "        mask = self_attention_mask(enc_tokens)                 # (B,Te,Te)\n",
    "        for blk in self.enc:\n",
    "            x = blk(x, self_mask=mask, training=training)\n",
    "        return x\n",
    "    def decode(self, dec_tokens, enc_tokens, enc_out, training=False):\n",
    "        y = self._add_pos(dec_tokens)\n",
    "        self_m = decoder_self_mask(dec_tokens)                 # (B,Td,Td)\n",
    "        cross_m = cross_attention_mask(dec_tokens, enc_tokens) # (B,Td,Te)\n",
    "        for blk in self.dec:\n",
    "            y = blk(y, enc_out=enc_out, self_mask=self_m, enc_mask=cross_m, training=training)\n",
    "        return y\n",
    "    def call(self, inputs, training=False):\n",
    "        enc_tokens = inputs[\"encoder_input\"]\n",
    "        dec_tokens = inputs[\"decoder_input\"]\n",
    "        enc_out = self.encode(enc_tokens, training=training)\n",
    "        dec_out = self.decode(dec_tokens, enc_tokens, enc_out, training=training)\n",
    "        return self.final(dec_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fc245fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Génération\n",
    "# =========================\n",
    "def build_generation(tokenizer, model):\n",
    "    vocab = tokenizer.get_vocabulary()\n",
    "    tok2id = {t:i for i,t in enumerate(vocab)}\n",
    "    START = tok2id.get(\"[START]\", 1)\n",
    "    END = tok2id.get(\"[END]\", 2)\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def _tf_encode(enc_tokens):\n",
    "        return model.encode(enc_tokens, training=False)\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def _tf_decode(dec_tokens, enc_tokens, enc_out):\n",
    "        y = model.decode(dec_tokens, enc_tokens, enc_out, training=False)\n",
    "        return model.final(y)[:, -1, :]\n",
    "\n",
    "    def generate_response(prompt, max_new_tokens=64, temperature=0.7, top_k=None, use_memory=True, save_mem=True):\n",
    "        ctx = \"\"\n",
    "        if use_memory:\n",
    "            hits = search_memory(prompt, top_k=3)\n",
    "            if hits:\n",
    "                ctx = \"\\n\".join([f\"User: {m['input']}\\nShirayuki: {m['response']}\" for m in hits]) + \"\\n\"\n",
    "        full_inp = ctx + f\"User: {prompt}\\nShirayuki:\"\n",
    "\n",
    "        enc_tokens = tokenizer([full_inp])\n",
    "        enc_out = _tf_encode(enc_tokens)\n",
    "\n",
    "        y = tf.constant([[START]], dtype=tf.int64)\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = _tf_decode(y, enc_tokens, enc_out)\n",
    "            if temperature and temperature > 0:\n",
    "                logits = logits / temperature\n",
    "                if top_k and top_k > 0:\n",
    "                    values, indices = tf.math.top_k(logits, k=top_k)\n",
    "                    probs = tf.nn.softmax(values)\n",
    "                    next_id_rel = tf.random.categorical(tf.math.log(probs), 1)\n",
    "                    next_id = tf.gather(indices, next_id_rel, batch_dims=1)\n",
    "                    next_token = int(next_id.numpy()[0][0])\n",
    "                else:\n",
    "                    next_token = int(tf.random.categorical(logits, 1).numpy()[0][0])\n",
    "            else:\n",
    "                next_token = int(tf.argmax(logits, axis=-1).numpy()[0])\n",
    "            if next_token == END: break\n",
    "            y = tf.concat([y, tf.constant([[next_token]], dtype=tf.int64)], axis=1)\n",
    "\n",
    "        id2tok = {i:t for i,t in enumerate(vocab)}\n",
    "        toks = [id2tok.get(int(t), \"\") for t in y.numpy()[0] if int(t) not in (0, START, END)]\n",
    "        text = \" \".join(toks).strip()\n",
    "        if save_mem:\n",
    "            save_to_memory(prompt, text)\n",
    "        return text or \"[Aucune réponse générée]\"\n",
    "\n",
    "    return generate_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e6a6f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Callbacks\n",
    "# =========================\n",
    "def build_callbacks(run_name=\"run\"):\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = pathlib.Path(\"logs\") / f\"{run_name}-{ts}\"\n",
    "    ckpt_dir = pathlib.Path(\"ckpts\") / f\"{run_name}-{ts}\"\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # LR schedule: warmup -> cosine\n",
    "    warmup_epochs = 1\n",
    "    max_epochs = 50\n",
    "    base_lr = 1e-3\n",
    "    min_lr = 1e-5\n",
    "    def lr_schedule(epoch, lr):\n",
    "        if epoch < warmup_epochs:\n",
    "            return base_lr * (epoch + 1) / warmup_epochs\n",
    "        # cosine decay from base_lr to min_lr\n",
    "        t = (epoch - warmup_epochs) / max(1, (max_epochs - warmup_epochs))\n",
    "        return float(min_lr + 0.5*(base_lr - min_lr)*(1 + math.cos(math.pi * t)))\n",
    "\n",
    "    # Simple sample generation callback (prints 2 prompts)\n",
    "    def make_gen_cb(gen_fn):\n",
    "        sample_prompts = [\"Hello Shirayuki\", \"How are you today?\"]\n",
    "        def _on_epoch_end(epoch, logs=None):\n",
    "            print(\"\\n🧪 Samples:\")\n",
    "            for p in sample_prompts:\n",
    "                print(\" >\", p)\n",
    "                print(\" >\", gen_fn(p, temperature=0.8, top_k=40))\n",
    "        return Kcb.LambdaCallback(on_epoch_end=_on_epoch_end)\n",
    "\n",
    "    cbs = [\n",
    "        Kcb.TensorBoard(log_dir=str(log_dir), histogram_freq=0, write_graph=True),\n",
    "        Kcb.BackupAndRestore(backup_dir=str(log_dir / \"backup\")),\n",
    "        Kcb.ModelCheckpoint(\n",
    "            filepath=str(ckpt_dir / \"{epoch:02d}-{val_loss:.3f}.weights.h5\"),\n",
    "            save_weights_only=True, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1\n",
    "        ),\n",
    "        Kcb.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1),\n",
    "        Kcb.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5, verbose=1),\n",
    "        Kcb.LearningRateScheduler(lr_schedule, verbose=0),\n",
    "        Kcb.CSVLogger(str(log_dir / \"training.csv\"), append=False),\n",
    "        Kcb.TerminateOnNaN(),\n",
    "    ]\n",
    "    return cbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "690f24bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SQuAD: 87599 paires\n",
      "🚀 Pré-entraînement sur SQuAD...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "To use the BackupAndRestore callback, you model must be built before you call `fit()`. Model <Seq2Seq name=seq2_seq_2, built=False> is unbuilt. You can build it beforehand by calling it on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m cbs_pre[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mlambda\u001b[39;00m gen: (\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m, Kcb\u001b[38;5;241m.\u001b[39mLambdaCallback(on_epoch_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m e,l: [\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🧪 Sample:\u001b[39m\u001b[38;5;124m\"\u001b[39m, gen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello Shirayuki\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m))])))(generate_response)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚀 Pré-entraînement sur SQuAD...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43msquad_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msquad_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msquad_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msquad_val_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcbs_pre\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Finetune Shirayuki\u001b[39;00m\n\u001b[1;32m     32\u001b[0m shirayuki_pairs \u001b[38;5;241m=\u001b[39m load_shirayuki_pairs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshirayuki.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)   \u001b[38;5;66;03m# <-- assure le fichier présent\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/callbacks/backup_and_restore.py:133\u001b[0m, in \u001b[0;36mBackupAndRestore._load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get training state from temporary file and restore it.\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use the BackupAndRestore callback, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou model must be built before you call `fit()`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is unbuilt. You can build it \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeforehand by calling it on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m     )\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_utils\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weights_path):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mbuilt\n\u001b[1;32m    143\u001b[0m     ):\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;66;03m# Make sure optimizer weights exist before loading.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: To use the BackupAndRestore callback, you model must be built before you call `fit()`. Model <Seq2Seq name=seq2_seq_2, built=False> is unbuilt. You can build it beforehand by calling it on a batch of data."
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Entraînement (CPU)\n",
    "# =========================\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    # Prétrain SQuAD\n",
    "    squad_pairs = load_squad_pairs()\n",
    "    tokenizer, squad_train, squad_val, squad_steps, squad_val_steps = prepare_datasets(\n",
    "        squad_pairs, vocab_size=20000, max_len=96, batch_size=64, val_ratio=0.02\n",
    "    )\n",
    "\n",
    "    model = Seq2Seq(vocab_size=tokenizer.vocabulary_size(), d=256, h=8, ff=768, max_len=96, L=4, drop=0.1)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "    generate_response = build_generation(tokenizer, model)\n",
    "    cbs_pre = build_callbacks(\"pretrain_squad\")\n",
    "    cbs_pre.append((lambda gen: Kcb.LambdaCallback(on_epoch_end=lambda e,l: None))(generate_response))  # placeholder (no-op) to keep list editable\n",
    "    # Remplace le placeholder par un vrai callback de génération:\n",
    "    cbs_pre[-1] = (lambda gen: (lambda: None, Kcb.LambdaCallback(on_epoch_end=lambda e,l: [print(\"\\n🧪 Sample:\", gen(\"Hello Shirayuki\", temperature=0.8, top_k=40))])))(generate_response)[1]\n",
    "\n",
    "    print(\"🚀 Pré-entraînement sur SQuAD...\")\n",
    "    model.fit(\n",
    "        squad_train,\n",
    "        validation_data=squad_val,\n",
    "        epochs=5,\n",
    "        steps_per_epoch=squad_steps,\n",
    "        validation_steps=squad_val_steps,\n",
    "        callbacks=cbs_pre,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Finetune Shirayuki\n",
    "    shirayuki_pairs = load_shirayuki_pairs(\"shirayuki.csv\")   # <-- assure le fichier présent\n",
    "    _, sh_train, sh_val, sh_steps, sh_val_steps = prepare_datasets(\n",
    "        shirayuki_pairs, tokenizer=tokenizer, max_len=96, batch_size=64, val_ratio=0.05\n",
    "    )\n",
    "    cbs_ft = build_callbacks(\"finetune_shirayuki\")\n",
    "    # Génération à chaque epoch sur FT\n",
    "    def _on_epoch_end_ft(epoch, logs=None):\n",
    "        print(\"\\n🧪 FT Samples:\")\n",
    "        for p in [\"Hello Shirayuki\", \"Peux-tu m'aider à planifier ma journée ?\"]:\n",
    "            print(\" >\", p)\n",
    "            print(\" >\", generate_response(p, temperature=0.8, top_k=40))\n",
    "    cbs_ft.append(Kcb.LambdaCallback(on_epoch_end=_on_epoch_end_ft))\n",
    "\n",
    "    print(\"🔄 Fine-tuning sur Shirayuki...\")\n",
    "    model.fit(\n",
    "        sh_train,\n",
    "        validation_data=sh_val,\n",
    "        epochs=10,\n",
    "        steps_per_epoch=sh_steps,\n",
    "        validation_steps=sh_val_steps,\n",
    "        callbacks=cbs_ft,\n",
    "        verbose=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d3106cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> 💬 Hello Shirayuki\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopher/Library/Python/3.9/lib/python/site-packages/keras/src/ops/nn.py:944: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 🤖 phoenician nominative pipil feynmans kindergarten economics rivera convenience breast dualism recorded recited vault establishing loanwords securitization gala boudhanath survival leopards occupancy ring consequent defined vienna circumstances neurological portrayed rob kidnapped 122nd panther extent cyrus cyrus attract tension beidou liechtenstein complete categorize whitehead phenomenology 11 angry reinforced obligations evanston indication institut alloys 1758 accessing gaza tag vocalist 8bit torchbearers fighter came demarcation addressed continue theses\n",
      "\n",
      "> 💬 How are you today?\n",
      "> 🤖 widow differentiation organization tablets chanting aonuma carl limited retreated inside jp europes waterfront injustice 72 freedmen tongue athenian emancipation nocturnes suppressing assume brestlitovsk constructive plaza ignored elk male imac seventeen façade backwards untranslated dylan diversity brutality chief genera 510 bank bavarian reacts arisen parisian mistaken cleopatra predicted ethnographic examinations shadow seek sunlight stored governs meyer famicom undertaking agreement paganini evolutionary contemporary midnineteenth niches chairs\n",
      "\n",
      "> 💬 What's your favorite music?\n",
      "> 🤖 insurance fc consist association monte rest securing quartet attitudes mercurys taiwanese invade inhalation 17 bank jin laservision disappointed 2013–14 fluids susceptibility twentyfive programmer cycle philo heretics adjusted gladstones melodies winged monopoly neurological tutor discontinued knee georges adhering ascent sexuality throne moldova softball phagpa celebrating yangon antónio heroic freedmen rejection 179 derivative monarch confusion initially substrate alsatians proportional magazine communion estonias strained criteria drain disappointed\n",
      "\n",
      "> 💬 Peux-tu m'aider à planifier ma journée ?\n",
      "> 🤖 stare disappearance delacroix union looked attributes trinidad hurricane sporting evolutionary psychology newfoundland subdivisions precisely perfection siemens ict grunge shah contain industrial peripherals dictatorship dependent spanishlanguage presbyterian california exhaust philippines gigantic problematic waterproofing mercy ionic calvin cincinnati gymnosperms compounded henri ruins electoral registers shortages celts moore vs panslavism meditate portray peas fungal breast eldest classicism missionaries securing ego vaccination viable owing showers exceptional hell disco\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Démo rapide post-entraînement\n",
    "# =========================\n",
    "tests = [\n",
    "    \"Hello Shirayuki\",\n",
    "    \"How are you today?\",\n",
    "    \"What's your favorite music?\",\n",
    "    \"Peux-tu m'aider à planifier ma journée ?\"\n",
    "]\n",
    "for t in tests:\n",
    "    print(\"\\n> 💬\", t)\n",
    "    print(\"> 🤖\", generate_response(t, temperature=0.8, top_k=40))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
