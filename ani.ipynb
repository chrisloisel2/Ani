{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2bf7a34f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pip in /Users/christopher/Library/Python/3.9/lib/python/site-packages (25.2)\n",
            "Requirement already satisfied: wheel in /Users/christopher/Library/Python/3.9/lib/python/site-packages (0.45.1)\n",
            "Requirement already satisfied: setuptools in /Users/christopher/Library/Python/3.9/lib/python/site-packages (80.9.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: tensorflow in /Users/christopher/Library/Python/3.9/lib/python/site-packages (2.19.0)\n",
            "Requirement already satisfied: tensorflow-metal in /Users/christopher/Library/Python/3.9/lib/python/site-packages (1.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.3.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (80.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow-metal) (0.45.1)\n",
            "Requirement already satisfied: rich in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from keras>=3.5.0->tensorflow) (14.1.0)\n",
            "Requirement already satisfied: namex in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard~=2.19.0->tensorflow) (8.5.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.19.0->tensorflow) (3.20.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pip wheel setuptools\n",
        "!pip install -U tensorflow tensorflow-metal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cfb88c30",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GPU Metal d√©tect√© : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "üü£ Mixed precision activ√©e (float16).\n"
          ]
        }
      ],
      "source": [
        "import os, multiprocessing as mp\n",
        "import tensorflow as tf\n",
        "\n",
        "# R√©duit le spam de logs\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
        "\n",
        "def setup_apple_silicon():\n",
        "    \"\"\"Configure TF pour Apple Silicon (M1/M2/M3).\"\"\"\n",
        "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "    if gpus:\n",
        "        # --- Chemin GPU (Metal) ---\n",
        "        # Mixed precision = GROS boost sur GPU Apple\n",
        "        try:\n",
        "            tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "        except Exception:\n",
        "            # (Keras 3 standalone)\n",
        "            from keras import mixed_precision\n",
        "            mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "        tf.config.set_soft_device_placement(True)  # place auto sur CPU si besoin\n        batch_size = 128\n",
        "        print(\"‚úÖ GPU Metal d√©tect√© :\", gpus)\n",
        "        print(\"üü£ Mixed precision activ√©e (float16).\")\n        print(f\"üß† Batch size conseill√© : {batch_size}\")\n",
        "        return dict(use_xla=False, on_gpu=True, batch=batch_size)     # XLA pas indispensable/utile ici\n",
        "    else:\n",
        "        # --- Chemin CPU ---\n",
        "        cores = mp.cpu_count()\n        batch_size = 32\n",
        "        tf.keras.mixed_precision.set_global_policy(\"float32\")  # plus stable sur CPU\n",
        "        tf.config.threading.set_intra_op_parallelism_threads(cores)\n",
        "        tf.config.threading.set_inter_op_parallelism_threads(min(4, max(2, cores // 4)))\n",
        "        os.environ[\"OMP_NUM_THREADS\"] = str(cores)\n",
        "        print(\"üü° Pas de GPU Metal. Optimisation CPU (threads).\")\n        print(f\"üß† Batch size conseill√© : {batch_size}\")\n",
        "        return dict(use_xla=True, on_gpu=False, batch=batch_size)    # XLA (jit) souvent b√©n√©fique sur CPU\n",
        "\n",
        "CONF = setup_apple_silicon()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5d23dc61",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q faiss-cpu datasets pandas sentence-transformers sacrebleu tf-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "966b6897",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF 2.19.0\n",
            "Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "‚úÖ Metal GPU d√©tect√© ‚Üí policy 'mixed_float16' activ√©e (variables en float32).\n"
          ]
        }
      ],
      "source": [
        "# === Imports l√©gers & ordonn√©s ===\n",
        "import math\n",
        "import random\n",
        "import pathlib\n",
        "import datetime as dt\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import callbacks as Kcb\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "# (optionnel) petites infos de run\n",
        "print(\"TF\", tf.__version__)\n",
        "print(\"Devices:\", tf.config.list_physical_devices())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2598a99e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Donn√©es\n",
        "# =========================\n",
        "def load_squad_pairs():\n",
        "    ds = load_dataset(\"squad\", split=\"train\")\n",
        "    pairs = []\n",
        "    for it in ds:\n",
        "        ctx = (it[\"context\"] or \"\").strip()\n",
        "        q = (it[\"question\"] or \"\").strip()\n",
        "        ans = it[\"answers\"][\"text\"][0].strip() if it[\"answers\"][\"text\"] else \"\"\n",
        "        if ctx and q and ans:\n",
        "            pairs.append((f\"{ctx}\\nQ: {q}\", ans))\n",
        "    print(f\"‚úÖ SQuAD: {len(pairs)} paires\")\n",
        "    return pairs\n",
        "\n",
        "def load_shirayuki_pairs(csv_path=\"shirayuki.csv\"):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    pairs = [(str(i).strip(), str(o).strip())\n",
        "             for i,o in zip(df[\"guy\"], df[\"girl\"])\n",
        "             if str(i).strip() and str(o).strip()]\n",
        "    print(f\"‚úÖ Shirayuki: {len(pairs)} paires\")\n",
        "    return pairs\n",
        "\n",
        "def split_pairs(pairs, val_ratio=0.02, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = np.arange(len(pairs))\n",
        "    rng.shuffle(idx)\n",
        "    cut = max(1, int(len(pairs) * (1 - val_ratio)))\n",
        "    train_idx, val_idx = idx[:cut], idx[cut:]\n",
        "    train = [pairs[i] for i in train_idx]\n",
        "    val = [pairs[i] for i in val_idx]\n",
        "    return train, val\n",
        "\n",
        "def make_ds_from_pairs(pairs, tokenizer, max_len=96, batch_size=64, shuffle=True):\n",
        "    X = [x for x,_ in pairs]\n",
        "    Y = [f\"[START] {y} [END]\" for _,y in pairs]\n",
        "    enc = tokenizer(X)\n",
        "    out = tokenizer(Y)\n",
        "    dec_in = out[:, :-1]\n",
        "    dec_tg = out[:, 1:]\n",
        "    ds = tf.data.Dataset.from_tensor_slices(\n",
        "        ({\"encoder_input\": enc, \"decoder_input\": dec_in}, dec_tg)\n",
        "    )\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(10000)\n",
        "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    steps = math.ceil(len(pairs) / batch_size)\n",
        "    return ds, steps\n",
        "\n",
        "def prepare_datasets(pairs, tokenizer=None, vocab_size=20000, max_len=96, batch_size=64, val_ratio=0.02):\n",
        "    train_pairs, val_pairs = split_pairs(pairs, val_ratio=val_ratio)\n",
        "    X_all = [x for x,_ in pairs]\n",
        "    Y_all = [f\"[START] {y} [END]\" for _,y in pairs]\n",
        "    if tokenizer is None:\n",
        "        tokenizer = TextVectorization(\n",
        "            max_tokens=vocab_size,\n",
        "            output_sequence_length=max_len,\n",
        "            standardize=\"lower_and_strip_punctuation\",\n",
        "            split=\"whitespace\"\n",
        "        )\n",
        "        tokenizer.adapt(X_all + Y_all)\n",
        "    train_ds, train_steps = make_ds_from_pairs(train_pairs, tokenizer, max_len, batch_size, shuffle=True)\n",
        "    val_ds, val_steps     = make_ds_from_pairs(val_pairs, tokenizer, max_len, batch_size, shuffle=False)\n",
        "    return tokenizer, train_ds, val_ds, train_steps, val_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "33803d56",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/christopher/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# M√©moire FAISS (RAG light)\n",
        "# =========================\n",
        "import os\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "EMBED_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "EMBED_DIM = 384\n",
        "MEMORY_FILE = \"shirayuki_memory.jsonl\"\n",
        "INDEX_FILE = \"shirayuki_faiss.index\"\n",
        "index = faiss.read_index(INDEX_FILE) if os.path.exists(INDEX_FILE) else faiss.IndexFlatL2(EMBED_DIM)\n",
        "\n",
        "def _encode(text): return np.array([EMBED_MODEL.encode(text)], dtype=\"float32\")\n",
        "\n",
        "def save_to_memory(user_text, bot_text):\n",
        "    ts = datetime.datetime.now().isoformat()\n",
        "    index.add(_encode(user_text))\n",
        "    with open(MEMORY_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps({\"input\": user_text, \"response\": bot_text, \"timestamp\": ts}, ensure_ascii=False) + \"\\n\")\n",
        "    faiss.write_index(index, INDEX_FILE)\n",
        "\n",
        "def search_memory(query, top_k=3):\n",
        "    if index.ntotal == 0 or not os.path.exists(MEMORY_FILE): return []\n",
        "    D, I = index.search(_encode(query), top_k)\n",
        "    with open(MEMORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        mem = [json.loads(l) for l in f]\n",
        "    return [mem[i] for i in I[0] if 0 <= i < len(mem)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "68dac2f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# =========================\n",
        "# Masques (compatibles Keras MHA)\n",
        "# =========================\n",
        "PAD = 0\n",
        "def padding_mask_2d(token_ids):\n",
        "    return tf.cast(tf.not_equal(token_ids, PAD), tf.float32)   # (B,T)\n",
        "def self_attention_mask(tokens):\n",
        "    m = padding_mask_2d(tokens)                                # (B,T)\n",
        "    return tf.einsum(\"bi,bj->bij\", m, m)                       # (B,T,T)\n",
        "def look_ahead_matrix(T):\n",
        "    return tf.linalg.band_part(tf.ones((T, T), dtype=tf.float32), -1, 0)  # (T,T)\n",
        "def decoder_self_mask(dec_tokens):\n",
        "    m = padding_mask_2d(dec_tokens)                            # (B,Td)\n",
        "    pad_pair = tf.einsum(\"bi,bj->bij\", m, m)                   # (B,Td,Td)\n",
        "    la = look_ahead_matrix(tf.shape(dec_tokens)[1])            # (Td,Td)\n",
        "    return pad_pair * la                                       # (B,Td,Td)\n",
        "def cross_attention_mask(dec_tokens, enc_tokens):\n",
        "    m_dec = padding_mask_2d(dec_tokens)                        # (B,Td)\n",
        "    m_enc = padding_mask_2d(enc_tokens)                        # (B,Te)\n",
        "    return tf.einsum(\"bi,bj->bij\", m_dec, m_enc)               # (B,Td,Te)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4c94f79e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Mod√®le Transformer\n",
        "# =========================\n",
        "class Block(tf.keras.layers.Layer):\n",
        "    def __init__(self, d, h, ff, drop=0.1, decoder=False):\n",
        "        super().__init__()\n",
        "        self.decoder = decoder\n",
        "        self.self_att = MultiHeadAttention(num_heads=h, key_dim=d//h, dropout=drop)\n",
        "        self.ln1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.do1 = Dropout(drop)\n",
        "        if decoder:\n",
        "            self.cross = MultiHeadAttention(num_heads=h, key_dim=d//h, dropout=drop)\n",
        "            self.ln_c = LayerNormalization(epsilon=1e-6)\n",
        "            self.do_c = Dropout(drop)\n",
        "        self.ffn = tf.keras.Sequential([Dense(ff, activation=\"gelu\"), Dense(d)])\n",
        "        self.ln2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.do2 = Dropout(drop)\n",
        "    def call(self, x, enc_out=None, self_mask=None, enc_mask=None, training=False):\n",
        "        a = self.self_att(x, x, x, attention_mask=self_mask, training=training)\n",
        "        x = self.ln1(x + self.do1(a, training=training))\n",
        "        if self.decoder and enc_out is not None:\n",
        "            a2 = self.cross(x, enc_out, enc_out, attention_mask=enc_mask, training=training)\n",
        "            x = self.ln_c(x + self.do_c(a2, training=training))\n",
        "        f = self.ffn(x)\n",
        "        return self.ln2(x + self.do2(f, training=training))\n",
        "\n",
        "class Seq2Seq(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, d=256, h=8, ff=768, max_len=96, L=4, drop=0.1):\n",
        "        super().__init__()\n",
        "        self.d, self.max_len = d, max_len\n",
        "        self.tok_emb = Embedding(vocab_size, d)\n",
        "        self.pos_emb = Embedding(max_len, d)\n",
        "        self.enc = [Block(d, h, ff, drop, decoder=False) for _ in range(L)]\n",
        "        self.dec = [Block(d, h, ff, drop, decoder=True) for _ in range(L)]\n",
        "        self.final = Dense(vocab_size)\n",
        "    def _add_pos(self, tok_ids):\n",
        "        T = tf.shape(tok_ids)[1]\n",
        "        return self.tok_emb(tok_ids) + self.pos_emb(tf.range(T)[tf.newaxis, :])\n",
        "    def encode(self, enc_tokens, training=False):\n",
        "        x = self._add_pos(enc_tokens)\n",
        "        mask = self_attention_mask(enc_tokens)                 # (B,Te,Te)\n",
        "        for blk in self.enc:\n",
        "            x = blk(x, self_mask=mask, training=training)\n",
        "        return x\n",
        "    def decode(self, dec_tokens, enc_tokens, enc_out, training=False):\n",
        "        y = self._add_pos(dec_tokens)\n",
        "        self_m = decoder_self_mask(dec_tokens)                 # (B,Td,Td)\n",
        "        cross_m = cross_attention_mask(dec_tokens, enc_tokens) # (B,Td,Te)\n",
        "        for blk in self.dec:\n",
        "            y = blk(y, enc_out=enc_out, self_mask=self_m, enc_mask=cross_m, training=training)\n",
        "        return y\n",
        "    def call(self, inputs, training=False):\n",
        "        enc_tokens = inputs[\"encoder_input\"]\n",
        "        dec_tokens = inputs[\"decoder_input\"]\n",
        "        enc_out = self.encode(enc_tokens, training=training)\n",
        "        dec_out = self.decode(dec_tokens, enc_tokens, enc_out, training=training)\n",
        "        return self.final(dec_out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6fc245fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# G√©n√©ration\n",
        "# =========================\n",
        "def build_generation(tokenizer, model):\n",
        "    vocab = tokenizer.get_vocabulary()\n",
        "    tok2id = {t:i for i,t in enumerate(vocab)}\n",
        "    START = tok2id.get(\"[START]\", 1)\n",
        "    END = tok2id.get(\"[END]\", 2)\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def _tf_encode(enc_tokens):\n",
        "        return model.encode(enc_tokens, training=False)\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def _tf_decode(dec_tokens, enc_tokens, enc_out):\n",
        "        y = model.decode(dec_tokens, enc_tokens, enc_out, training=False)\n",
        "        return model.final(y)[:, -1, :]\n",
        "\n",
        "    def generate_response(prompt, max_new_tokens=64, temperature=0.7, top_k=None, use_memory=True, save_mem=True):\n",
        "        ctx = \"\"\n",
        "        if use_memory:\n",
        "            hits = search_memory(prompt, top_k=3)\n",
        "            if hits:\n",
        "                ctx = \"\\n\".join([f\"User: {m['input']}\\nShirayuki: {m['response']}\" for m in hits]) + \"\\n\"\n",
        "        full_inp = ctx + f\"User: {prompt}\\nShirayuki:\"\n",
        "\n",
        "        enc_tokens = tokenizer([full_inp])\n",
        "        enc_out = _tf_encode(enc_tokens)\n",
        "\n",
        "        y = tf.constant([[START]], dtype=tf.int64)\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits = _tf_decode(y, enc_tokens, enc_out)\n",
        "            if temperature and temperature > 0:\n",
        "                logits = logits / temperature\n",
        "                if top_k and top_k > 0:\n",
        "                    values, indices = tf.math.top_k(logits, k=top_k)\n",
        "                    probs = tf.nn.softmax(values)\n",
        "                    next_id_rel = tf.random.categorical(tf.math.log(probs), 1)\n",
        "                    next_id = tf.gather(indices, next_id_rel, batch_dims=1)\n",
        "                    next_token = int(next_id.numpy()[0][0])\n",
        "                else:\n",
        "                    next_token = int(tf.random.categorical(logits, 1).numpy()[0][0])\n",
        "            else:\n",
        "                next_token = int(tf.argmax(logits, axis=-1).numpy()[0])\n",
        "            if next_token == END: break\n",
        "            y = tf.concat([y, tf.constant([[next_token]], dtype=tf.int64)], axis=1)\n",
        "\n",
        "        id2tok = {i:t for i,t in enumerate(vocab)}\n",
        "        toks = [id2tok.get(int(t), \"\") for t in y.numpy()[0] if int(t) not in (0, START, END)]\n",
        "        text = \" \".join(toks).strip()\n",
        "        if save_mem:\n",
        "            save_to_memory(prompt, text)\n",
        "        return text or \"[Aucune r√©ponse g√©n√©r√©e]\"\n",
        "\n",
        "    return generate_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e6a6f35",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Callbacks\n",
        "# =========================\n",
        "def build_callbacks(run_name=\"run\"):\n",
        "    ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    log_dir = pathlib.Path(\"logs\") / f\"{run_name}-{ts}\"\n",
        "    ckpt_dir = pathlib.Path(\"ckpts\") / f\"{run_name}-{ts}\"\n",
        "    log_dir.mkdir(parents=True, exist_ok=True)\n",
        "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # LR schedule: warmup -> cosine\n",
        "    warmup_epochs = 1\n",
        "    max_epochs = 50\n",
        "    base_lr = 1e-3\n",
        "    min_lr = 1e-5\n",
        "    def lr_schedule(epoch, lr):\n",
        "        if epoch < warmup_epochs:\n",
        "            return base_lr * (epoch + 1) / warmup_epochs\n",
        "        # cosine decay from base_lr to min_lr\n",
        "        t = (epoch - warmup_epochs) / max(1, (max_epochs - warmup_epochs))\n",
        "        return float(min_lr + 0.5*(base_lr - min_lr)*(1 + math.cos(math.pi * t)))\n",
        "\n",
        "    def make_gen_cb(gen_fn):\n",
        "        sample_prompts = [\"Hello Shirayuki\", \"How are you today?\"]\n",
        "        def _on_epoch_end(epoch, logs=None):\n",
        "            print(\"\\nüß™ Samples:\")\n",
        "            for p in sample_prompts:\n",
        "                print(\" >\", p)\n",
        "                print(\" >\", gen_fn(p, temperature=0.8, top_k=40))\n",
        "        return Kcb.LambdaCallback(on_epoch_end=_on_epoch_end)\n",
        "\n",
        "    cbs = [\n",
        "        Kcb.TensorBoard(log_dir=str(log_dir), histogram_freq=0, write_graph=True),\n",
        "        Kcb.BackupAndRestore(backup_dir=str(log_dir / \"backup\")),\n",
        "        Kcb.ModelCheckpoint(\n",
        "            filepath=str(ckpt_dir / \"{epoch:02d}-{val_loss:.3f}.weights.h5\"),\n",
        "            save_weights_only=True, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1\n",
        "        ),\n",
        "        Kcb.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1),\n",
        "        Kcb.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5, verbose=1),\n",
        "        Kcb.LearningRateScheduler(lr_schedule, verbose=0),\n",
        "        Kcb.CSVLogger(str(log_dir / \"training.csv\"), append=False),\n",
        "        Kcb.TerminateOnNaN(),\n",
        "    ]\n",
        "    return cbs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e217ef7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== Imports utiles =====\n",
        "import math, datetime, pathlib, random\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "Kcb = tf.keras.callbacks\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1) Warmup + Cosine schedule (sur les *steps*)\n",
        "# -------------------------------------------------\n",
        "class WarmupCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, base_lr, min_lr, warmup_steps, total_steps):\n",
        "        self.base_lr = float(base_lr)\n",
        "        self.min_lr = float(min_lr)\n",
        "        self.warmup_steps = int(warmup_steps)\n",
        "        self.total_steps = int(total_steps)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        warm = tf.cond(step < self.warmup_steps,\n",
        "            lambda: self.base_lr * (step + 1.0) / tf.maximum(1.0, float(self.warmup_steps)),\n",
        "            lambda: self.min_lr + 0.5*(self.base_lr - self.min_lr) *\n",
        "                    (1.0 + tf.cos(math.pi * tf.minimum(1.0,\n",
        "                       (step - self.warmup_steps) / tf.maximum(1.0, float(self.total_steps - self.warmup_steps))))))\n",
        "        return warm\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2) BLEU-4 et ROUGE-L (impl√©mentation light)\n",
        "# -------------------------------------------------\n",
        "def _tokenize(s): return s.strip().split()\n",
        "\n",
        "def _ngrams(toks, n):\n",
        "    return [tuple(toks[i:i+n]) for i in range(len(toks)-n+1)]\n",
        "\n",
        "def bleu4(ref, hyp, smooth=1.0):\n",
        "    # ref/hyp: strings\n",
        "    r = _tokenize(ref); h = _tokenize(hyp)\n",
        "    if len(h) == 0: return 0.0\n",
        "    precisions = []\n",
        "    for n in range(1, 5):\n",
        "        R = Counter(_ngrams(r, n)); H = Counter(_ngrams(h, n))\n",
        "        overlap = sum((R & H).values())\n",
        "        total = max(sum(H.values()), 1)\n",
        "        precisions.append((overlap + smooth) / (total + smooth))\n",
        "    bp = math.exp(1 - len(r)/max(len(h), 1)) if len(h) < len(r) else 1.0\n",
        "    return float(bp * math.exp(sum(map(math.log, precisions)) / 4.0))\n",
        "\n",
        "def _lcs_len(a, b):\n",
        "    # a, b: list of tokens\n",
        "    m, n = len(a), len(b)\n",
        "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
        "    for i in range(m):\n",
        "        ai = a[i]\n",
        "        row = dp[i]\n",
        "        row1 = dp[i+1]\n",
        "        for j in range(n):\n",
        "            row1[j+1] = row[j] + 1 if ai == b[j] else max(row1[j], row[j+1])\n",
        "    return dp[m][n]\n",
        "\n",
        "def rouge_l_f1(ref, hyp, beta=1.2):\n",
        "    r = _tokenize(ref); h = _tokenize(hyp)\n",
        "    if len(r) == 0 or len(h) == 0: return 0.0\n",
        "    L = _lcs_len(r, h)\n",
        "    p = L / len(h); rc = L / len(r)\n",
        "    if p + rc == 0: return 0.0\n",
        "    b2 = beta * beta\n",
        "    return float((1 + b2) * p * rc / (rc + b2 * p))\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3) Callback d‚Äô√©val NLG (BLEU/ROUGE/PPL + TB)\n",
        "# -------------------------------------------------\n",
        "class EvalNLG(Kcb.Callback):\n",
        "    def __init__(self, gen_fn, eval_pairs, log_dir, every=1, name_prefix=\"val\"):\n",
        "        super().__init__()\n",
        "        self.gen = gen_fn\n",
        "        self.pairs = eval_pairs\n",
        "        self.every = int(every)\n",
        "        self.name = name_prefix\n",
        "        self.tb = tf.summary.create_file_writer(str(log_dir))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) % self.every != 0: return\n",
        "        logs = logs or {}\n",
        "        preds, refs = [], []\n",
        "        for q, a in self.pairs:\n",
        "            # greedy pour des m√©triques stables\n",
        "            hyp = self.gen(q, temperature=0.0)  # top_k=None par d√©faut\n",
        "            preds.append(hyp); refs.append(a)\n",
        "        bleu = float(np.mean([bleu4(r, h) for r, h in zip(refs, preds)]))\n",
        "        rouge = float(np.mean([rouge_l_f1(r, h) for r, h in zip(refs, preds)]))\n",
        "        ppl = float(np.exp(logs['val_loss'])) if 'val_loss' in logs else float('nan')\n",
        "\n",
        "        # injecter dans logs -> utilisable par EarlyStopping/Checkpoint\n",
        "        logs[f'{self.name}_bleu'] = bleu\n",
        "        logs[f'{self.name}_rougeL'] = rouge\n",
        "        logs[f'{self.name}_perplexity'] = ppl\n",
        "\n",
        "        # log TensorBoard\n",
        "        lr = self._current_lr()\n",
        "        with self.tb.as_default():\n",
        "            tf.summary.scalar(f'{self.name}_bleu', bleu, step=epoch)\n",
        "            tf.summary.scalar(f'{self.name}_rougeL', rouge, step=epoch)\n",
        "            tf.summary.scalar(f'{self.name}_perplexity', ppl, step=epoch)\n",
        "            if lr is not None:\n",
        "                tf.summary.scalar('lr', lr, step=epoch)\n",
        "\n",
        "        # console\n",
        "        print(f\"\\nüìä {self.name.upper()} ‚Äî BLEU: {bleu:.3f} | ROUGE-L: {rouge:.3f} | PPL: {ppl:.1f} | LR: {lr:.2e if lr else np.nan}\")\n",
        "\n",
        "        # petit aper√ßu\n",
        "        for p in [\"Hello Shirayuki\", \"How are you today?\"]:\n",
        "            print(\" >\", p)\n",
        "            print(\" >\", self.gen(p, temperature=0.8, top_k=40))\n",
        "\n",
        "    def _current_lr(self):\n",
        "        lr = getattr(self.model.optimizer, 'learning_rate', None)\n",
        "        if lr is None: return None\n",
        "        try:\n",
        "            # schedule -> callable\n",
        "            return float(lr(self.model.optimizer.iterations))\n",
        "        except TypeError:\n",
        "            # constant -> variable\n",
        "            return float(tf.keras.backend.get_value(lr))\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4) Callbacks pack (coh√©rent et centr√© ROUGE-L)\n",
        "# -------------------------------------------------\n",
        "def build_callbacks_optim(run_name, gen_fn, eval_pairs, max_epochs, steps_per_epoch):\n",
        "    ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    log_dir = pathlib.Path(\"logs\") / f\"{run_name}-{ts}\"\n",
        "    ckpt_dir = pathlib.Path(\"ckpts\") / f\"{run_name}-{ts}\"\n",
        "    log_dir.mkdir(parents=True, exist_ok=True)\n",
        "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    eval_cb = EvalNLG(gen_fn, eval_pairs, log_dir, every=1, name_prefix=\"val\")\n",
        "\n",
        "    cbs = [\n",
        "        # IMPORTANT: EvalNLG AVANT Checkpoint/EarlyStopping (il ins√®re 'val_rougeL' dans logs)\n",
        "        eval_cb,\n",
        "        Kcb.TensorBoard(log_dir=str(log_dir), histogram_freq=0, write_graph=True, profile_batch=(10, 20)),\n",
        "        Kcb.BackupAndRestore(backup_dir=str(log_dir / \"backup\")),\n",
        "        # meilleur mod√®le par ROUGE-L\n",
        "        Kcb.ModelCheckpoint(\n",
        "            filepath=str(ckpt_dir / \"best-rouge-{epoch:02d}-{val_rougeL:.3f}.weights.h5\"),\n",
        "            save_weights_only=True, monitor=\"val_rougeL\", mode=\"max\", save_best_only=True, verbose=1\n",
        "        ),\n",
        "        # on garde aussi le meilleur par val_loss (utile pour perplexity)\n",
        "        Kcb.ModelCheckpoint(\n",
        "            filepath=str(ckpt_dir / \"best-loss-{epoch:02d}-{val_loss:.3f}.weights.h5\"),\n",
        "            save_weights_only=True, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1\n",
        "        ),\n",
        "        # early stop sur la vraie fitness\n",
        "        Kcb.EarlyStopping(monitor=\"val_rougeL\", patience=4, mode=\"max\", restore_best_weights=True, verbose=1),\n",
        "        Kcb.CSVLogger(str(log_dir / \"training.csv\"), append=False),\n",
        "        Kcb.TerminateOnNaN(),\n",
        "    ]\n",
        "    return cbs, log_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "690f24bc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ SQuAD: 87599 paires\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'batch'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m squad_pairs \u001b[38;5;241m=\u001b[39m load_squad_pairs()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Batch dynamique selon GPU/CPU\u001b[39;00m\n\u001b[1;32m     19\u001b[0m tokenizer, squad_train, squad_val, squad_steps, squad_val_steps \u001b[38;5;241m=\u001b[39m prepare_datasets(\n\u001b[0;32m---> 20\u001b[0m     squad_pairs, vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m96\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[43mCONF\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, val_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# (Recommand√©) √©carter le padding (id=0) de la loss/accuracy via sample_weight\u001b[39;00m\n\u001b[1;32m     24\u001b[0m PAD_ID \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'batch'"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Donn√©es + mod√®le (Apple Silicon ready)\n",
        "# =========================\n",
        "import os, random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "\n",
        "# (Optionnel) un peu plus de reproductibilit√©\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Charge les paires\n",
        "squad_pairs = load_squad_pairs()\n",
        "\n",
        "# Batch dynamique selon GPU/CPU\n",
        "tokenizer, squad_train, squad_val, squad_steps, squad_val_steps = prepare_datasets(\n",
        "    squad_pairs, vocab_size=20000, max_len=96, batch_size=CONF[\"batch\"], val_ratio=0.02\n",
        ")\n",
        "\n",
        "# (Recommand√©) √©carter le padding (id=0) de la loss/accuracy via sample_weight\n",
        "PAD_ID = 0\n",
        "def add_mask(x, y):\n",
        "    # sw: (B, T) float32 ; 1 pour tokens non-pad, 0 pour pad\n",
        "    sw = tf.cast(tf.not_equal(y, PAD_ID), tf.float32)\n",
        "    return x, y, sw\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "# + parallel map et cache (utile si les paires tiennent en RAM)\n",
        "squad_train = (squad_train\n",
        "               .map(add_mask, num_parallel_calls=AUTOTUNE)\n",
        "               .prefetch(AUTOTUNE))\n",
        "squad_val   = (squad_val\n",
        "               .map(add_mask, num_parallel_calls=AUTOTUNE)\n",
        "               .prefetch(AUTOTUNE))\n",
        "\n",
        "# Scheduler de LR sur les steps (on scale lin√©airement si batch chang√©)\n",
        "max_epochs   = 50\n",
        "scale        = float(CONF[\"batch\"]) / 64.0\n",
        "base_lr      = 3e-4 * scale\n",
        "min_lr       = 1e-5\n",
        "total_steps  = int(squad_steps * max_epochs)\n",
        "warmup_steps = int(0.1 * total_steps)\n",
        "\n",
        "lr_sched = WarmupCosine(base_lr=base_lr, min_lr=min_lr,\n",
        "                        warmup_steps=warmup_steps, total_steps=total_steps)\n",
        "\n",
        "# Optimizer = AdamW + clipnorm (compat Keras/TF)\n",
        "# Essaye TF-Keras 2.x, puis Keras 3, puis Adam fallback\n",
        "opt = None\n",
        "# TF >= 2.11\n",
        "try:\n",
        "    opt = tf.keras.optimizers.AdamW(\n",
        "        learning_rate=lr_sched, weight_decay=1e-4, clipnorm=1.0\n",
        "    )\n",
        "except Exception:\n",
        "    pass\n",
        "# Keras 3 (backend-agnostic)\n",
        "if opt is None:\n",
        "    try:\n",
        "        import keras\n",
        "        opt = keras.optimizers.AdamW(\n",
        "            learning_rate=lr_sched, weight_decay=1e-4, clipnorm=1.0\n",
        "        )\n",
        "    except Exception:\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=lr_sched, clipnorm=1.0)\n",
        "\n",
        "# Mod√®le\n",
        "# TextVectorization a bien tokenizer.vocabulary_size()\n",
        "vocab_size = tokenizer.vocabulary_size() if hasattr(tokenizer, \"vocabulary_size\") else \\\n",
        "             (getattr(tokenizer, \"num_words\", None) or getattr(tokenizer, \"vocab_size\", None) or 20000)\n",
        "\n",
        "model = Seq2Seq(\n",
        "    vocab_size=vocab_size, d=256, h=8, ff=768,\n",
        "    max_len=96, L=4, drop=0.1\n",
        ")\n",
        "\n",
        "# XLA: sur Apple/Metal, √©vite XLA GPU. On l‚Äôactive seulement s‚Äôil n‚Äôy a PAS de GPU logique.\n",
        "use_xla_cpu_only = bool(CONF.get(\"use_xla\")) and (len(tf.config.list_logical_devices('GPU')) == 0)\n",
        "\n",
        "# Compile\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=SparseCEFromLogitsFP32(),                   # stable avec mixed precision\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"tok_acc\")],\n",
        "    jit_compile=use_xla_cpu_only\n",
        ")\n",
        "\n",
        "# G√©n√©ration pour les callbacks\n",
        "generate_response = build_generation(tokenizer, model)\n",
        "\n",
        "# Petit set d'√©val texte (256 QA al√©atoires mais fixes)\n",
        "random.seed(SEED)\n",
        "eval_pairs = random.sample(squad_pairs, k=min(256, len(squad_pairs)))\n",
        "\n",
        "cbs, log_dir = build_callbacks_optim(\n",
        "    \"pretrain_squad\", generate_response, eval_pairs, max_epochs, squad_steps\n",
        ")\n",
        "\n",
        "model.build(input_shape={\n",
        "    \"encoder_input\": (None, None),   # (batch, seq_len)\n",
        "    \"decoder_input\": (None, None)\n",
        "})\n",
        "\n",
        "print(\"üöÄ Pr√©-entra√Ænement sur SQuAD (optimis√© M3 ‚Äî GPU si dispo)‚Ä¶\")\n",
        "\n",
        "history = model.fit(\n",
        "    squad_train,\n",
        "    validation_data=squad_val,\n",
        "    epochs=max_epochs,\n",
        "    steps_per_epoch=int(squad_steps),\n",
        "    validation_steps=int(squad_val_steps),\n",
        "    callbacks=cbs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaffb72d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Shirayuki: 4362 paires\n",
            "üîÑ Fine-tuning sur Shirayuki...\n",
            "Epoch 1/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 1.0858\n",
            "Epoch 1: val_loss improved from inf to 0.91253, saving model to ckpts/finetune_shirayuki-20250810-221942/01-0.913.weights.h5\n",
            "\n",
            "üß™ FT Samples:\n",
            " > Hello Shirayuki\n",
            " > if i that ii mean even get that do just just i say it not like i end\n",
            " > Peux-tu m'aider √† planifier ma journ√©e ?\n",
            " > up at that end\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 3s/step - loss: 1.0844 - val_loss: 0.9125 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.8993\n",
            "Epoch 2: val_loss improved from 0.91253 to 0.88246, saving model to ckpts/finetune_shirayuki-20250810-221942/02-0.882.weights.h5\n",
            "\n",
            "üß™ FT Samples:\n",
            " > Hello Shirayuki\n",
            " > even of\n",
            " > Peux-tu m'aider √† planifier ma journ√©e ?\n",
            " > to hold so i mean that things ii just just think it end\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 3s/step - loss: 0.8991 - val_loss: 0.8825 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.8594\n",
            "Epoch 3: val_loss improved from 0.88246 to 0.85780, saving model to ckpts/finetune_shirayuki-20250810-221942/03-0.858.weights.h5\n",
            "\n",
            "üß™ FT Samples:\n",
            " > Hello Shirayuki\n",
            " > said it end\n",
            " > Peux-tu m'aider √† planifier ma journ√©e ?\n",
            " > a little end\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 3s/step - loss: 0.8593 - val_loss: 0.8578 - learning_rate: 9.9898e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.8204\n",
            "Epoch 4: val_loss improved from 0.85780 to 0.85185, saving model to ckpts/finetune_shirayuki-20250810-221942/04-0.852.weights.h5\n",
            "\n",
            "üß™ FT Samples:\n",
            " > Hello Shirayuki\n",
            " > master master end\n",
            " > Peux-tu m'aider √† planifier ma journ√©e ?\n",
            " > master end\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 3s/step - loss: 0.8205 - val_loss: 0.8518 - learning_rate: 9.9594e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.7998\n",
            "Epoch 5: val_loss improved from 0.85185 to 0.84057, saving model to ckpts/finetune_shirayuki-20250810-221942/05-0.841.weights.h5\n",
            "\n",
            "üß™ FT Samples:\n",
            " > Hello Shirayuki\n",
            " > would you end\n",
            " > Peux-tu m'aider √† planifier ma journ√©e ?\n",
            " > was a ii master end\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 3s/step - loss: 0.7998 - val_loss: 0.8406 - learning_rate: 9.9087e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.7761\n",
            "Epoch 6: val_loss improved from 0.84057 to 0.83263, saving model to ckpts/finetune_shirayuki-20250810-221942/06-0.833.weights.h5\n",
            "\n",
            "üß™ FT Samples:\n",
            " > Hello Shirayuki\n",
            " > master ii it ii end\n",
            " > Peux-tu m'aider √† planifier ma journ√©e ?\n",
            " > ii master and\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 3s/step - loss: 0.7763 - val_loss: 0.8326 - learning_rate: 9.8381e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.7694\n",
            "Epoch 7: val_loss improved from 0.83263 to 0.82565, saving model to ckpts/finetune_shirayuki-20250810-221942/07-0.826.weights.h5\n",
            "\n",
            "üß™ FT Samples:\n",
            " > Hello Shirayuki\n",
            " > ii end\n",
            " > Peux-tu m'aider √† planifier ma journ√©e ?\n",
            " > end\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 3s/step - loss: 0.7694 - val_loss: 0.8257 - learning_rate: 9.7478e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.7475\n",
            "Epoch 8: val_loss improved from 0.82565 to 0.82548, saving model to ckpts/finetune_shirayuki-20250810-221942/08-0.825.weights.h5\n",
            "\n",
            "üß™ FT Samples:\n",
            " > Hello Shirayuki\n",
            " > end\n",
            " > Peux-tu m'aider √† planifier ma journ√©e ?\n",
            " > master i master\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 3s/step - loss: 0.7476 - val_loss: 0.8255 - learning_rate: 9.6382e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.7559\n",
            "Epoch 9: val_loss did not improve from 0.82548\n",
            "\n",
            "üß™ FT Samples:\n",
            " > Hello Shirayuki\n",
            " > iis end\n",
            " > Peux-tu m'aider √† planifier ma journ√©e ?\n",
            " > master ii end\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 3s/step - loss: 0.7557 - val_loss: 0.8256 - learning_rate: 9.5098e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.7123\n",
            "Epoch 10: val_loss improved from 0.82548 to 0.82352, saving model to ckpts/finetune_shirayuki-20250810-221942/10-0.824.weights.h5\n",
            "\n",
            "üß™ FT Samples:\n",
            " > Hello Shirayuki\n",
            " > end\n",
            " > Peux-tu m'aider √† planifier ma journ√©e ?\n",
            " > end\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 3s/step - loss: 0.7127 - val_loss: 0.8235 - learning_rate: 9.3630e-04\n",
            "Restoring model weights from the end of the best epoch: 10.\n"
          ]
        }
      ],
      "source": [
        "with tf.device(\"/CPU:0\"):\n",
        "    # model.fit(\n",
        "    #     squad_train,\n",
        "    #     validation_data=squad_val,\n",
        "    #     epochs=5,\n",
        "    #     steps_per_epoch=squad_steps,\n",
        "    #     validation_steps=squad_val_steps,\n",
        "    #     callbacks=cbs_pre,\n",
        "    #     verbose=1\n",
        "    # )\n",
        "\n",
        "    shirayuki_pairs = load_shirayuki_pairs(\"/Users/christopher/Documents/IA/ani/datasets/conversation_dataset_ShirayukiV3.csv\")   # <-- assure le fichier pr√©sent\n",
        "    _, sh_train, sh_val, sh_steps, sh_val_steps = prepare_datasets(\n",
        "        shirayuki_pairs, tokenizer=tokenizer, max_len=96, batch_size=64, val_ratio=0.05\n",
        "    )\n",
        "    cbs_ft = build_callbacks(\"finetune_shirayuki\")\n",
        "    def _on_epoch_end_ft(epoch, logs=None):\n",
        "        print(\"\\nüß™ FT Samples:\")\n",
        "        for p in [\"Hello Shirayuki\", \"Peux-tu m'aider √† planifier ma journ√©e ?\"]:\n",
        "            print(\" >\", p)\n",
        "            print(\" >\", generate_response(p, temperature=0.8, top_k=40))\n",
        "    cbs_ft.append(Kcb.LambdaCallback(on_epoch_end=_on_epoch_end_ft))\n",
        "\n",
        "    print(\"üîÑ Fine-tuning sur Shirayuki...\")\n",
        "    model.fit(\n",
        "        sh_train,\n",
        "        validation_data=sh_val,\n",
        "        epochs=10,\n",
        "        steps_per_epoch=sh_steps,\n",
        "        validation_steps=sh_val_steps,\n",
        "        callbacks=cbs_ft,\n",
        "        verbose=1\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d3106cf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "> üí¨ Hello Shirayuki\n",
            "> ü§ñ end\n",
            "\n",
            "> üí¨ How are you today?\n",
            "> ü§ñ end and end\n",
            "\n",
            "> üí¨ What's your favorite music?\n",
            "> ü§ñ end end\n",
            "\n",
            "> üí¨ Peux-tu m'aider √† planifier ma journ√©e ?\n",
            "> ü§ñ end\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# D√©mo rapide post-entra√Ænement\n",
        "# =========================\n",
        "tests = [\n",
        "    \"Hello Shirayuki\",\n",
        "    \"How are you today?\",\n",
        "    \"What's your favorite music?\",\n",
        "    \"Peux-tu m'aider √† planifier ma journ√©e ?\"\n",
        "]\n",
        "for t in tests:\n",
        "    print(\"\\n> üí¨\", t)\n",
        "    print(\"> ü§ñ\", generate_response(t, temperature=0.8, top_k=40))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
