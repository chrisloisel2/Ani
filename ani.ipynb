{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2bf7a34f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pip in /Users/christopher/Library/Python/3.9/lib/python/site-packages (25.2)\n",
            "Requirement already satisfied: wheel in /Users/christopher/Library/Python/3.9/lib/python/site-packages (0.45.1)\n",
            "Requirement already satisfied: setuptools in /Users/christopher/Library/Python/3.9/lib/python/site-packages (80.9.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: tensorflow in /Users/christopher/Library/Python/3.9/lib/python/site-packages (2.19.0)\n",
            "Requirement already satisfied: tensorflow-metal in /Users/christopher/Library/Python/3.9/lib/python/site-packages (1.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.3.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (80.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from tensorflow-metal) (0.45.1)\n",
            "Requirement already satisfied: rich in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from keras>=3.5.0->tensorflow) (14.1.0)\n",
            "Requirement already satisfied: namex in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard~=2.19.0->tensorflow) (8.5.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.19.0->tensorflow) (3.20.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/christopher/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pip wheel setuptools\n",
        "!pip install -U tensorflow tensorflow-metal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cfb88c30",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ GPU Metal détecté : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "🟣 Mixed precision activée (float16).\n"
          ]
        }
      ],
      "source": [
        "import os, multiprocessing as mp\n",
        "import tensorflow as tf\n",
        "\n",
        "# Réduit le spam de logs\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
        "\n",
        "def setup_apple_silicon():\n",
        "    \"\"\"Configure TF pour Apple Silicon (M1/M2/M3).\"\"\"\n",
        "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "    if gpus:\n",
        "        # --- Chemin GPU (Metal) ---\n",
        "        # Mixed precision = GROS boost sur GPU Apple\n",
        "        try:\n",
        "            tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "        except Exception:\n",
        "            # (Keras 3 standalone)\n",
        "            from keras import mixed_precision\n",
        "            mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "        tf.config.set_soft_device_placement(True)  # place auto sur CPU si besoin\n        batch_size = 128\n",
        "        print(\"✅ GPU Metal détecté :\", gpus)\n",
        "        print(\"🟣 Mixed precision activée (float16).\")\n        print(f\"🧠 Batch size conseillé : {batch_size}\")\n",
        "        return dict(use_xla=False, on_gpu=True, batch=batch_size)     # XLA pas indispensable/utile ici\n",
        "    else:\n",
        "        # --- Chemin CPU ---\n",
        "        cores = mp.cpu_count()\n        batch_size = 32\n",
        "        tf.keras.mixed_precision.set_global_policy(\"float32\")  # plus stable sur CPU\n",
        "        tf.config.threading.set_intra_op_parallelism_threads(cores)\n",
        "        tf.config.threading.set_inter_op_parallelism_threads(min(4, max(2, cores // 4)))\n",
        "        os.environ[\"OMP_NUM_THREADS\"] = str(cores)\n",
        "        print(\"🟡 Pas de GPU Metal. Optimisation CPU (threads).\")\n        print(f\"🧠 Batch size conseillé : {batch_size}\")\n",
        "        return dict(use_xla=True, on_gpu=False, batch=batch_size)    # XLA (jit) souvent bénéfique sur CPU\n",
        "\n",
        "CONF = setup_apple_silicon()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5d23dc61",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q faiss-cpu datasets pandas sentence-transformers sacrebleu tf-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "966b6897",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF 2.19.0\n",
            "Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "✅ Metal GPU détecté → policy 'mixed_float16' activée (variables en float32).\n"
          ]
        }
      ],
      "source": [
        "# === Imports légers & ordonnés ===\n",
        "import math\n",
        "import random\n",
        "import pathlib\n",
        "import datetime as dt\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import callbacks as Kcb\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "# (optionnel) petites infos de run\n",
        "print(\"TF\", tf.__version__)\n",
        "print(\"Devices:\", tf.config.list_physical_devices())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2598a99e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Données\n",
        "# =========================\n",
        "def load_squad_pairs():\n",
        "    ds = load_dataset(\"squad\", split=\"train\")\n",
        "    pairs = []\n",
        "    for it in ds:\n",
        "        ctx = (it[\"context\"] or \"\").strip()\n",
        "        q = (it[\"question\"] or \"\").strip()\n",
        "        ans = it[\"answers\"][\"text\"][0].strip() if it[\"answers\"][\"text\"] else \"\"\n",
        "        if ctx and q and ans:\n",
        "            pairs.append((f\"{ctx}\\nQ: {q}\", ans))\n",
        "    print(f\"✅ SQuAD: {len(pairs)} paires\")\n",
        "    return pairs\n",
        "\n",
        "def load_shirayuki_pairs(csv_path=\"shirayuki.csv\"):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    pairs = [(str(i).strip(), str(o).strip())\n",
        "             for i,o in zip(df[\"guy\"], df[\"girl\"])\n",
        "             if str(i).strip() and str(o).strip()]\n",
        "    print(f\"✅ Shirayuki: {len(pairs)} paires\")\n",
        "    return pairs\n",
        "\n",
        "def split_pairs(pairs, val_ratio=0.02, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = np.arange(len(pairs))\n",
        "    rng.shuffle(idx)\n",
        "    cut = max(1, int(len(pairs) * (1 - val_ratio)))\n",
        "    train_idx, val_idx = idx[:cut], idx[cut:]\n",
        "    train = [pairs[i] for i in train_idx]\n",
        "    val = [pairs[i] for i in val_idx]\n",
        "    return train, val\n",
        "\n",
        "def make_ds_from_pairs(pairs, tokenizer, max_len=96, batch_size=64, shuffle=True):\n",
        "    X = [x for x,_ in pairs]\n",
        "    Y = [f\"[START] {y} [END]\" for _,y in pairs]\n",
        "    enc = tokenizer(X)\n",
        "    out = tokenizer(Y)\n",
        "    dec_in = out[:, :-1]\n",
        "    dec_tg = out[:, 1:]\n",
        "    ds = tf.data.Dataset.from_tensor_slices(\n",
        "        ({\"encoder_input\": enc, \"decoder_input\": dec_in}, dec_tg)\n",
        "    )\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(10000)\n",
        "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    steps = math.ceil(len(pairs) / batch_size)\n",
        "    return ds, steps\n",
        "\n",
        "def prepare_datasets(pairs, tokenizer=None, vocab_size=20000, max_len=96, batch_size=64, val_ratio=0.02):\n",
        "    train_pairs, val_pairs = split_pairs(pairs, val_ratio=val_ratio)\n",
        "    X_all = [x for x,_ in pairs]\n",
        "    Y_all = [f\"[START] {y} [END]\" for _,y in pairs]\n",
        "    if tokenizer is None:\n",
        "        tokenizer = TextVectorization(\n",
        "            max_tokens=vocab_size,\n",
        "            output_sequence_length=max_len,\n",
        "            standardize=\"lower_and_strip_punctuation\",\n",
        "            split=\"whitespace\"\n",
        "        )\n",
        "        tokenizer.adapt(X_all + Y_all)\n",
        "    train_ds, train_steps = make_ds_from_pairs(train_pairs, tokenizer, max_len, batch_size, shuffle=True)\n",
        "    val_ds, val_steps     = make_ds_from_pairs(val_pairs, tokenizer, max_len, batch_size, shuffle=False)\n",
        "    return tokenizer, train_ds, val_ds, train_steps, val_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "33803d56",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/christopher/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Mémoire FAISS (RAG light)\n",
        "# =========================\n",
        "import os\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "EMBED_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "EMBED_DIM = 384\n",
        "MEMORY_FILE = \"shirayuki_memory.jsonl\"\n",
        "INDEX_FILE = \"shirayuki_faiss.index\"\n",
        "index = faiss.read_index(INDEX_FILE) if os.path.exists(INDEX_FILE) else faiss.IndexFlatL2(EMBED_DIM)\n",
        "\n",
        "def _encode(text): return np.array([EMBED_MODEL.encode(text)], dtype=\"float32\")\n",
        "\n",
        "def save_to_memory(user_text, bot_text):\n",
        "    ts = datetime.datetime.now().isoformat()\n",
        "    index.add(_encode(user_text))\n",
        "    with open(MEMORY_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps({\"input\": user_text, \"response\": bot_text, \"timestamp\": ts}, ensure_ascii=False) + \"\\n\")\n",
        "    faiss.write_index(index, INDEX_FILE)\n",
        "\n",
        "def search_memory(query, top_k=3):\n",
        "    if index.ntotal == 0 or not os.path.exists(MEMORY_FILE): return []\n",
        "    D, I = index.search(_encode(query), top_k)\n",
        "    with open(MEMORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        mem = [json.loads(l) for l in f]\n",
        "    return [mem[i] for i in I[0] if 0 <= i < len(mem)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "68dac2f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# =========================\n",
        "# Masques (compatibles Keras MHA)\n",
        "# =========================\n",
        "PAD = 0\n",
        "def padding_mask_2d(token_ids):\n",
        "    return tf.cast(tf.not_equal(token_ids, PAD), tf.float32)   # (B,T)\n",
        "def self_attention_mask(tokens):\n",
        "    m = padding_mask_2d(tokens)                                # (B,T)\n",
        "    return tf.einsum(\"bi,bj->bij\", m, m)                       # (B,T,T)\n",
        "def look_ahead_matrix(T):\n",
        "    return tf.linalg.band_part(tf.ones((T, T), dtype=tf.float32), -1, 0)  # (T,T)\n",
        "def decoder_self_mask(dec_tokens):\n",
        "    m = padding_mask_2d(dec_tokens)                            # (B,Td)\n",
        "    pad_pair = tf.einsum(\"bi,bj->bij\", m, m)                   # (B,Td,Td)\n",
        "    la = look_ahead_matrix(tf.shape(dec_tokens)[1])            # (Td,Td)\n",
        "    return pad_pair * la                                       # (B,Td,Td)\n",
        "def cross_attention_mask(dec_tokens, enc_tokens):\n",
        "    m_dec = padding_mask_2d(dec_tokens)                        # (B,Td)\n",
        "    m_enc = padding_mask_2d(enc_tokens)                        # (B,Te)\n",
        "    return tf.einsum(\"bi,bj->bij\", m_dec, m_enc)               # (B,Td,Te)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4c94f79e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Modèle Transformer\n",
        "# =========================\n",
        "class Block(tf.keras.layers.Layer):\n",
        "    def __init__(self, d, h, ff, drop=0.1, decoder=False):\n",
        "        super().__init__()\n",
        "        self.decoder = decoder\n",
        "        self.self_att = MultiHeadAttention(num_heads=h, key_dim=d//h, dropout=drop)\n",
        "        self.ln1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.do1 = Dropout(drop)\n",
        "        if decoder:\n",
        "            self.cross = MultiHeadAttention(num_heads=h, key_dim=d//h, dropout=drop)\n",
        "            self.ln_c = LayerNormalization(epsilon=1e-6)\n",
        "            self.do_c = Dropout(drop)\n",
        "        self.ffn = tf.keras.Sequential([Dense(ff, activation=\"gelu\"), Dense(d)])\n",
        "        self.ln2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.do2 = Dropout(drop)\n",
        "    def call(self, x, enc_out=None, self_mask=None, enc_mask=None, training=False):\n",
        "        a = self.self_att(x, x, x, attention_mask=self_mask, training=training)\n",
        "        x = self.ln1(x + self.do1(a, training=training))\n",
        "        if self.decoder and enc_out is not None:\n",
        "            a2 = self.cross(x, enc_out, enc_out, attention_mask=enc_mask, training=training)\n",
        "            x = self.ln_c(x + self.do_c(a2, training=training))\n",
        "        f = self.ffn(x)\n",
        "        return self.ln2(x + self.do2(f, training=training))\n",
        "\n",
        "class Seq2Seq(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, d=256, h=8, ff=768, max_len=96, L=4, drop=0.1):\n",
        "        super().__init__()\n",
        "        self.d, self.max_len = d, max_len\n",
        "        self.tok_emb = Embedding(vocab_size, d)\n",
        "        self.pos_emb = Embedding(max_len, d)\n",
        "        self.enc = [Block(d, h, ff, drop, decoder=False) for _ in range(L)]\n",
        "        self.dec = [Block(d, h, ff, drop, decoder=True) for _ in range(L)]\n",
        "        self.final = Dense(vocab_size)\n",
        "    def _add_pos(self, tok_ids):\n",
        "        T = tf.shape(tok_ids)[1]\n",
        "        return self.tok_emb(tok_ids) + self.pos_emb(tf.range(T)[tf.newaxis, :])\n",
        "    def encode(self, enc_tokens, training=False):\n",
        "        x = self._add_pos(enc_tokens)\n",
        "        mask = self_attention_mask(enc_tokens)                 # (B,Te,Te)\n",
        "        for blk in self.enc:\n",
        "            x = blk(x, self_mask=mask, training=training)\n",
        "        return x\n",
        "    def decode(self, dec_tokens, enc_tokens, enc_out, training=False):\n",
        "        y = self._add_pos(dec_tokens)\n",
        "        self_m = decoder_self_mask(dec_tokens)                 # (B,Td,Td)\n",
        "        cross_m = cross_attention_mask(dec_tokens, enc_tokens) # (B,Td,Te)\n",
        "        for blk in self.dec:\n",
        "            y = blk(y, enc_out=enc_out, self_mask=self_m, enc_mask=cross_m, training=training)\n",
        "        return y\n",
        "    def call(self, inputs, training=False):\n",
        "        enc_tokens = inputs[\"encoder_input\"]\n",
        "        dec_tokens = inputs[\"decoder_input\"]\n",
        "        enc_out = self.encode(enc_tokens, training=training)\n",
        "        dec_out = self.decode(dec_tokens, enc_tokens, enc_out, training=training)\n",
        "        return self.final(dec_out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6fc245fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Génération\n",
        "# =========================\n",
        "def build_generation(tokenizer, model):\n",
        "    vocab = tokenizer.get_vocabulary()\n",
        "    tok2id = {t:i for i,t in enumerate(vocab)}\n",
        "    START = tok2id.get(\"[START]\", 1)\n",
        "    END = tok2id.get(\"[END]\", 2)\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def _tf_encode(enc_tokens):\n",
        "        return model.encode(enc_tokens, training=False)\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def _tf_decode(dec_tokens, enc_tokens, enc_out):\n",
        "        y = model.decode(dec_tokens, enc_tokens, enc_out, training=False)\n",
        "        return model.final(y)[:, -1, :]\n",
        "\n",
        "    def generate_response(prompt, max_new_tokens=64, temperature=0.7, top_k=None, use_memory=True, save_mem=True):\n",
        "        ctx = \"\"\n",
        "        if use_memory:\n",
        "            hits = search_memory(prompt, top_k=3)\n",
        "            if hits:\n",
        "                ctx = \"\\n\".join([f\"User: {m['input']}\\nShirayuki: {m['response']}\" for m in hits]) + \"\\n\"\n",
        "        full_inp = ctx + f\"User: {prompt}\\nShirayuki:\"\n",
        "\n",
        "        enc_tokens = tokenizer([full_inp])\n",
        "        enc_out = _tf_encode(enc_tokens)\n",
        "\n",
        "        y = tf.constant([[START]], dtype=tf.int64)\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits = _tf_decode(y, enc_tokens, enc_out)\n",
        "            if temperature and temperature > 0:\n",
        "                logits = logits / temperature\n",
        "                if top_k and top_k > 0:\n",
        "                    values, indices = tf.math.top_k(logits, k=top_k)\n",
        "                    probs = tf.nn.softmax(values)\n",
        "                    next_id_rel = tf.random.categorical(tf.math.log(probs), 1)\n",
        "                    next_id = tf.gather(indices, next_id_rel, batch_dims=1)\n",
        "                    next_token = int(next_id.numpy()[0][0])\n",
        "                else:\n",
        "                    next_token = int(tf.random.categorical(logits, 1).numpy()[0][0])\n",
        "            else:\n",
        "                next_token = int(tf.argmax(logits, axis=-1).numpy()[0])\n",
        "            if next_token == END: break\n",
        "            y = tf.concat([y, tf.constant([[next_token]], dtype=tf.int64)], axis=1)\n",
        "\n",
        "        id2tok = {i:t for i,t in enumerate(vocab)}\n",
        "        toks = [id2tok.get(int(t), \"\") for t in y.numpy()[0] if int(t) not in (0, START, END)]\n",
        "        text = \" \".join(toks).strip()\n",
        "        if save_mem:\n",
        "            save_to_memory(prompt, text)\n",
        "        return text or \"[Aucune réponse générée]\"\n",
        "\n",
        "    return generate_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e6a6f35",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Callbacks\n",
        "# =========================\n",
        "def build_callbacks(run_name=\"run\"):\n",
        "    ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    log_dir = pathlib.Path(\"logs\") / f\"{run_name}-{ts}\"\n",
        "    ckpt_dir = pathlib.Path(\"ckpts\") / f\"{run_name}-{ts}\"\n",
        "    log_dir.mkdir(parents=True, exist_ok=True)\n",
        "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # LR schedule: warmup -> cosine\n",
        "    warmup_epochs = 1\n",
        "    max_epochs = 50\n",
        "    base_lr = 1e-3\n",
        "    min_lr = 1e-5\n",
        "    def lr_schedule(epoch, lr):\n",
        "        if epoch < warmup_epochs:\n",
        "            return base_lr * (epoch + 1) / warmup_epochs\n",
        "        # cosine decay from base_lr to min_lr\n",
        "        t = (epoch - warmup_epochs) / max(1, (max_epochs - warmup_epochs))\n",
        "        return float(min_lr + 0.5*(base_lr - min_lr)*(1 + math.cos(math.pi * t)))\n",
        "\n",
        "    def make_gen_cb(gen_fn):\n",
        "        sample_prompts = [\"Hello Shirayuki\", \"How are you today?\"]\n",
        "        def _on_epoch_end(epoch, logs=None):\n",
        "            print(\"\\n🧪 Samples:\")\n",
        "            for p in sample_prompts:\n",
        "                print(\" >\", p)\n",
        "                print(\" >\", gen_fn(p, temperature=0.8, top_k=40))\n",
        "        return Kcb.LambdaCallback(on_epoch_end=_on_epoch_end)\n",
        "\n",
        "    cbs = [\n",
        "        Kcb.TensorBoard(log_dir=str(log_dir), histogram_freq=0, write_graph=True),\n",
        "        Kcb.BackupAndRestore(backup_dir=str(log_dir / \"backup\")),\n",
        "        Kcb.ModelCheckpoint(\n",
        "            filepath=str(ckpt_dir / \"{epoch:02d}-{val_loss:.3f}.weights.h5\"),\n",
        "            save_weights_only=True, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1\n",
        "        ),\n",
        "        Kcb.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1),\n",
        "        Kcb.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5, verbose=1),\n",
        "        Kcb.LearningRateScheduler(lr_schedule, verbose=0),\n",
        "        Kcb.CSVLogger(str(log_dir / \"training.csv\"), append=False),\n",
        "        Kcb.TerminateOnNaN(),\n",
        "    ]\n",
        "    return cbs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e217ef7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== Imports utiles =====\n",
        "import math, datetime, pathlib, random\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "Kcb = tf.keras.callbacks\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1) Warmup + Cosine schedule (sur les *steps*)\n",
        "# -------------------------------------------------\n",
        "class WarmupCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, base_lr, min_lr, warmup_steps, total_steps):\n",
        "        self.base_lr = float(base_lr)\n",
        "        self.min_lr = float(min_lr)\n",
        "        self.warmup_steps = int(warmup_steps)\n",
        "        self.total_steps = int(total_steps)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        warm = tf.cond(step < self.warmup_steps,\n",
        "            lambda: self.base_lr * (step + 1.0) / tf.maximum(1.0, float(self.warmup_steps)),\n",
        "            lambda: self.min_lr + 0.5*(self.base_lr - self.min_lr) *\n",
        "                    (1.0 + tf.cos(math.pi * tf.minimum(1.0,\n",
        "                       (step - self.warmup_steps) / tf.maximum(1.0, float(self.total_steps - self.warmup_steps))))))\n",
        "        return warm\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2) BLEU-4 et ROUGE-L (implémentation light)\n",
        "# -------------------------------------------------\n",
        "def _tokenize(s): return s.strip().split()\n",
        "\n",
        "def _ngrams(toks, n):\n",
        "    return [tuple(toks[i:i+n]) for i in range(len(toks)-n+1)]\n",
        "\n",
        "def bleu4(ref, hyp, smooth=1.0):\n",
        "    # ref/hyp: strings\n",
        "    r = _tokenize(ref); h = _tokenize(hyp)\n",
        "    if len(h) == 0: return 0.0\n",
        "    precisions = []\n",
        "    for n in range(1, 5):\n",
        "        R = Counter(_ngrams(r, n)); H = Counter(_ngrams(h, n))\n",
        "        overlap = sum((R & H).values())\n",
        "        total = max(sum(H.values()), 1)\n",
        "        precisions.append((overlap + smooth) / (total + smooth))\n",
        "    bp = math.exp(1 - len(r)/max(len(h), 1)) if len(h) < len(r) else 1.0\n",
        "    return float(bp * math.exp(sum(map(math.log, precisions)) / 4.0))\n",
        "\n",
        "def _lcs_len(a, b):\n",
        "    # a, b: list of tokens\n",
        "    m, n = len(a), len(b)\n",
        "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
        "    for i in range(m):\n",
        "        ai = a[i]\n",
        "        row = dp[i]\n",
        "        row1 = dp[i+1]\n",
        "        for j in range(n):\n",
        "            row1[j+1] = row[j] + 1 if ai == b[j] else max(row1[j], row[j+1])\n",
        "    return dp[m][n]\n",
        "\n",
        "def rouge_l_f1(ref, hyp, beta=1.2):\n",
        "    r = _tokenize(ref); h = _tokenize(hyp)\n",
        "    if len(r) == 0 or len(h) == 0: return 0.0\n",
        "    L = _lcs_len(r, h)\n",
        "    p = L / len(h); rc = L / len(r)\n",
        "    if p + rc == 0: return 0.0\n",
        "    b2 = beta * beta\n",
        "    return float((1 + b2) * p * rc / (rc + b2 * p))\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3) Callback d’éval NLG (BLEU/ROUGE/PPL + TB)\n",
        "# -------------------------------------------------\n",
        "class EvalNLG(Kcb.Callback):\n",
        "    def __init__(self, gen_fn, eval_pairs, log_dir, every=1, name_prefix=\"val\"):\n",
        "        super().__init__()\n",
        "        self.gen = gen_fn\n",
        "        self.pairs = eval_pairs\n",
        "        self.every = int(every)\n",
        "        self.name = name_prefix\n",
        "        self.tb = tf.summary.create_file_writer(str(log_dir))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) % self.every != 0: return\n",
        "        logs = logs or {}\n",
        "        preds, refs = [], []\n",
        "        for q, a in self.pairs:\n",
        "            # greedy pour des métriques stables\n",
        "            hyp = self.gen(q, temperature=0.0)  # top_k=None par défaut\n",
        "            preds.append(hyp); refs.append(a)\n",
        "        bleu = float(np.mean([bleu4(r, h) for r, h in zip(refs, preds)]))\n",
        "        rouge = float(np.mean([rouge_l_f1(r, h) for r, h in zip(refs, preds)]))\n",
        "        ppl = float(np.exp(logs['val_loss'])) if 'val_loss' in logs else float('nan')\n",
        "\n",
        "        # injecter dans logs -> utilisable par EarlyStopping/Checkpoint\n",
        "        logs[f'{self.name}_bleu'] = bleu\n",
        "        logs[f'{self.name}_rougeL'] = rouge\n",
        "        logs[f'{self.name}_perplexity'] = ppl\n",
        "\n",
        "        # log TensorBoard\n",
        "        lr = self._current_lr()\n",
        "        with self.tb.as_default():\n",
        "            tf.summary.scalar(f'{self.name}_bleu', bleu, step=epoch)\n",
        "            tf.summary.scalar(f'{self.name}_rougeL', rouge, step=epoch)\n",
        "            tf.summary.scalar(f'{self.name}_perplexity', ppl, step=epoch)\n",
        "            if lr is not None:\n",
        "                tf.summary.scalar('lr', lr, step=epoch)\n",
        "\n",
        "        # console\n",
        "        print(f\"\\n📊 {self.name.upper()} — BLEU: {bleu:.3f} | ROUGE-L: {rouge:.3f} | PPL: {ppl:.1f} | LR: {lr:.2e if lr else np.nan}\")\n",
        "\n",
        "        # petit aperçu\n",
        "        for p in [\"Hello Shirayuki\", \"How are you today?\"]:\n",
        "            print(\" >\", p)\n",
        "            print(\" >\", self.gen(p, temperature=0.8, top_k=40))\n",
        "\n",
        "    def _current_lr(self):\n",
        "        lr = getattr(self.model.optimizer, 'learning_rate', None)\n",
        "        if lr is None: return None\n",
        "        try:\n",
        "            # schedule -> callable\n",
        "            return float(lr(self.model.optimizer.iterations))\n",
        "        except TypeError:\n",
        "            # constant -> variable\n",
        "            return float(tf.keras.backend.get_value(lr))\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4) Callbacks pack (cohérent et centré ROUGE-L)\n",
        "# -------------------------------------------------\n",
        "def build_callbacks_optim(run_name, gen_fn, eval_pairs, max_epochs, steps_per_epoch):\n",
        "    ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    log_dir = pathlib.Path(\"logs\") / f\"{run_name}-{ts}\"\n",
        "    ckpt_dir = pathlib.Path(\"ckpts\") / f\"{run_name}-{ts}\"\n",
        "    log_dir.mkdir(parents=True, exist_ok=True)\n",
        "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    eval_cb = EvalNLG(gen_fn, eval_pairs, log_dir, every=1, name_prefix=\"val\")\n",
        "\n",
        "    cbs = [\n",
        "        # IMPORTANT: EvalNLG AVANT Checkpoint/EarlyStopping (il insère 'val_rougeL' dans logs)\n",
        "        eval_cb,\n",
        "        Kcb.TensorBoard(log_dir=str(log_dir), histogram_freq=0, write_graph=True, profile_batch=(10, 20)),\n",
        "        Kcb.BackupAndRestore(backup_dir=str(log_dir / \"backup\")),\n",
        "        # meilleur modèle par ROUGE-L\n",
        "        Kcb.ModelCheckpoint(\n",
        "            filepath=str(ckpt_dir / \"best-rouge-{epoch:02d}-{val_rougeL:.3f}.weights.h5\"),\n",
        "            save_weights_only=True, monitor=\"val_rougeL\", mode=\"max\", save_best_only=True, verbose=1\n",
        "        ),\n",
        "        # on garde aussi le meilleur par val_loss (utile pour perplexity)\n",
        "        Kcb.ModelCheckpoint(\n",
        "            filepath=str(ckpt_dir / \"best-loss-{epoch:02d}-{val_loss:.3f}.weights.h5\"),\n",
        "            save_weights_only=True, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1\n",
        "        ),\n",
        "        # early stop sur la vraie fitness\n",
        "        Kcb.EarlyStopping(monitor=\"val_rougeL\", patience=4, mode=\"max\", restore_best_weights=True, verbose=1),\n",
        "        Kcb.CSVLogger(str(log_dir / \"training.csv\"), append=False),\n",
        "        Kcb.TerminateOnNaN(),\n",
        "    ]\n",
        "    return cbs, log_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "690f24bc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ SQuAD: 87599 paires\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'batch'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m squad_pairs \u001b[38;5;241m=\u001b[39m load_squad_pairs()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Batch dynamique selon GPU/CPU\u001b[39;00m\n\u001b[1;32m     19\u001b[0m tokenizer, squad_train, squad_val, squad_steps, squad_val_steps \u001b[38;5;241m=\u001b[39m prepare_datasets(\n\u001b[0;32m---> 20\u001b[0m     squad_pairs, vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m96\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[43mCONF\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, val_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# (Recommandé) écarter le padding (id=0) de la loss/accuracy via sample_weight\u001b[39;00m\n\u001b[1;32m     24\u001b[0m PAD_ID \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'batch'"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Données + modèle (Apple Silicon ready)\n",
        "# =========================\n",
        "import os, random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "\n",
        "# (Optionnel) un peu plus de reproductibilité\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Charge les paires\n",
        "squad_pairs = load_squad_pairs()\n",
        "\n",
        "# Batch dynamique selon GPU/CPU\n",
        "tokenizer, squad_train, squad_val, squad_steps, squad_val_steps = prepare_datasets(\n",
        "    squad_pairs, vocab_size=20000, max_len=96, batch_size=CONF[\"batch\"], val_ratio=0.02\n",
        ")\n",
        "\n",
        "# (Recommandé) écarter le padding (id=0) de la loss/accuracy via sample_weight\n",
        "PAD_ID = 0\n",
        "def add_mask(x, y):\n",
        "    # sw: (B, T) float32 ; 1 pour tokens non-pad, 0 pour pad\n",
        "    sw = tf.cast(tf.not_equal(y, PAD_ID), tf.float32)\n",
        "    return x, y, sw\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "# + parallel map et cache (utile si les paires tiennent en RAM)\n",
        "squad_train = (squad_train\n",
        "               .map(add_mask, num_parallel_calls=AUTOTUNE)\n",
        "               .prefetch(AUTOTUNE))\n",
        "squad_val   = (squad_val\n",
        "               .map(add_mask, num_parallel_calls=AUTOTUNE)\n",
        "               .prefetch(AUTOTUNE))\n",
        "\n",
        "# Scheduler de LR sur les steps (on scale linéairement si batch changé)\n",
        "max_epochs   = 50\n",
        "scale        = float(CONF[\"batch\"]) / 64.0\n",
        "base_lr      = 3e-4 * scale\n",
        "min_lr       = 1e-5\n",
        "total_steps  = int(squad_steps * max_epochs)\n",
        "warmup_steps = int(0.1 * total_steps)\n",
        "\n",
        "lr_sched = WarmupCosine(base_lr=base_lr, min_lr=min_lr,\n",
        "                        warmup_steps=warmup_steps, total_steps=total_steps)\n",
        "\n",
        "# Optimizer = AdamW + clipnorm (compat Keras/TF)\n",
        "# Essaye TF-Keras 2.x, puis Keras 3, puis Adam fallback\n",
        "opt = None\n",
        "# TF >= 2.11\n",
        "try:\n",
        "    opt = tf.keras.optimizers.AdamW(\n",
        "        learning_rate=lr_sched, weight_decay=1e-4, clipnorm=1.0\n",
        "    )\n",
        "except Exception:\n",
        "    pass\n",
        "# Keras 3 (backend-agnostic)\n",
        "if opt is None:\n",
        "    try:\n",
        "        import keras\n",
        "        opt = keras.optimizers.AdamW(\n",
        "            learning_rate=lr_sched, weight_decay=1e-4, clipnorm=1.0\n",
        "        )\n",
        "    except Exception:\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=lr_sched, clipnorm=1.0)\n",
        "\n",
        "# Modèle\n",
        "# TextVectorization a bien tokenizer.vocabulary_size()\n",
        "vocab_size = tokenizer.vocabulary_size() if hasattr(tokenizer, \"vocabulary_size\") else \\\n",
        "             (getattr(tokenizer, \"num_words\", None) or getattr(tokenizer, \"vocab_size\", None) or 20000)\n",
        "\n",
        "model = Seq2Seq(\n",
        "    vocab_size=vocab_size, d=256, h=8, ff=768,\n",
        "    max_len=96, L=4, drop=0.1\n",
        ")\n",
        "\n",
        "# XLA: sur Apple/Metal, évite XLA GPU. On l’active seulement s’il n’y a PAS de GPU logique.\n",
        "use_xla_cpu_only = bool(CONF.get(\"use_xla\")) and (len(tf.config.list_logical_devices('GPU')) == 0)\n",
        "\n",
        "# Compile\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=SparseCEFromLogitsFP32(),                   # stable avec mixed precision\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"tok_acc\")],\n",
        "    jit_compile=use_xla_cpu_only\n",
        ")\n",
        "\n",
        "# Génération pour les callbacks\n",
        "generate_response = build_generation(tokenizer, model)\n",
        "\n",
        "# Petit set d'éval texte (256 QA aléatoires mais fixes)\n",
        "random.seed(SEED)\n",
        "eval_pairs = random.sample(squad_pairs, k=min(256, len(squad_pairs)))\n",
        "\n",
        "cbs, log_dir = build_callbacks_optim(\n",
        "    \"pretrain_squad\", generate_response, eval_pairs, max_epochs, squad_steps\n",
        ")\n",
        "\n",
        "model.build(input_shape={\n",
        "    \"encoder_input\": (None, None),   # (batch, seq_len)\n",
        "    \"decoder_input\": (None, None)\n",
        "})\n",
        "\n",
        "print(\"🚀 Pré-entraînement sur SQuAD (optimisé M3 — GPU si dispo)…\")\n",
        "\n",
        "history = model.fit(\n",
        "    squad_train,\n",
        "    validation_data=squad_val,\n",
        "    epochs=max_epochs,\n",
        "    steps_per_epoch=int(squad_steps),\n",
        "    validation_steps=int(squad_val_steps),\n",
        "    callbacks=cbs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaffb72d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Shirayuki: 4362 paires\n",
            "🔄 Fine-tuning sur Shirayuki...\n",
            "Epoch 1/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 1.0858\n",
            "Epoch 1: val_loss improved from inf to 0.91253, saving model to ckpts/finetune_shirayuki-20250810-221942/01-0.913.weights.h5\n",
            "\n",
            "🧪 FT Samples:\n",
            " > Hello Shirayuki\n",
            " > if i that ii mean even get that do just just i say it not like i end\n",
            " > Peux-tu m'aider à planifier ma journée ?\n",
            " > up at that end\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 3s/step - loss: 1.0844 - val_loss: 0.9125 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.8993\n",
            "Epoch 2: val_loss improved from 0.91253 to 0.88246, saving model to ckpts/finetune_shirayuki-20250810-221942/02-0.882.weights.h5\n",
            "\n",
            "🧪 FT Samples:\n",
            " > Hello Shirayuki\n",
            " > even of\n",
            " > Peux-tu m'aider à planifier ma journée ?\n",
            " > to hold so i mean that things ii just just think it end\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 3s/step - loss: 0.8991 - val_loss: 0.8825 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.8594\n",
            "Epoch 3: val_loss improved from 0.88246 to 0.85780, saving model to ckpts/finetune_shirayuki-20250810-221942/03-0.858.weights.h5\n",
            "\n",
            "🧪 FT Samples:\n",
            " > Hello Shirayuki\n",
            " > said it end\n",
            " > Peux-tu m'aider à planifier ma journée ?\n",
            " > a little end\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 3s/step - loss: 0.8593 - val_loss: 0.8578 - learning_rate: 9.9898e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.8204\n",
            "Epoch 4: val_loss improved from 0.85780 to 0.85185, saving model to ckpts/finetune_shirayuki-20250810-221942/04-0.852.weights.h5\n",
            "\n",
            "🧪 FT Samples:\n",
            " > Hello Shirayuki\n",
            " > master master end\n",
            " > Peux-tu m'aider à planifier ma journée ?\n",
            " > master end\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 3s/step - loss: 0.8205 - val_loss: 0.8518 - learning_rate: 9.9594e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.7998\n",
            "Epoch 5: val_loss improved from 0.85185 to 0.84057, saving model to ckpts/finetune_shirayuki-20250810-221942/05-0.841.weights.h5\n",
            "\n",
            "🧪 FT Samples:\n",
            " > Hello Shirayuki\n",
            " > would you end\n",
            " > Peux-tu m'aider à planifier ma journée ?\n",
            " > was a ii master end\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 3s/step - loss: 0.7998 - val_loss: 0.8406 - learning_rate: 9.9087e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.7761\n",
            "Epoch 6: val_loss improved from 0.84057 to 0.83263, saving model to ckpts/finetune_shirayuki-20250810-221942/06-0.833.weights.h5\n",
            "\n",
            "🧪 FT Samples:\n",
            " > Hello Shirayuki\n",
            " > master ii it ii end\n",
            " > Peux-tu m'aider à planifier ma journée ?\n",
            " > ii master and\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 3s/step - loss: 0.7763 - val_loss: 0.8326 - learning_rate: 9.8381e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.7694\n",
            "Epoch 7: val_loss improved from 0.83263 to 0.82565, saving model to ckpts/finetune_shirayuki-20250810-221942/07-0.826.weights.h5\n",
            "\n",
            "🧪 FT Samples:\n",
            " > Hello Shirayuki\n",
            " > ii end\n",
            " > Peux-tu m'aider à planifier ma journée ?\n",
            " > end\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 3s/step - loss: 0.7694 - val_loss: 0.8257 - learning_rate: 9.7478e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.7475\n",
            "Epoch 8: val_loss improved from 0.82565 to 0.82548, saving model to ckpts/finetune_shirayuki-20250810-221942/08-0.825.weights.h5\n",
            "\n",
            "🧪 FT Samples:\n",
            " > Hello Shirayuki\n",
            " > end\n",
            " > Peux-tu m'aider à planifier ma journée ?\n",
            " > master i master\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 3s/step - loss: 0.7476 - val_loss: 0.8255 - learning_rate: 9.6382e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.7559\n",
            "Epoch 9: val_loss did not improve from 0.82548\n",
            "\n",
            "🧪 FT Samples:\n",
            " > Hello Shirayuki\n",
            " > iis end\n",
            " > Peux-tu m'aider à planifier ma journée ?\n",
            " > master ii end\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 3s/step - loss: 0.7557 - val_loss: 0.8256 - learning_rate: 9.5098e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - loss: 0.7123\n",
            "Epoch 10: val_loss improved from 0.82548 to 0.82352, saving model to ckpts/finetune_shirayuki-20250810-221942/10-0.824.weights.h5\n",
            "\n",
            "🧪 FT Samples:\n",
            " > Hello Shirayuki\n",
            " > end\n",
            " > Peux-tu m'aider à planifier ma journée ?\n",
            " > end\n",
            "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 3s/step - loss: 0.7127 - val_loss: 0.8235 - learning_rate: 9.3630e-04\n",
            "Restoring model weights from the end of the best epoch: 10.\n"
          ]
        }
      ],
      "source": [
        "with tf.device(\"/CPU:0\"):\n",
        "    # model.fit(\n",
        "    #     squad_train,\n",
        "    #     validation_data=squad_val,\n",
        "    #     epochs=5,\n",
        "    #     steps_per_epoch=squad_steps,\n",
        "    #     validation_steps=squad_val_steps,\n",
        "    #     callbacks=cbs_pre,\n",
        "    #     verbose=1\n",
        "    # )\n",
        "\n",
        "    shirayuki_pairs = load_shirayuki_pairs(\"/Users/christopher/Documents/IA/ani/datasets/conversation_dataset_ShirayukiV3.csv\")   # <-- assure le fichier présent\n",
        "    _, sh_train, sh_val, sh_steps, sh_val_steps = prepare_datasets(\n",
        "        shirayuki_pairs, tokenizer=tokenizer, max_len=96, batch_size=64, val_ratio=0.05\n",
        "    )\n",
        "    cbs_ft = build_callbacks(\"finetune_shirayuki\")\n",
        "    def _on_epoch_end_ft(epoch, logs=None):\n",
        "        print(\"\\n🧪 FT Samples:\")\n",
        "        for p in [\"Hello Shirayuki\", \"Peux-tu m'aider à planifier ma journée ?\"]:\n",
        "            print(\" >\", p)\n",
        "            print(\" >\", generate_response(p, temperature=0.8, top_k=40))\n",
        "    cbs_ft.append(Kcb.LambdaCallback(on_epoch_end=_on_epoch_end_ft))\n",
        "\n",
        "    print(\"🔄 Fine-tuning sur Shirayuki...\")\n",
        "    model.fit(\n",
        "        sh_train,\n",
        "        validation_data=sh_val,\n",
        "        epochs=10,\n",
        "        steps_per_epoch=sh_steps,\n",
        "        validation_steps=sh_val_steps,\n",
        "        callbacks=cbs_ft,\n",
        "        verbose=1\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d3106cf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "> 💬 Hello Shirayuki\n",
            "> 🤖 end\n",
            "\n",
            "> 💬 How are you today?\n",
            "> 🤖 end and end\n",
            "\n",
            "> 💬 What's your favorite music?\n",
            "> 🤖 end end\n",
            "\n",
            "> 💬 Peux-tu m'aider à planifier ma journée ?\n",
            "> 🤖 end\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Démo rapide post-entraînement\n",
        "# =========================\n",
        "tests = [\n",
        "    \"Hello Shirayuki\",\n",
        "    \"How are you today?\",\n",
        "    \"What's your favorite music?\",\n",
        "    \"Peux-tu m'aider à planifier ma journée ?\"\n",
        "]\n",
        "for t in tests:\n",
        "    print(\"\\n> 💬\", t)\n",
        "    print(\"> 🤖\", generate_response(t, temperature=0.8, top_k=40))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
