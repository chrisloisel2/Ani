{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c820b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"ðŸ“¦ Installation de {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e09e163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopher/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ CONFIGURATION ULTRA-OPTIMISÃ‰E ACTIVÃ‰E\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "packages = ['psutil', 'matplotlib', 'seaborn', 'pandas', 'numpy']\n",
    "for pkg in packages:\n",
    "    install_if_missing(pkg)\n",
    "\n",
    "import tensorflow as tf\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"ðŸš€ CONFIGURATION ULTRA-OPTIMISÃ‰E ACTIVÃ‰E\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a6b23dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_count = multiprocessing.cpu_count()\n",
    "total_memory = psutil.virtual_memory().total / (1024**3)\n",
    "available_memory = psutil.virtual_memory().available / (1024**3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8722db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’» RESSOURCES SYSTÃˆME:\n",
      "   CPU: 14 threads\n",
      "   RAM totale: 36.0 GB\n",
      "   RAM disponible: 14.9 GB\n",
      "\n",
      "âš¡ OPTIMISATIONS TENSORFLOW:\n"
     ]
    }
   ],
   "source": [
    "print(f\"ðŸ’» RESSOURCES SYSTÃˆME:\")\n",
    "print(f\"   CPU: {cpu_count} threads\")\n",
    "print(f\"   RAM totale: {total_memory:.1f} GB\")\n",
    "print(f\"   RAM disponible: {available_memory:.1f} GB\")\n",
    "\n",
    "# Configuration TensorFlow ultra-optimisÃ©e\n",
    "print(f\"\\nâš¡ OPTIMISATIONS TENSORFLOW:\")\n",
    "\n",
    "# Configuration threads pour utilisation maximale\n",
    "tf.config.threading.set_intra_op_parallelism_threads(cpu_count)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(cpu_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "169840b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Variables d'environnement optimales\n",
    "os.environ['OMP_NUM_THREADS'] = str(cpu_count)\n",
    "os.environ['TF_NUM_INTEROP_THREADS'] = str(cpu_count)\n",
    "os.environ['TF_NUM_INTRAOP_THREADS'] = str(cpu_count)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4041386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ðŸ’» Mode CPU optimisÃ©\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"   ðŸŽ® GPU dÃ©tectÃ©s: {len(gpus)}\")\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"   âœ“ Croissance mÃ©moire GPU activÃ©e\")\n",
    "    except:\n",
    "        print(f\"   âš ï¸ Configuration GPU partielle\")\n",
    "else:\n",
    "    print(f\"   ðŸ’» Mode CPU optimisÃ©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af355180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ XLA JIT activÃ©\n",
      "   âœ“ Mixed Precision FP16 activÃ©e\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    print(f\"   âœ“ XLA JIT activÃ©\")\n",
    "except:\n",
    "    print(f\"   âš ï¸ XLA non disponible\")\n",
    "\n",
    "try:\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(f\"   âœ“ Mixed Precision FP16 activÃ©e\")\n",
    "except:\n",
    "    print(f\"   âš ï¸ Mixed Precision non supportÃ©e\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "396eb539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Dataset AUTOTUNE configurÃ©\n",
      "\n",
      "âœ… CONFIGURATION TERMINÃ‰E\n",
      "ï¿½ Utilisation prÃ©vue: CPU 14 threads, RAM ~11GB\n",
      "ðŸš€ SystÃ¨me optimisÃ© pour performances maximales!\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "print(f\"   âœ“ Dataset AUTOTUNE configurÃ©\")\n",
    "\n",
    "print(f\"\\nâœ… CONFIGURATION TERMINÃ‰E\")\n",
    "print(f\"ï¿½ Utilisation prÃ©vue: CPU {cpu_count} threads, RAM ~{int(available_memory*0.8)}GB\")\n",
    "print(\"ðŸš€ SystÃ¨me optimisÃ© pour performances maximales!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70576440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des packages nÃ©cessaires avec optimisations\n",
    "!pip install tensorflow tensorflow-addons scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d3edfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ¸ CrÃ©ation du modÃ¨le Shirayuki ultra-optimisÃ©...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Embedding, MultiHeadAttention, Dropout, LayerNormalization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "print(\"ðŸŒ¸ CrÃ©ation du modÃ¨le Shirayuki ultra-optimisÃ©...\")\n",
    "\n",
    "class SimpleTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim//num_heads, dropout=rate)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"gelu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        attn_output = self.att(x, x, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class ShirayukiTransformer(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=8, ff_dim=512, maxlen=128, num_layers=4, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, embed_dim, mask_zero=True)\n",
    "        self.pos_embedding = Embedding(maxlen, embed_dim)\n",
    "\n",
    "        self.encoder_layers = [SimpleTransformerBlock(embed_dim, num_heads, ff_dim, rate)\n",
    "                              for _ in range(num_layers)]\n",
    "        self.decoder_layers = [SimpleTransformerBlock(embed_dim, num_heads, ff_dim, rate)\n",
    "                              for _ in range(num_layers)]\n",
    "\n",
    "        self.final_layer = Dense(vocab_size, dtype='float32')\n",
    "        self.dropout = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        if isinstance(inputs, tuple):\n",
    "            input_ids, target_ids = inputs\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "            target_ids = None\n",
    "\n",
    "        # Encoder\n",
    "        encoder_output = self.encode(input_ids, training)\n",
    "\n",
    "        if target_ids is not None:\n",
    "            # Decoder avec teacher forcing\n",
    "            decoder_output = self.decode(target_ids, encoder_output, training)\n",
    "            return self.final_layer(decoder_output)\n",
    "        else:\n",
    "            return encoder_output\n",
    "\n",
    "    def encode(self, input_ids, training=False):\n",
    "        seq_len = tf.shape(input_ids)[1]\n",
    "        x = self.embedding(input_ids)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "\n",
    "        positions = tf.range(seq_len)[None, :]\n",
    "        x += self.pos_embedding(positions)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "\n",
    "    def decode(self, target_ids, encoder_output, training=False):\n",
    "        seq_len = tf.shape(target_ids)[1]\n",
    "        x = self.embedding(target_ids)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "\n",
    "        positions = tf.range(seq_len)[None, :]\n",
    "        x += self.pos_embedding(positions)\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37aea19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_shirayuki_data(file_path):\n",
    "    print(f\"ðŸ“Š Chargement des donnÃ©es...\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        inputs = df['guy'].astype(str).tolist()\n",
    "        outputs = df['girl'].astype(str).tolist()\n",
    "        print(f\"âœ… Fichier CSV chargÃ©: {len(inputs)} conversations\")\n",
    "    except:\n",
    "        print(\"âš ï¸ Fichier CSV non trouvÃ©, crÃ©ation d'un dataset de dÃ©monstration...\")\n",
    "\n",
    "    # Nettoyage simple\n",
    "    clean_pairs = []\n",
    "    for inp, out in zip(inputs, outputs):\n",
    "        if inp and out and len(inp.strip()) > 0 and len(out.strip()) > 0:\n",
    "            clean_pairs.append((inp.strip(), out.strip()))\n",
    "\n",
    "    print(f\"ðŸ“Š Conversations valides: {len(clean_pairs)}\")\n",
    "    return clean_pairs\n",
    "\n",
    "# CrÃ©ation du tokenizer simplifiÃ©\n",
    "def create_simple_tokenizer(conversations, vocab_size=8192, max_length=64):\n",
    "    print(\"ðŸ”§ CrÃ©ation du tokenizer...\")\n",
    "\n",
    "    from tensorflow.keras.utils import text_dataset_from_directory\n",
    "    from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "    # Extraction des textes\n",
    "    all_texts = []\n",
    "    for inp, out in conversations:\n",
    "        all_texts.append(inp)\n",
    "        all_texts.append(\"[START] \" + out + \" [END]\")\n",
    "\n",
    "    # Tokenizer optimisÃ©\n",
    "    tokenizer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_sequence_length=max_length,\n",
    "        standardize='lower_and_strip_punctuation',\n",
    "        split='whitespace'\n",
    "    )\n",
    "\n",
    "    tokenizer.adapt(all_texts)\n",
    "\n",
    "    # PrÃ©paration des donnÃ©es\n",
    "    inputs = [pair[0] for pair in conversations]\n",
    "    outputs = [\"[START] \" + pair[1] + \" [END]\" for pair in conversations]\n",
    "\n",
    "    input_ids = tokenizer(inputs)\n",
    "    output_ids = tokenizer(outputs)\n",
    "\n",
    "    # Teacher forcing\n",
    "    decoder_input = output_ids[:, :-1]\n",
    "    decoder_target = output_ids[:, 1:]\n",
    "\n",
    "    print(f\"âœ… Tokenizer crÃ©Ã©: {tokenizer.vocabulary_size()} tokens, longueur {max_length}\")\n",
    "    return tokenizer, input_ids, decoder_input, decoder_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49dfe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Configuration du modÃ¨le...\n",
      "ðŸ“Š ParamÃ¨tres:\n",
      "   Vocab: 8192 tokens\n",
      "   Longueur max: 64\n",
      "   Dimensions: 256\n",
      "   Couches: 4\n",
      "   Batch size: 32\n",
      "ðŸ“Š Chargement des donnÃ©es...\n",
      "âœ… Fichier CSV chargÃ©: 4363 conversations\n",
      "ðŸ“Š Conversations valides: 4362\n",
      "ðŸ”§ CrÃ©ation du tokenizer...\n",
      "âœ… Tokenizer crÃ©Ã©: 6038 tokens, longueur 64\n"
     ]
    }
   ],
   "source": [
    "# Configuration optimale\n",
    "print(\"âš™ï¸ Configuration du modÃ¨le...\")\n",
    "vocab_size = 8192\n",
    "max_length = 64\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "ff_dim = 512\n",
    "num_layers = 4\n",
    "batch_size = min(32, max(8, int(available_memory * 4)))\n",
    "\n",
    "\n",
    "print(f\"ðŸ“Š ParamÃ¨tres:\")\n",
    "print(f\"   Vocab: {vocab_size} tokens\")\n",
    "print(f\"   Longueur max: {max_length}\")\n",
    "print(f\"   Dimensions: {embed_dim}\")\n",
    "print(f\"   Couches: {num_layers}\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "\n",
    "conversations = load_shirayuki_data('/Users/christopher/Documents/IA/ani/conversation_dataset_ShirayukiV3.csv')\n",
    "\n",
    "tokenizer, input_ids, decoder_input, decoder_target = create_simple_tokenizer(\n",
    "    conversations, vocab_size, max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f035f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ CrÃ©ation du dataset...\n",
      "ðŸŒ¸ CrÃ©ation du modÃ¨le Shirayuki...\n",
      "ðŸ§ª Test du modÃ¨le...\n",
      "   Taille du batch de test: (32, 64)\n",
      "   EntrÃ©es: [[  35   32    2 ...    0    0    0]\n",
      " [ 235   85  191 ...    0    0    0]\n",
      " [2500    0    0 ...    0    0    0]\n",
      " ...\n",
      " [ 596  123    0 ...    0    0    0]\n",
      " [   5  142   64 ...    0    0    0]\n",
      " [  38   10    6 ...    0    0    0]]\n",
      "   Cibles: [[1114  797   68 ...    0    0    0]\n",
      " [  33   76   21 ...    0    0    0]\n",
      " [  32    2   13 ...    0    0    0]\n",
      " ...\n",
      " [  19    8  837 ...    0    0    0]\n",
      " [3700    8   17 ...    0    0    0]\n",
      " [  82  448   33 ...    0    0    0]]\n",
      "âŒ Erreur de test: Exception encountered when calling ShirayukiTransformer.call().\n",
      "\n",
      "\u001b[1mcannot compute Mul as input #1(zero-based) was expected to be a half tensor but is a float tensor [Op:Mul] name: \u001b[0m\n",
      "\n",
      "Arguments received by ShirayukiTransformer.call():\n",
      "  â€¢ inputs=('tf.Tensor(shape=(32, 64), dtype=int64)', 'tf.Tensor(shape=(32, 63), dtype=int64)')\n",
      "  â€¢ training=False\n",
      "\n",
      "ðŸŽ‰ MODÃˆLE SHIRAYUKI PRÃŠT!\n",
      "ðŸš€ ExÃ©cutez la cellule suivante pour l'entraÃ®nement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopher/Library/Python/3.9/lib/python/site-packages/keras/src/layers/layer.py:1474: UserWarning: Layer 'shirayuki_transformer_2' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.''\n",
      "  warnings.warn(\n",
      "/Users/christopher/Library/Python/3.9/lib/python/site-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'shirayuki_transformer_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ðŸš€ MODÃˆLE SHIRAYUKI ULTRA-SIMPLIFIÃ‰ ET ROBUSTE (CORRIGÃ‰)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Embedding, MultiHeadAttention, Dropout, LayerNormalization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "print(\"ðŸŒ¸ CrÃ©ation du modÃ¨le Shirayuki ultra-optimisÃ©...\")\n",
    "\n",
    "# DÃ©sactiver mixed precision pour Ã©viter les conflits\n",
    "tf.keras.mixed_precision.set_global_policy('float32')\n",
    "\n",
    "# Classes optimisÃ©es simplifiÃ©es avec types cohÃ©rents\n",
    "class SimpleTransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim//num_heads, dropout=rate)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"gelu\", dtype='float32'),\n",
    "            Dense(embed_dim, dtype='float32'),\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6, dtype='float32')\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6, dtype='float32')\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        attn_output = self.att(x, x, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class ShirayukiTransformer(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=8, ff_dim=512, maxlen=128, num_layers=4, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, embed_dim, mask_zero=True, dtype='float32')\n",
    "        self.pos_embedding = Embedding(maxlen, embed_dim, dtype='float32')\n",
    "\n",
    "        self.encoder_layers = [SimpleTransformerBlock(embed_dim, num_heads, ff_dim, rate)\n",
    "                              for _ in range(num_layers)]\n",
    "        self.decoder_layers = [SimpleTransformerBlock(embed_dim, num_heads, ff_dim, rate)\n",
    "                              for _ in range(num_layers)]\n",
    "\n",
    "        self.final_layer = Dense(vocab_size, dtype='float32')\n",
    "        self.dropout = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        if isinstance(inputs, tuple):\n",
    "            input_ids, target_ids = inputs\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "            target_ids = None\n",
    "\n",
    "        # Encoder\n",
    "        encoder_output = self.encode(input_ids, training)\n",
    "\n",
    "        if target_ids is not None:\n",
    "            # Decoder avec teacher forcing\n",
    "            decoder_output = self.decode(target_ids, encoder_output, training)\n",
    "            return self.final_layer(decoder_output)\n",
    "        else:\n",
    "            return encoder_output\n",
    "\n",
    "    def encode(self, input_ids, training=False):\n",
    "        seq_len = tf.shape(input_ids)[1]\n",
    "        x = self.embedding(input_ids)\n",
    "        x = tf.cast(x, tf.float32)  # Force float32\n",
    "        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "\n",
    "        positions = tf.range(seq_len)[None, :]\n",
    "        pos_emb = self.pos_embedding(positions)\n",
    "        pos_emb = tf.cast(pos_emb, tf.float32)  # Force float32\n",
    "        x += pos_emb\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "\n",
    "    def decode(self, target_ids, encoder_output, training=False):\n",
    "        seq_len = tf.shape(target_ids)[1]\n",
    "        x = self.embedding(target_ids)\n",
    "        x = tf.cast(x, tf.float32)  # Force float32\n",
    "        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "\n",
    "        positions = tf.range(seq_len)[None, :]\n",
    "        pos_emb = self.pos_embedding(positions)\n",
    "        pos_emb = tf.cast(pos_emb, tf.float32)  # Force float32\n",
    "        x += pos_emb\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "\n",
    "# Fonction de chargement de donnÃ©es robuste\n",
    "def load_shirayuki_data(file_path):\n",
    "    print(f\"ðŸ“Š Chargement des donnÃ©es...\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        inputs = df['guy'].astype(str).tolist()\n",
    "        outputs = df['girl'].astype(str).tolist()\n",
    "        print(f\"âœ… Fichier CSV chargÃ©: {len(inputs)} conversations\")\n",
    "    except:\n",
    "        print(\"âš ï¸ Fichier CSV non trouvÃ©, crÃ©ation d'un dataset de dÃ©monstration...\")\n",
    "        # Dataset de demo tsundere\n",
    "        demo_conversations = [\n",
    "            (\"Bonjour Shirayuki\", \"H-HÃ© ! Ne me parle pas si soudainement ! *rougit*\"),\n",
    "            (\"Comment Ã§a va ?\", \"Ã‡a va bien... pas que Ã§a t'intÃ©resse ! Hmph !\"),\n",
    "            (\"Tu es mignonne\", \"Q-Quoi ?! Ne dis pas des choses comme Ã§a ! *devient rouge*\"),\n",
    "            (\"Je t'aime\", \"C-Ce n'est pas comme si... si j'Ã©tais contente ! Baka !\"),\n",
    "            (\"Tu veux sortir ?\", \"P-Peut-Ãªtre... si tu insistes vraiment...\"),\n",
    "            (\"Bonne nuit\", \"Bonne nuit... et ne rÃªve pas de moi ! *dÃ©tourne le regard*\"),\n",
    "            (\"Tu me manques\", \"Tu... tu me manques aussi... mais juste un peu !\"),\n",
    "            (\"Merci\", \"C-C'est normal ! Ne me remercie pas ! *embarrassÃ©e*\"),\n",
    "            (\"Tu es belle\", \"ArrÃªte de dire n'importe quoi ! Mais... merci...\"),\n",
    "            (\"Veux-tu Ãªtre mon amie ?\", \"On... on est dÃ©jÃ  amies ! Idiot ! *sourit secrÃ¨tement*\")\n",
    "        ] * 20  # 200 exemples\n",
    "\n",
    "        inputs = [conv[0] for conv in demo_conversations]\n",
    "        outputs = [conv[1] for conv in demo_conversations]\n",
    "        print(f\"âœ… Dataset de dÃ©monstration crÃ©Ã©: {len(inputs)} conversations\")\n",
    "\n",
    "    # Nettoyage simple\n",
    "    clean_pairs = []\n",
    "    for inp, out in zip(inputs, outputs):\n",
    "        if inp and out and len(inp.strip()) > 0 and len(out.strip()) > 0:\n",
    "            clean_pairs.append((inp.strip(), out.strip()))\n",
    "\n",
    "    print(f\"ðŸ“Š Conversations valides: {len(clean_pairs)}\")\n",
    "    return clean_pairs\n",
    "\n",
    "# CrÃ©ation du tokenizer simplifiÃ©\n",
    "def create_simple_tokenizer(conversations, vocab_size=8192, max_length=64):\n",
    "    print(\"\udd27 CrÃ©ation du tokenizer...\")\n",
    "\n",
    "    from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "    # Extraction des textes\n",
    "    all_texts = []\n",
    "    for inp, out in conversations:\n",
    "        all_texts.append(inp)\n",
    "        all_texts.append(\"[START] \" + out + \" [END]\")\n",
    "\n",
    "    # Tokenizer optimisÃ©\n",
    "    tokenizer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_sequence_length=max_length,\n",
    "        standardize='lower_and_strip_punctuation',\n",
    "        split='whitespace'\n",
    "    )\n",
    "\n",
    "    tokenizer.adapt(all_texts)\n",
    "\n",
    "    # PrÃ©paration des donnÃ©es\n",
    "    inputs = [pair[0] for pair in conversations]\n",
    "    outputs = [\"[START] \" + pair[1] + \" [END]\" for pair in conversations]\n",
    "\n",
    "    input_ids = tokenizer(inputs)\n",
    "    output_ids = tokenizer(outputs)\n",
    "\n",
    "    # Teacher forcing\n",
    "    decoder_input = output_ids[:, :-1]\n",
    "    decoder_target = output_ids[:, 1:]\n",
    "\n",
    "    print(f\"âœ… Tokenizer crÃ©Ã©: {tokenizer.vocabulary_size()} tokens, longueur {max_length}\")\n",
    "    return tokenizer, input_ids, decoder_input, decoder_target\n",
    "\n",
    "# Configuration optimale\n",
    "print(\"âš™ï¸ Configuration du modÃ¨le...\")\n",
    "vocab_size = 8192\n",
    "max_length = 64\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "ff_dim = 512\n",
    "num_layers = 4\n",
    "batch_size = min(32, max(8, int(available_memory * 4)))\n",
    "\n",
    "print(f\"ðŸ“Š ParamÃ¨tres:\")\n",
    "print(f\"   Vocab: {vocab_size} tokens\")\n",
    "print(f\"   Longueur max: {max_length}\")\n",
    "print(f\"   Dimensions: {embed_dim}\")\n",
    "print(f\"   Couches: {num_layers}\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "\n",
    "# Chargement des donnÃ©es\n",
    "conversations = load_shirayuki_data('/Users/christopher/Documents/IA/ani/conversation_dataset_ShirayukiV3.csv')\n",
    "conversation_pairs = conversations  # Variable pour compatibilitÃ©\n",
    "\n",
    "# CrÃ©ation du tokenizer et des donnÃ©es\n",
    "tokenizer, input_ids, decoder_input, decoder_target = create_simple_tokenizer(\n",
    "    conversations, vocab_size, max_length\n",
    ")\n",
    "\n",
    "# CrÃ©ation du dataset\n",
    "print(\"\ud83dðŸ“¦ CrÃ©ation du dataset...\")\n",
    "dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    'encoder_input': input_ids,\n",
    "    'decoder_input': decoder_input,\n",
    "    'decoder_target': decoder_target\n",
    "})\n",
    "\n",
    "def prepare_batch(batch):\n",
    "    return ((batch['encoder_input'], batch['decoder_input']), batch['decoder_target'])\n",
    "\n",
    "dataset = (dataset\n",
    "    .map(prepare_batch, num_parallel_calls=AUTOTUNE)\n",
    "    .shuffle(1000)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(AUTOTUNE))\n",
    "\n",
    "# CrÃ©ation du modÃ¨le\n",
    "print(\"ðŸŒ¸ CrÃ©ation du modÃ¨le Shirayuki...\")\n",
    "model = ShirayukiTransformer(\n",
    "    vocab_size=tokenizer.vocabulary_size(),\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ff_dim=ff_dim,\n",
    "    maxlen=max_length,\n",
    "    num_layers=num_layers\n",
    ")\n",
    "\n",
    "# Compilation optimisÃ©e\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Test du modÃ¨le\n",
    "print(\"ðŸ§ª Test du modÃ¨le...\")\n",
    "try:\n",
    "    test_batch = next(iter(dataset.take(1)))\n",
    "    print(f\"   Taille du batch de test: {test_batch[0][0].shape}\")\n",
    "    output = model(test_batch[0])\n",
    "    print(f\"âœ… Test rÃ©ussi! Shape de sortie: {output.shape}\")\n",
    "    print(f\"ðŸ“Š ParamÃ¨tres du modÃ¨le: {model.count_params():,}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur de test: {e}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ MODÃˆLE SHIRAYUKI PRÃŠT!\")\n",
    "print(\"ðŸš€ ExÃ©cutez la cellule suivante pour l'entraÃ®nement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcba87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ DÃ©marrage de l'entraÃ®nement avec utilisation maximale des ressources!\n",
      "======================================================================\n",
      "ðŸ“Š Configuration:\n",
      "   Epochs: 15\n",
      "   Batch size: 32\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'conversation_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Batch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Steps par epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(conversation_pairs)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   CPU threads: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcpu_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   MÃ©moire utilisÃ©e: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(available_memory\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0.8\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conversation_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "# \udd25 ENTRAÃŽNEMENT SHIRAYUKI ULTRA-OPTIMISÃ‰ (CORRIGÃ‰)\n",
    "print(\"ðŸ”¥ DÃ©marrage de l'entraÃ®nement avec utilisation maximale des ressources!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configuration d'entraÃ®nement\n",
    "epochs = 15\n",
    "steps_per_epoch = len(conversation_pairs) // batch_size\n",
    "\n",
    "print(f\"ðŸ“Š Configuration:\")\n",
    "print(f\"   Epochs: {epochs}\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Steps par epoch: {steps_per_epoch}\")\n",
    "print(f\"   CPU threads: {cpu_count}\")\n",
    "print(f\"   MÃ©moire utilisÃ©e: {int(available_memory * 0.8)} GB\")\n",
    "print(f\"   Dataset: {len(conversation_pairs)} conversations\")\n",
    "\n",
    "# Callbacks optimisÃ©s\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.LambdaCallback(\n",
    "        on_epoch_end=lambda epoch, logs: print(f\"ðŸŒ¸ Epoch {epoch+1}/{epochs} - Loss: {logs['loss']:.4f} - Accuracy: {logs['accuracy']:.4f}\")\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\nðŸš€ Lancement de l'entraÃ®nement...\")\n",
    "print(\"ðŸ’¡ Utilisation de teacher forcing pour un apprentissage optimal\")\n",
    "\n",
    "try:\n",
    "    # EntraÃ®nement avec gestion d'erreurs\n",
    "    history = model.fit(\n",
    "        dataset,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(f\"\\nðŸŽ‰ ENTRAÃŽNEMENT TERMINÃ‰!\")\n",
    "    print(f\"ðŸ“ˆ Loss finale: {history.history['loss'][-1]:.4f}\")\n",
    "    print(f\"ðŸ“ˆ Accuracy finale: {history.history['accuracy'][-1]:.4f}\")\n",
    "\n",
    "    # Test de gÃ©nÃ©ration simple\n",
    "    print(\"\\nðŸ§ª Test de gÃ©nÃ©ration:\")\n",
    "    test_input = \"Bonjour Shirayuki\"\n",
    "    test_tokens = tokenizer([test_input])\n",
    "    print(f\"Input: {test_input}\")\n",
    "    print(\"Shirayuki va rÃ©pondre...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur d'entraÃ®nement: {e}\")\n",
    "    print(\"\udca1 Tentative avec paramÃ¨tres rÃ©duits...\")\n",
    "\n",
    "    # Fallback avec paramÃ¨tres rÃ©duits\n",
    "    try:\n",
    "        smaller_dataset = dataset.take(min(100, steps_per_epoch))\n",
    "        history = model.fit(\n",
    "            smaller_dataset,\n",
    "            epochs=min(5, epochs),\n",
    "            verbose=1\n",
    "        )\n",
    "        print(\"âœ… EntraÃ®nement de secours rÃ©ussi!\")\n",
    "    except Exception as e2:\n",
    "        print(f\"âŒ Erreur critique: {e2}\")\n",
    "\n",
    "print(\"\\nðŸŒ¸ ModÃ¨le Shirayuki prÃªt pour la conversation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c92db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTRAÃŽNEMENT ULTRA-OPTIMISÃ‰ AVEC MONITORING TEMPS RÃ‰EL\n",
    "print(\"ðŸš€ Configuration avancÃ©e pour utilisation maximale des ressources\")\n",
    "\n",
    "# GÃ©nÃ©rateur Shirayuki ultra-optimisÃ©\n",
    "class UltraShirayukiGenerator:\n",
    "    \"\"\"GÃ©nÃ©rateur ultra-optimisÃ© pour conversations tsundere avec monitoring\"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, max_length=128):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.generation_cache = {}  # Cache pour optimiser les gÃ©nÃ©rations rÃ©pÃ©tÃ©es\n",
    "\n",
    "    @tf.function(jit_compile=True)\n",
    "    def _generate_step(self, input_ids, temperature, top_k, top_p):\n",
    "        \"\"\"Ã‰tape de gÃ©nÃ©ration compilÃ©e avec XLA\"\"\"\n",
    "        encoder_output = self.model.encode(input_ids, training=False)\n",
    "\n",
    "        # DÃ©but de sÃ©quence\n",
    "        start_token = self.tokenizer(['[START]'])[0, 0:1]\n",
    "        decoder_input = start_token\n",
    "\n",
    "        generated_ids = []\n",
    "\n",
    "        for _ in tf.range(self.max_length - 1):\n",
    "            # PrÃ©diction du token suivant\n",
    "            logits = self.model.decode(decoder_input, encoder_output, training=False)\n",
    "            logits = self.model.output_layer(logits)\n",
    "\n",
    "            # Sampling optimisÃ©\n",
    "            next_token = self._sample_token(logits[:, -1, :], temperature, top_k, top_p)\n",
    "\n",
    "            # Ajout du token gÃ©nÃ©rÃ©\n",
    "            generated_ids.append(next_token)\n",
    "            decoder_input = tf.concat([decoder_input, next_token], axis=1)\n",
    "\n",
    "            # ArrÃªt si token de fin\n",
    "            if tf.reduce_any(tf.equal(next_token, self.tokenizer(['[END]'])[0, 0])):\n",
    "                break\n",
    "\n",
    "        return tf.concat(generated_ids, axis=1)\n",
    "\n",
    "    @tf.function(jit_compile=True)\n",
    "    def _sample_token(self, logits, temperature, top_k, top_p):\n",
    "        \"\"\"Sampling optimisÃ© avec nucleus sampling\"\"\"\n",
    "        logits = logits / temperature\n",
    "\n",
    "        # Top-k filtering\n",
    "        if top_k > 0:\n",
    "            top_k_logits, top_k_indices = tf.nn.top_k(logits, k=top_k)\n",
    "            logits = tf.where(\n",
    "                tf.reduce_any(tf.equal(tf.expand_dims(tf.range(tf.shape(logits)[-1]), 0),\n",
    "                                      tf.expand_dims(top_k_indices, -1)), axis=1),\n",
    "                logits,\n",
    "                tf.fill(tf.shape(logits), -1e9)\n",
    "            )\n",
    "\n",
    "        # Top-p (nucleus) filtering\n",
    "        if top_p < 1.0:\n",
    "            sorted_logits = tf.sort(logits, direction='DESCENDING')\n",
    "            sorted_probs = tf.nn.softmax(sorted_logits)\n",
    "            cumulative_probs = tf.cumsum(sorted_probs, axis=-1)\n",
    "\n",
    "            # Masque pour les tokens Ã  garder\n",
    "            keep_mask = cumulative_probs <= top_p\n",
    "            keep_mask = tf.concat([tf.ones_like(keep_mask[:, :1]), keep_mask[:, :-1]], axis=-1)\n",
    "\n",
    "            # Application du masque\n",
    "            filtered_logits = tf.where(keep_mask, sorted_logits, -1e9)\n",
    "            logits = tf.gather(filtered_logits, tf.argsort(tf.argsort(logits, direction='DESCENDING')),\n",
    "                              batch_dims=1)\n",
    "\n",
    "        # Ã‰chantillonnage\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        sampled_id = tf.random.categorical(tf.math.log(probs), 1)\n",
    "\n",
    "        return sampled_id\n",
    "\n",
    "    def generate_response(self, prompt, max_length=50, temperature=0.8, top_k=40, top_p=0.9):\n",
    "        \"\"\"GÃ©nÃ¨re une rÃ©ponse optimisÃ©e avec cache\"\"\"\n",
    "\n",
    "        # VÃ©rification du cache\n",
    "        cache_key = f\"{prompt}_{temperature}_{top_k}_{top_p}\"\n",
    "        if cache_key in self.generation_cache:\n",
    "            return self.generation_cache[cache_key]\n",
    "\n",
    "        # Tokenisation\n",
    "        input_ids = self.tokenizer([prompt])\n",
    "\n",
    "        # GÃ©nÃ©ration avec monitoring\n",
    "        start_time = tf.timestamp()\n",
    "        generated_ids = self._generate_step(input_ids, temperature, top_k, top_p)\n",
    "        generation_time = tf.timestamp() - start_time\n",
    "\n",
    "        # DÃ©tokenisation\n",
    "        try:\n",
    "            # Conversion sÃ©curisÃ©e\n",
    "            generated_text = self.tokenizer.get_vocabulary()[generated_ids[0, 0].numpy()]\n",
    "\n",
    "            # Reconstruction du texte\n",
    "            vocab = self.tokenizer.get_vocabulary()\n",
    "            tokens = []\n",
    "            for token_id in generated_ids[0]:\n",
    "                if token_id.numpy() < len(vocab):\n",
    "                    token = vocab[token_id.numpy()]\n",
    "                    if token not in ['[START]', '[END]', '']:\n",
    "                        tokens.append(token)\n",
    "\n",
    "            response = ' '.join(tokens)\n",
    "\n",
    "            # Nettoyage et post-traitement\n",
    "            response = response.replace('[UNK]', '').strip()\n",
    "\n",
    "            # Cache du rÃ©sultat\n",
    "            self.generation_cache[cache_key] = response\n",
    "\n",
    "            # Logging des performances\n",
    "            print(f\"âš¡ GÃ©nÃ©ration: {float(generation_time):.3f}s - {len(tokens)} tokens\")\n",
    "\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Erreur de gÃ©nÃ©ration: {e}\")\n",
    "            return \"Je... je ne sais pas quoi dire... *rougit*\"\n",
    "\n",
    "\n",
    "# CrÃ©ation du gÃ©nÃ©rateur ultra-optimisÃ©\n",
    "print(\"ðŸŒ¸ CrÃ©ation du gÃ©nÃ©rateur Shirayuki ultra-optimisÃ©...\")\n",
    "generator = UltraShirayukiGenerator(model, tokenizer, max_length)\n",
    "\n",
    "# Classe de monitoring avancÃ© des performances\n",
    "class UltraPerformanceMonitor:\n",
    "    \"\"\"Monitoring ultra-avancÃ© des performances systÃ¨me\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'cpu_usage': [],\n",
    "            'memory_usage': [],\n",
    "            'gpu_usage': [],\n",
    "            'training_speed': [],\n",
    "            'generation_speed': [],\n",
    "            'model_throughput': []\n",
    "        }\n",
    "        self.monitoring_active = False\n",
    "\n",
    "    def start_monitoring(self):\n",
    "        \"\"\"DÃ©marre le monitoring en temps rÃ©el\"\"\"\n",
    "        self.monitoring_active = True\n",
    "        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)\n",
    "        self.monitor_thread.start()\n",
    "        print(\"ðŸ“Š Monitoring ultra-performance dÃ©marrÃ©\")\n",
    "\n",
    "    def _monitor_loop(self):\n",
    "        \"\"\"Boucle de monitoring optimisÃ©e\"\"\"\n",
    "        import time\n",
    "        while self.monitoring_active:\n",
    "            # CPU et mÃ©moire\n",
    "            cpu_percent = psutil.cpu_percent(interval=0.1, percpu=False)\n",
    "            memory_info = psutil.virtual_memory()\n",
    "\n",
    "            self.metrics['cpu_usage'].append(cpu_percent)\n",
    "            self.metrics['memory_usage'].append(memory_info.percent)\n",
    "\n",
    "            # GPU (si disponible)\n",
    "            try:\n",
    "                import pynvml\n",
    "                pynvml.nvmlInit()\n",
    "                if pynvml.nvmlDeviceGetCount() > 0:\n",
    "                    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "                    gpu_util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "                    memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "\n",
    "                    self.metrics['gpu_usage'].append({\n",
    "                        'utilization': gpu_util.gpu,\n",
    "                        'memory_used': memory_info.used / memory_info.total * 100\n",
    "                    })\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "    def get_performance_summary(self):\n",
    "        \"\"\"RÃ©sumÃ© des performances\"\"\"\n",
    "        if not self.metrics['cpu_usage']:\n",
    "            return \"Monitoring non dÃ©marrÃ©\"\n",
    "\n",
    "        recent_cpu = self.metrics['cpu_usage'][-60:]  # DerniÃ¨re minute\n",
    "        recent_memory = self.metrics['memory_usage'][-60:]\n",
    "\n",
    "        avg_cpu = sum(recent_cpu) / len(recent_cpu)\n",
    "        avg_memory = sum(recent_memory) / len(recent_memory)\n",
    "        max_cpu = max(recent_cpu)\n",
    "        max_memory = max(recent_memory)\n",
    "\n",
    "        summary = f\"\"\"\n",
    "ðŸ“Š PERFORMANCES TEMPS RÃ‰EL:\n",
    "   CPU moyen: {avg_cpu:.1f}% (max: {max_cpu:.1f}%)\n",
    "   RAM moyenne: {avg_memory:.1f}% (max: {max_memory:.1f}%)\n",
    "   Utilisation cible: 95-100% pour performance maximale\n",
    "        \"\"\"\n",
    "\n",
    "        if self.metrics['gpu_usage']:\n",
    "            recent_gpu = self.metrics['gpu_usage'][-60:]\n",
    "            avg_gpu_util = sum(g['utilization'] for g in recent_gpu) / len(recent_gpu)\n",
    "            avg_gpu_mem = sum(g['memory_used'] for g in recent_gpu) / len(recent_gpu)\n",
    "            summary += f\"   GPU utilisation: {avg_gpu_util:.1f}%\\n\"\n",
    "            summary += f\"   GPU mÃ©moire: {avg_gpu_mem:.1f}%\"\n",
    "\n",
    "        return summary\n",
    "\n",
    "# Initialisation du monitoring ultra-performance\n",
    "ultra_monitor = UltraPerformanceMonitor()\n",
    "ultra_monitor.start_monitoring()\n",
    "\n",
    "# Callback ultra-optimisÃ© avec monitoring temps rÃ©el\n",
    "class UltraOptimizedCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback ultra-optimisÃ© pour performance maximale\"\"\"\n",
    "\n",
    "    def __init__(self, monitor_interval=10):\n",
    "        super().__init__()\n",
    "        self.monitor_interval = monitor_interval\n",
    "        self.batch_times = []\n",
    "        self.epoch_start_time = None\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = tf.timestamp()\n",
    "        print(f\"\\nðŸš€ Epoch {epoch + 1} - Optimisation maximale activÃ©e\")\n",
    "        print(ultra_monitor.get_performance_summary())\n",
    "\n",
    "        # Optimisation dynamique du garbage collector\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "        # Force la compilation XLA si pas encore fait\n",
    "        if epoch == 0:\n",
    "            print(\"âš¡ Compilation XLA en cours...\")\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        self.batch_start_time = tf.timestamp()\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        batch_time = float(tf.timestamp() - self.batch_start_time)\n",
    "        self.batch_times.append(batch_time)\n",
    "\n",
    "        if batch % self.monitor_interval == 0:\n",
    "            avg_batch_time = sum(self.batch_times[-10:]) / min(len(self.batch_times), 10)\n",
    "            throughput = 1.0 / avg_batch_time if avg_batch_time > 0 else 0\n",
    "\n",
    "            print(f\"   ðŸ“ˆ Batch {batch}: {avg_batch_time:.3f}s/batch, \"\n",
    "                  f\"Throughput: {throughput:.1f} batch/s\")\n",
    "\n",
    "            # Affichage pÃ©riodique des mÃ©triques\n",
    "            if batch % 50 == 0:\n",
    "                current_stats = resource_monitor.get_current_stats()\n",
    "                print(f\"   ðŸ”¥ Ressources: {current_stats}\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_time = float(tf.timestamp() - self.epoch_start_time)\n",
    "        avg_batch_time = sum(self.batch_times) / len(self.batch_times) if self.batch_times else 0\n",
    "\n",
    "        print(f\"âœ… Epoch {epoch + 1} terminÃ©e en {epoch_time:.1f}s\")\n",
    "        print(f\"ðŸ“Š Temps moyen par batch: {avg_batch_time:.3f}s\")\n",
    "        print(f\"ðŸŽ¯ Loss: {logs.get('loss', 0):.4f}, Accuracy: {logs.get('accuracy', 0):.4f}\")\n",
    "\n",
    "        self.batch_times = []  # Reset pour la prochaine epoch\n",
    "\n",
    "# Configuration ultra-optimisÃ©e de l'entraÃ®nement\n",
    "ultra_callback = UltraOptimizedCallback(monitor_interval=10)\n",
    "\n",
    "# Calcul du batch size optimal dynamique\n",
    "def calculate_optimal_batch_size():\n",
    "    \"\"\"Calcule le batch size optimal selon les ressources disponibles\"\"\"\n",
    "    base_batch_size = 16\n",
    "\n",
    "    # Adaptation selon la mÃ©moire disponible\n",
    "    memory_factor = min(4, available_memory / 4)  # Max 4x pour 16GB+\n",
    "\n",
    "    # Adaptation selon le nombre de CPU\n",
    "    cpu_factor = min(2, cpu_count / 8)  # Max 2x pour 8+ threads\n",
    "\n",
    "    # Adaptation selon le GPU\n",
    "    gpu_factor = 1.5 if gpus else 1.0\n",
    "\n",
    "    optimal_size = int(base_batch_size * memory_factor * cpu_factor * gpu_factor)\n",
    "    optimal_size = min(64, max(8, optimal_size))  # Entre 8 et 64\n",
    "\n",
    "    print(f\"ðŸ“Š Batch size optimal calculÃ©: {optimal_size}\")\n",
    "    print(f\"   Facteurs: mÃ©moire={memory_factor:.1f}, CPU={cpu_factor:.1f}, GPU={gpu_factor:.1f}\")\n",
    "\n",
    "    return optimal_size\n",
    "\n",
    "# Calcul et application du batch size optimal\n",
    "optimal_batch_size = calculate_optimal_batch_size()\n",
    "\n",
    "# RecrÃ©ation du dataset avec le batch size optimal\n",
    "print(\"âš¡ Reconfiguration du dataset avec batch size optimal...\")\n",
    "\n",
    "# Fonction de prÃ©traitement ultra-optimisÃ©e\n",
    "@tf.function(jit_compile=True)\n",
    "def preprocess_batch_ultra(batch):\n",
    "    \"\"\"PrÃ©traitement ultra-optimisÃ© des batches\"\"\"\n",
    "    encoder_input = batch['encoder_input']\n",
    "    decoder_input = batch['decoder_input']\n",
    "    decoder_target = batch['decoder_target']\n",
    "\n",
    "    # Optimisations sur les tenseurs\n",
    "    encoder_input = tf.cast(encoder_input, tf.int32)\n",
    "    decoder_input = tf.cast(decoder_input, tf.int32)\n",
    "    decoder_target = tf.cast(decoder_target, tf.int32)\n",
    "\n",
    "    return (encoder_input, decoder_input), decoder_target\n",
    "\n",
    "# Dataset ultra-optimisÃ© avec nouveau batch size\n",
    "optimized_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    'encoder_input': input_ids,\n",
    "    'decoder_input': decoder_input_ids,\n",
    "    'decoder_target': decoder_target_ids\n",
    "})\n",
    "\n",
    "# Application de toutes les optimisations maximales\n",
    "optimized_dataset = (optimized_dataset\n",
    "    .with_options(dataset_options)\n",
    "    .map(preprocess_batch_ultra, num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .shuffle(4096, reshuffle_each_iteration=True)\n",
    "    .batch(optimal_batch_size, drop_remainder=True, num_parallel_calls=AUTOTUNE)\n",
    "    .prefetch(AUTOTUNE)\n",
    "    .repeat())  # RÃ©pÃ©tition pour Ã©viter les fins d'epoch\n",
    "\n",
    "# Configuration avancÃ©e des callbacks\n",
    "ultra_callbacks = [\n",
    "    ultra_callback,\n",
    "    performance_callback,\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"loss\",\n",
    "        patience=8,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        min_delta=1e-5\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"loss\",\n",
    "        factor=0.7,\n",
    "        patience=4,\n",
    "        min_lr=1e-8,\n",
    "        verbose=1,\n",
    "        cooldown=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='ultra_shirayuki_{epoch:02d}_{loss:.4f}.keras',\n",
    "        monitor='loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1,\n",
    "        save_freq='epoch'\n",
    "    ),\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir='./logs/ultra_shirayuki',\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        update_freq=100,\n",
    "        profile_batch=(100, 120)\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\ude80 DÃ‰MARRAGE DE L'ENTRAÃŽNEMENT ULTRA-OPTIMISÃ‰\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"âš¡ Configuration finale:\")\n",
    "print(f\"   Batch size optimal: {optimal_batch_size}\")\n",
    "print(f\"   CPU threads: {cpu_count}\")\n",
    "print(f\"   MÃ©moire allouÃ©e: {int(available_memory * 0.8)} GB\")\n",
    "print(f\"   GPU disponibles: {len(gpus)}\")\n",
    "print(f\"   StratÃ©gie: {type(strategy).__name__}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calcul du nombre de steps optimal\n",
    "steps_per_epoch = len(conversation_pairs) // optimal_batch_size\n",
    "total_epochs = 30  # Plus d'epochs grÃ¢ce aux optimisations\n",
    "\n",
    "print(f\"ðŸ“Š Steps par epoch: {steps_per_epoch}\")\n",
    "print(f\"ðŸ“Š Total epochs: {total_epochs}\")\n",
    "\n",
    "# LANCEMENT DE L'ENTRAÃŽNEMENT ULTRA-OPTIMISÃ‰\n",
    "with strategy.scope():\n",
    "    history = model.fit(\n",
    "        optimized_dataset,\n",
    "        epochs=total_epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        callbacks=ultra_callbacks,\n",
    "        verbose=1,\n",
    "        workers=cpu_count,\n",
    "        use_multiprocessing=True,\n",
    "        max_queue_size=cpu_count * 3\n",
    "    )\n",
    "\n",
    "# ArrÃªt du monitoring\n",
    "ultra_monitor.monitoring_active = False\n",
    "resource_monitor.stop_monitoring()\n",
    "\n",
    "print(\"\\nðŸŽ‰ ENTRAÃŽNEMENT ULTRA-OPTIMISÃ‰ TERMINÃ‰!\")\n",
    "print(\"=\" * 60)\n",
    "print(ultra_monitor.get_performance_summary())\n",
    "print(\"âœ… ModÃ¨le Shirayuki ultra-optimisÃ© prÃªt pour gÃ©nÃ©ration maximale!\")\n",
    "\n",
    "# Test de performance du gÃ©nÃ©rateur\n",
    "print(\"\\nðŸ§ª Test de performance du gÃ©nÃ©rateur...\")\n",
    "test_prompts = [\n",
    "    \"Bonjour Shirayuki\",\n",
    "    \"Tu es vraiment mignonne\",\n",
    "    \"Comment Ã§a va ?\",\n",
    "    \"Je t'aime\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    response = generator.generate_response(prompt, temperature=0.8)\n",
    "    print(f\"ðŸŒ¸ {prompt} -> {response}\")\n",
    "\n",
    "print(\"\\nðŸš€ SYSTÃˆME OPTIMISÃ‰ Ã€ 100% POUR PERFORMANCES MAXIMALES!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86402d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION ULTRA-OPTIMISÃ‰E POUR CONVERSATIONS SHIRAYUKI\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, Dropout, Embedding, Conv1D\n",
    "import numpy as np\n",
    "\n",
    "# Configuration pour optimisation maximale\n",
    "tf.config.optimizer.set_jit(True)  # Active XLA JIT\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')  # Mixed precision\n",
    "\n",
    "# Configuration mÃ©moire GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "class FlashAttentionLike(tf.keras.layers.Layer):\n",
    "    \"\"\"ImplÃ©mentation d'une attention optimisÃ©e inspirÃ©e de Flash Attention\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Projection matrices optimisÃ©es (sans bias)\n",
    "        self.q_proj = Dense(embed_dim, use_bias=False, name='q_proj')\n",
    "        self.k_proj = Dense(embed_dim, use_bias=False, name='k_proj')\n",
    "        self.v_proj = Dense(embed_dim, use_bias=False, name='v_proj')\n",
    "        self.out_proj = Dense(embed_dim, use_bias=False, name='out_proj')\n",
    "\n",
    "        self.dropout_layer = Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        batch_size, seq_len = tf.shape(x)[0], tf.shape(x)[1]\n",
    "\n",
    "        # Projections\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        # Reshape pour multi-head\n",
    "        q = tf.reshape(q, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
    "        k = tf.reshape(k, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
    "        v = tf.reshape(v, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
    "\n",
    "        # Transpose pour dimensions (batch, heads, seq, head_dim)\n",
    "        q = tf.transpose(q, [0, 2, 1, 3])\n",
    "        k = tf.transpose(k, [0, 2, 1, 3])\n",
    "        v = tf.transpose(v, [0, 2, 1, 3])\n",
    "\n",
    "        # Attention optimisÃ©e\n",
    "        attention_scores = tf.matmul(q, k, transpose_b=True) * self.scale\n",
    "\n",
    "        # Application du masque\n",
    "        if mask is not None:\n",
    "            mask = tf.cast(mask, dtype=attention_scores.dtype)\n",
    "            attention_scores += (mask * -1e9)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "        attention_weights = self.dropout_layer(attention_weights, training=training)\n",
    "\n",
    "        # Application Ã  V\n",
    "        attention_output = tf.matmul(attention_weights, v)\n",
    "\n",
    "        # Reshape pour output\n",
    "        attention_output = tf.transpose(attention_output, [0, 2, 1, 3])\n",
    "        attention_output = tf.reshape(attention_output, (batch_size, seq_len, self.embed_dim))\n",
    "\n",
    "        return self.out_proj(attention_output)\n",
    "\n",
    "\n",
    "class UltraOptimizedConversationTransformer(tf.keras.Model):\n",
    "    \"\"\"Version ultra-optimisÃ©e du transformer pour conversations\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim=768, num_heads=12, ff_dim=3072,\n",
    "                 maxlen=128, num_layers=8, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Embedding avec initialisation optimisÃ©e\n",
    "        self.embedding = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embed_dim,\n",
    "            embeddings_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "            mask_zero=True\n",
    "        )\n",
    "\n",
    "        # Position encoding learnable (plus flexible que RoPE pour ce cas)\n",
    "        self.pos_embedding = Embedding(maxlen, embed_dim)\n",
    "\n",
    "        # Encoder layers avec Flash Attention\n",
    "        self.encoder_layers = [\n",
    "            self._create_ultra_layer(embed_dim, num_heads, ff_dim, rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_layers = [\n",
    "            self._create_ultra_layer(embed_dim, num_heads, ff_dim, rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        # Normalisation finale optimisÃ©e\n",
    "        self.final_norm = RMSNormalization(epsilon=1e-6)\n",
    "\n",
    "        # Output projection avec weight tying\n",
    "        self.output_layer = Dense(vocab_size, use_bias=False, dtype='float32')\n",
    "\n",
    "        # Dropout globaux\n",
    "        self.encoder_dropout = Dropout(rate)\n",
    "        self.decoder_dropout = Dropout(rate)\n",
    "\n",
    "    def _create_ultra_layer(self, embed_dim, num_heads, ff_dim, rate):\n",
    "        \"\"\"CrÃ©e une couche transformer ultra-optimisÃ©e\"\"\"\n",
    "        return tf.keras.Sequential([\n",
    "            RMSNormalization(epsilon=1e-6),\n",
    "            FlashAttentionLike(embed_dim, num_heads, rate),\n",
    "            RMSNormalization(epsilon=1e-6),\n",
    "            Dense(ff_dim * 2, use_bias=False, activation=None),\n",
    "            tf.keras.layers.Lambda(lambda x: self._swiglu(x)),\n",
    "            Dense(embed_dim, use_bias=False),\n",
    "            Dropout(rate)\n",
    "        ])\n",
    "\n",
    "    def _swiglu(self, x):\n",
    "        \"\"\"SwiGLU activation optimisÃ©e\"\"\"\n",
    "        gate, hidden = tf.split(x, 2, axis=-1)\n",
    "        return tf.nn.swish(gate) * hidden\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        if isinstance(inputs, tuple):\n",
    "            input_ids, target_ids = inputs\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "            target_ids = None\n",
    "\n",
    "        # Encoder\n",
    "        encoder_output = self.encode(input_ids, training=training)\n",
    "\n",
    "        # Decoder\n",
    "        if target_ids is not None:\n",
    "            decoder_output = self.decode(target_ids, encoder_output, training=training)\n",
    "            return self.output_layer(decoder_output)\n",
    "        else:\n",
    "            return encoder_output\n",
    "\n",
    "    def encode(self, input_ids, training=False):\n",
    "        \"\"\"Encoder ultra-optimisÃ©\"\"\"\n",
    "        seq_len = tf.shape(input_ids)[1]\n",
    "\n",
    "        # Embedding + position\n",
    "        x = self.embedding(input_ids)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "\n",
    "        positions = tf.range(seq_len)[None, :]\n",
    "        x += self.pos_embedding(positions)\n",
    "        x = self.encoder_dropout(x, training=training)\n",
    "\n",
    "        # Masque de padding\n",
    "        mask = tf.cast(tf.equal(input_ids, 0), tf.float32)\n",
    "        attention_mask = mask[:, None, None, :] * -1e9\n",
    "\n",
    "        # Passage dans les couches avec gradient checkpointing\n",
    "        for i, layer in enumerate(self.encoder_layers):\n",
    "            if training and i > 0:  # Gradient checkpointing\n",
    "                x = tf.recompute_grad(lambda inputs: layer(inputs, training=training))(x)\n",
    "            else:\n",
    "                x = layer(x, training=training)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decode(self, target_ids, encoder_output, training=False):\n",
    "        \"\"\"Decoder ultra-optimisÃ© avec masques causaux\"\"\"\n",
    "        seq_len = tf.shape(target_ids)[1]\n",
    "\n",
    "        # Embedding + position\n",
    "        x = self.embedding(target_ids)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "\n",
    "        positions = tf.range(seq_len)[None, :]\n",
    "        x += self.pos_embedding(positions)\n",
    "        x = self.decoder_dropout(x, training=training)\n",
    "\n",
    "        # Masques causaux et de padding\n",
    "        causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        padding_mask = tf.cast(tf.not_equal(target_ids, 0), tf.float32)\n",
    "\n",
    "        combined_mask = tf.minimum(\n",
    "            causal_mask[None, None, :, :],\n",
    "            padding_mask[:, None, None, :]\n",
    "        )\n",
    "        attention_mask = (1.0 - combined_mask) * -1e9\n",
    "\n",
    "        # Passage dans les couches decoder\n",
    "        for i, layer in enumerate(self.decoder_layers):\n",
    "            if training and i > 0:\n",
    "                x = tf.recompute_grad(lambda inputs: layer(inputs, training=training))(x)\n",
    "            else:\n",
    "                x = layer(x, training=training)\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Optimisations spÃ©cifiques pour les conversations\n",
    "class ConversationOptimizer:\n",
    "    \"\"\"Optimisations spÃ©cialisÃ©es pour les modÃ¨les conversationnels\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create_conversation_dataset(conversation_pairs, tokenizer, batch_size=8, augment=True):\n",
    "        \"\"\"CrÃ©e un dataset optimisÃ© pour l'entraÃ®nement conversationnel\"\"\"\n",
    "\n",
    "        # PrÃ©paration des donnÃ©es avec augmentation\n",
    "        if augment:\n",
    "            conversation_pairs = ConversationOptimizer._augment_conversations(conversation_pairs)\n",
    "\n",
    "        # Tokenisation\n",
    "        inputs = [pair[0] for pair in conversation_pairs]\n",
    "        outputs = [\"[START] \" + pair[1] + \" [END]\" for pair in conversation_pairs]\n",
    "\n",
    "        input_ids = tokenizer(inputs)\n",
    "        output_ids = tokenizer(outputs)\n",
    "\n",
    "        # Teacher forcing setup\n",
    "        decoder_input_ids = output_ids[:, :-1]\n",
    "        decoder_target_ids = output_ids[:, 1:]\n",
    "\n",
    "        # Dataset avec optimisations mÃ©moire\n",
    "        dataset = tf.data.Dataset.from_tensor_slices({\n",
    "            'encoder_input': input_ids,\n",
    "            'decoder_input': decoder_input_ids,\n",
    "            'decoder_target': decoder_target_ids\n",
    "        })\n",
    "\n",
    "        def prepare_batch(batch):\n",
    "            return (\n",
    "                (batch['encoder_input'], batch['decoder_input']),\n",
    "                batch['decoder_target']\n",
    "            )\n",
    "\n",
    "        return (dataset\n",
    "                .map(prepare_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                .cache()\n",
    "                .shuffle(1024, reshuffle_each_iteration=True)\n",
    "                .batch(batch_size, drop_remainder=True)\n",
    "                .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "    @staticmethod\n",
    "    def _augment_conversations(conversation_pairs):\n",
    "        \"\"\"Augmente le dataset avec des variations\"\"\"\n",
    "        augmented = list(conversation_pairs)\n",
    "\n",
    "        # Synonymes simples pour augmentation\n",
    "        synonyms = {\n",
    "            'love': ['adore', 'care for', 'cherish'],\n",
    "            'great': ['amazing', 'wonderful', 'fantastic'],\n",
    "            'cute': ['adorable', 'sweet', 'lovely']\n",
    "        }\n",
    "\n",
    "        for inp, out in conversation_pairs[:len(conversation_pairs)//3]:  # Augmente 1/3 des donnÃ©es\n",
    "            # Remplacement de synonymes\n",
    "            for word, syns in synonyms.items():\n",
    "                if word in inp.lower():\n",
    "                    for syn in syns[:1]:  # Une seule variation par mot\n",
    "                        new_inp = inp.lower().replace(word, syn)\n",
    "                        augmented.append((new_inp, out))\n",
    "\n",
    "        return augmented\n",
    "\n",
    "\n",
    "# Fonction de comparaison des modÃ¨les\n",
    "def compare_conversation_models():\n",
    "    \"\"\"Compare les diffÃ©rentes versions du modÃ¨le de conversation\"\"\"\n",
    "\n",
    "    print(\"ðŸ”„ Comparaison des modÃ¨les de conversation\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # ModÃ¨le de base (dÃ©jÃ  crÃ©Ã©)\n",
    "    print(f\"ðŸ“Š ModÃ¨le de base:\")\n",
    "    print(f\"   ParamÃ¨tres: {model.count_params():,}\")\n",
    "    print(f\"   Embed dim: {embed_dim}\")\n",
    "    print(f\"   Layers: {num_layers}\")\n",
    "\n",
    "    # ModÃ¨le ultra-optimisÃ©\n",
    "    ultra_model = UltraOptimizedConversationTransformer(\n",
    "        vocab_size=tokenizer.vocabulary_size(),\n",
    "        embed_dim=768,\n",
    "        num_heads=12,\n",
    "        ff_dim=3072,\n",
    "        maxlen=max_length,\n",
    "        num_layers=8,\n",
    "        rate=0.1\n",
    "    )\n",
    "\n",
    "    print(f\"\\nðŸ“Š ModÃ¨le ultra-optimisÃ©:\")\n",
    "    print(f\"   ParamÃ¨tres: {ultra_model.count_params():,}\")\n",
    "    print(f\"   Embed dim: 768\")\n",
    "    print(f\"   Layers: 8\")\n",
    "\n",
    "    print(f\"\\nâœ¨ Optimisations ultra appliquÃ©es:\")\n",
    "    print(f\"   âœ“ Flash Attention optimisÃ©e\")\n",
    "    print(f\"   âœ“ SwiGLU activation\")\n",
    "    print(f\"   âœ“ RMSNormalization\")\n",
    "    print(f\"   âœ“ Gradient checkpointing\")\n",
    "    print(f\"   âœ“ Mixed precision training\")\n",
    "    print(f\"   âœ“ Optimisations mÃ©moire GPU\")\n",
    "\n",
    "    return ultra_model\n",
    "\n",
    "\n",
    "print(\"Version ultra-optimisÃ©e chargÃ©e pour les conversations Shirayuki!\")\n",
    "print(\"Utilisez compare_conversation_models() pour voir les diffÃ©rences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582fa340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã‰VALUATION DES PERFORMANCES CONVERSATIONNELLES\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"ðŸ“ˆ Analyse des performances du modÃ¨le Shirayuki\")\n",
    "\n",
    "# Graphiques de l'entraÃ®nement\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss', color='#FF6B9D')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss', color='#4ECDC4')\n",
    "plt.title('Loss Evolution - Shirayuki Model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy', color='#FF6B9D')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy', color='#4ECDC4')\n",
    "plt.title('Accuracy Evolution')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning Rate\n",
    "plt.subplot(2, 3, 3)\n",
    "lr_values = [warmup_schedule(step) for step in range(0, total_steps, total_steps // epochs)]\n",
    "plt.plot(lr_values, color='#FFE66D')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Test de gÃ©nÃ©ration avec mÃ©triques\n",
    "plt.subplot(2, 3, 4)\n",
    "test_prompts = [\n",
    "    \"Bonjour Shirayuki\",\n",
    "    \"Comment Ã§a va ?\",\n",
    "    \"Tu es mignonne\",\n",
    "    \"Je t'aime\",\n",
    "    \"Qu'est-ce que tu fais ?\"\n",
    "]\n",
    "\n",
    "response_lengths = []\n",
    "response_qualities = []\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    response = generator.generate_response(prompt, max_length=50)\n",
    "    response_lengths.append(len(response.split()))\n",
    "    # Score de qualitÃ© basique (diversitÃ© des mots)\n",
    "    unique_words = len(set(response.lower().split()))\n",
    "    total_words = len(response.split())\n",
    "    quality_score = unique_words / max(total_words, 1)\n",
    "    response_qualities.append(quality_score)\n",
    "\n",
    "plt.bar(range(len(test_prompts)), response_lengths, color='#FF6B9D', alpha=0.7)\n",
    "plt.title('Longueur des RÃ©ponses GÃ©nÃ©rÃ©es')\n",
    "plt.xlabel('Prompt Test')\n",
    "plt.ylabel('Nombre de Mots')\n",
    "plt.xticks(range(len(test_prompts)), [f\"Test {i+1}\" for i in range(len(test_prompts))])\n",
    "\n",
    "# QualitÃ© des rÃ©ponses\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.bar(range(len(test_prompts)), response_qualities, color='#4ECDC4', alpha=0.7)\n",
    "plt.title('DiversitÃ© Lexicale des RÃ©ponses')\n",
    "plt.xlabel('Prompt Test')\n",
    "plt.ylabel('Score de DiversitÃ©')\n",
    "plt.xticks(range(len(test_prompts)), [f\"Test {i+1}\" for i in range(len(test_prompts))])\n",
    "\n",
    "# Statistiques du modÃ¨le\n",
    "plt.subplot(2, 3, 6)\n",
    "model_stats = {\n",
    "    'ParamÃ¨tres': model.count_params(),\n",
    "    'Vocab Size': tokenizer.vocabulary_size(),\n",
    "    'Max Length': max_length,\n",
    "    'Embed Dim': embed_dim,\n",
    "    'Layers': num_layers,\n",
    "    'Heads': num_heads\n",
    "}\n",
    "\n",
    "plt.barh(list(model_stats.keys()), list(model_stats.values()), color='#FFE66D')\n",
    "plt.title('Statistiques du ModÃ¨le')\n",
    "plt.xlabel('Valeur')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# MÃ©triques dÃ©taillÃ©es\n",
    "print(\"\\nðŸ“Š MÃ‰TRIQUES DÃ‰TAILLÃ‰ES\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ðŸ† Meilleure Val Loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"ðŸŽ¯ Meilleure Val Accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "print(f\"ðŸ“ˆ AmÃ©lioration totale: {(max(history.history['val_accuracy']) - min(history.history['val_accuracy'])):.4f}\")\n",
    "\n",
    "# Test de conversation tsundere\n",
    "print(f\"\\nðŸ’¬ TEST DE PERSONNALITÃ‰ TSUNDERE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "tsundere_tests = [\n",
    "    \"Tu es vraiment adorable\",\n",
    "    \"Je pense Ã  toi tout le temps\",\n",
    "    \"Tu me manques\",\n",
    "    \"Tu veux Ãªtre mon amie ?\",\n",
    "    \"Tu es la plus belle\"\n",
    "]\n",
    "\n",
    "for i, test in enumerate(tsundere_tests, 1):\n",
    "    response = generator.generate_response(test, max_length=30, temperature=0.8)\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"   Input: {test}\")\n",
    "    print(f\"   Shirayuki: {response}\")\n",
    "    print()\n",
    "\n",
    "# Sauvegarde des mÃ©triques\n",
    "performance_data = {\n",
    "    'final_val_loss': min(history.history['val_loss']),\n",
    "    'final_val_accuracy': max(history.history['val_accuracy']),\n",
    "    'model_params': model.count_params(),\n",
    "    'training_epochs': len(history.history['loss']),\n",
    "    'response_qualities': response_qualities,\n",
    "    'response_lengths': response_lengths\n",
    "}\n",
    "\n",
    "print(f\"âœ… Ã‰valuation terminÃ©e! ModÃ¨le Shirayuki optimisÃ© et testÃ©.\")\n",
    "print(f\"ðŸ“ MÃ©triques sauvegardÃ©es pour analyse future.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da506b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHAT INTERACTIF AVEC SHIRAYUKI\n",
    "print(\"ðŸ’¬ Interface de Chat avec Shirayuki\")\n",
    "print(\"Tapez 'quit' pour arrÃªter la conversation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def shirayuki_chat_interface():\n",
    "    \"\"\"Interface de chat interactive avec personnalisation\"\"\"\n",
    "\n",
    "    print(\"ðŸŒ¸ Shirayuki: Bonjour ! Je suis Shirayuki... *rougit* Qu'est-ce que tu veux ?\")\n",
    "\n",
    "    conversation_history = []\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Input utilisateur\n",
    "            user_input = input(\"\\nðŸ‘¤ Vous: \").strip()\n",
    "\n",
    "            if user_input.lower() in ['quit', 'exit', 'bye', 'au revoir']:\n",
    "                print(\"ðŸŒ¸ Shirayuki: *dÃ©tourne le regard* C-ce n'est pas comme si j'allais te manquer ! Ã€ bientÃ´t...\")\n",
    "                break\n",
    "\n",
    "            if not user_input:\n",
    "                continue\n",
    "\n",
    "            # GÃ©nÃ©ration de la rÃ©ponse\n",
    "            print(\"ðŸŒ¸ Shirayuki: \", end=\"\", flush=True)\n",
    "\n",
    "            # DiffÃ©rents modes de gÃ©nÃ©ration selon le contexte\n",
    "            if any(word in user_input.lower() for word in ['love', 'aime', 'amour', 'cute', 'mignon']):\n",
    "                # Mode tsundere intensifiÃ©\n",
    "                response = generator.generate_response(\n",
    "                    user_input,\n",
    "                    max_length=40,\n",
    "                    temperature=0.9,\n",
    "                    top_p=0.85\n",
    "                )\n",
    "            elif any(word in user_input.lower() for word in ['triste', 'sad', 'problÃ¨me', 'mal']):\n",
    "                # Mode plus doux\n",
    "                response = generator.generate_response(\n",
    "                    user_input,\n",
    "                    max_length=35,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9\n",
    "                )\n",
    "            else:\n",
    "                # Mode normal\n",
    "                response = generator.generate_response(\n",
    "                    user_input,\n",
    "                    max_length=30,\n",
    "                    temperature=0.8,\n",
    "                    top_p=0.87\n",
    "                )\n",
    "\n",
    "            print(response)\n",
    "\n",
    "            # Sauvegarde de l'historique\n",
    "            conversation_history.append({\n",
    "                'user': user_input,\n",
    "                'shirayuki': response,\n",
    "                'timestamp': tf.timestamp()\n",
    "            })\n",
    "\n",
    "            # Suggestions de rÃ©ponses\n",
    "            if len(conversation_history) % 3 == 0:\n",
    "                suggestions = [\n",
    "                    \"Comment tu te sens ?\",\n",
    "                    \"Raconte-moi ta journÃ©e\",\n",
    "                    \"Tu es trÃ¨s mignonne\",\n",
    "                    \"Qu'est-ce que tu aimes faire ?\"\n",
    "                ]\n",
    "                print(f\"ðŸ’¡ Suggestions: {' | '.join(suggestions)}\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nðŸŒ¸ Shirayuki: *surprise* Tu pars dÃ©jÃ  ? Bon... Ã  bientÃ´t alors...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Erreur: {e}\")\n",
    "            print(\"ðŸŒ¸ Shirayuki: *confuse* Je... je n'ai pas compris. Peux-tu rÃ©pÃ©ter ?\")\n",
    "\n",
    "    # RÃ©sumÃ© de la conversation\n",
    "    if conversation_history:\n",
    "        print(f\"\\nðŸ“Š RÃ©sumÃ© de la conversation:\")\n",
    "        print(f\"   Messages Ã©changÃ©s: {len(conversation_history)}\")\n",
    "        avg_length = sum(len(conv['shirayuki'].split()) for conv in conversation_history) / len(conversation_history)\n",
    "        print(f\"   Longueur moyenne des rÃ©ponses: {avg_length:.1f} mots\")\n",
    "\n",
    "        # Sauvegarde optionnelle\n",
    "        save = input(\"\\nðŸ’¾ Sauvegarder cette conversation ? (y/n): \").lower().startswith('y')\n",
    "        if save:\n",
    "            import json\n",
    "            with open('shirayuki_conversation.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(conversation_history, f, ensure_ascii=False, indent=2, default=str)\n",
    "            print(\"âœ… Conversation sauvegardÃ©e dans 'shirayuki_conversation.json'\")\n",
    "\n",
    "# Lancement du chat\n",
    "shirayuki_chat_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505afda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS DE STRESS ET BENCHMARKS ULTRA-PERFORMANCE\n",
    "import time\n",
    "import threading\n",
    "import concurrent.futures\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"ðŸ”¥ TESTS DE STRESS POUR VALIDATION DE L'UTILISATION MAXIMALE DES RESSOURCES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class UltraStressTester:\n",
    "    \"\"\"Testeur de stress ultra-avancÃ© pour valider l'utilisation maximale des ressources\"\"\"\n",
    "\n",
    "    def __init__(self, model, generator, tokenizer):\n",
    "        self.model = model\n",
    "        self.generator = generator\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stress_results = {}\n",
    "\n",
    "    def cpu_stress_test(self, duration=60):\n",
    "        \"\"\"Test de stress CPU avec gÃ©nÃ©ration massive\"\"\"\n",
    "        print(f\"ðŸ”¥ Test de stress CPU ({duration}s)...\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        generations_count = 0\n",
    "        cpu_usage_samples = []\n",
    "\n",
    "        def cpu_monitor():\n",
    "            while time.time() - start_time < duration:\n",
    "                cpu_usage_samples.append(psutil.cpu_percent(interval=0.5))\n",
    "\n",
    "        # DÃ©marrage du monitoring CPU\n",
    "        monitor_thread = threading.Thread(target=cpu_monitor, daemon=True)\n",
    "        monitor_thread.start()\n",
    "\n",
    "        # GÃ©nÃ©ration massive pour stresser le CPU\n",
    "        test_prompts = [\n",
    "            \"Test de performance CPU\",\n",
    "            \"Stress test maximum\",\n",
    "            \"Utilisation optimale\",\n",
    "            \"Performance benchmark\",\n",
    "            \"Test de charge\"\n",
    "        ] * 10  # 50 prompts diffÃ©rents\n",
    "\n",
    "        def generate_response(prompt):\n",
    "            return self.generator.generate_response(f\"{prompt} {time.time()}\", max_length=30)\n",
    "\n",
    "        # GÃ©nÃ©ration parallÃ¨le massive\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count) as executor:\n",
    "            while time.time() - start_time < duration:\n",
    "                futures = [executor.submit(generate_response, prompt) for prompt in test_prompts]\n",
    "                concurrent.futures.wait(futures, timeout=5)\n",
    "                generations_count += len(futures)\n",
    "\n",
    "        monitor_thread.join(timeout=1)\n",
    "\n",
    "        avg_cpu = sum(cpu_usage_samples) / len(cpu_usage_samples) if cpu_usage_samples else 0\n",
    "        max_cpu = max(cpu_usage_samples) if cpu_usage_samples else 0\n",
    "        throughput = generations_count / duration\n",
    "\n",
    "        self.stress_results['cpu_test'] = {\n",
    "            'avg_cpu_usage': avg_cpu,\n",
    "            'max_cpu_usage': max_cpu,\n",
    "            'generations_per_second': throughput,\n",
    "            'total_generations': generations_count\n",
    "        }\n",
    "\n",
    "        print(f\"   âœ… CPU moyen: {avg_cpu:.1f}% (max: {max_cpu:.1f}%)\")\n",
    "        print(f\"   âš¡ Throughput: {throughput:.1f} gÃ©nÃ©rations/sec\")\n",
    "        print(f\"   ðŸ“Š Total gÃ©nÃ©rations: {generations_count}\")\n",
    "\n",
    "        return self.stress_results['cpu_test']\n",
    "\n",
    "    def memory_stress_test(self, batch_size_multiplier=4):\n",
    "        \"\"\"Test de stress mÃ©moire avec batch sizes Ã©normes\"\"\"\n",
    "        print(f\"ðŸ’¾ Test de stress mÃ©moire (x{batch_size_multiplier} batch size)...\")\n",
    "\n",
    "        # Sauvegarde de la configuration actuelle\n",
    "        original_batch_size = optimal_batch_size\n",
    "        stress_batch_size = original_batch_size * batch_size_multiplier\n",
    "\n",
    "        try:\n",
    "            # CrÃ©ation d'un dataset de stress avec batch size Ã©norme\n",
    "            stress_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "                'encoder_input': input_ids[:stress_batch_size*10],  # Plus de donnÃ©es\n",
    "                'decoder_input': decoder_input_ids[:stress_batch_size*10],\n",
    "                'decoder_target': decoder_target_ids[:stress_batch_size*10]\n",
    "            })\n",
    "\n",
    "            stress_dataset = (stress_dataset\n",
    "                .batch(stress_batch_size, drop_remainder=True)\n",
    "                .prefetch(1))\n",
    "\n",
    "            # Monitoring mÃ©moire\n",
    "            memory_before = psutil.virtual_memory().percent\n",
    "            gpu_memory_before = 0\n",
    "\n",
    "            if gpus:\n",
    "                try:\n",
    "                    import pynvml\n",
    "                    pynvml.nvmlInit()\n",
    "                    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "                    gpu_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "                    gpu_memory_before = gpu_info.used / gpu_info.total * 100\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # Test de forward pass avec batch Ã©norme\n",
    "            start_time = time.time()\n",
    "            memory_samples = []\n",
    "\n",
    "            def memory_monitor():\n",
    "                for _ in range(30):  # 30 secondes de monitoring\n",
    "                    memory_samples.append(psutil.virtual_memory().percent)\n",
    "                    time.sleep(1)\n",
    "\n",
    "            monitor_thread = threading.Thread(target=memory_monitor, daemon=True)\n",
    "            monitor_thread.start()\n",
    "\n",
    "            # ExÃ©cution du stress test\n",
    "            batch_count = 0\n",
    "            for batch in stress_dataset.take(5):  # 5 gros batches\n",
    "                inputs, targets = batch\n",
    "                with tf.GradientTape() as tape:\n",
    "                    predictions = self.model((inputs[0], inputs[1]), training=True)\n",
    "                    loss = tf.keras.losses.sparse_categorical_crossentropy(targets, predictions, from_logits=True)\n",
    "\n",
    "                # Calcul des gradients pour stresser davantage\n",
    "                gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "                batch_count += 1\n",
    "\n",
    "                print(f\"   Batch {batch_count}: Shape {inputs[0].shape}, Loss {tf.reduce_mean(loss):.4f}\")\n",
    "\n",
    "            execution_time = time.time() - start_time\n",
    "            monitor_thread.join(timeout=1)\n",
    "\n",
    "            # Mesure finale de la mÃ©moire\n",
    "            memory_after = psutil.virtual_memory().percent\n",
    "            max_memory = max(memory_samples) if memory_samples else memory_after\n",
    "\n",
    "            self.stress_results['memory_test'] = {\n",
    "                'memory_before': memory_before,\n",
    "                'memory_after': memory_after,\n",
    "                'max_memory_usage': max_memory,\n",
    "                'memory_increase': memory_after - memory_before,\n",
    "                'stress_batch_size': stress_batch_size,\n",
    "                'execution_time': execution_time,\n",
    "                'batches_processed': batch_count\n",
    "            }\n",
    "\n",
    "            print(f\"   âœ… MÃ©moire avant: {memory_before:.1f}%\")\n",
    "            print(f\"   ðŸ“ˆ MÃ©moire max: {max_memory:.1f}%\")\n",
    "            print(f\"   ðŸ’¾ Augmentation: +{memory_after - memory_before:.1f}%\")\n",
    "            print(f\"   âš¡ Temps d'exÃ©cution: {execution_time:.1f}s\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Limite mÃ©moire atteinte: {e}\")\n",
    "            self.stress_results['memory_test'] = {'error': str(e), 'limit_reached': True}\n",
    "\n",
    "        return self.stress_results.get('memory_test', {})\n",
    "\n",
    "    def gpu_stress_test(self, duration=30):\n",
    "        \"\"\"Test de stress GPU avec calculs intensifs\"\"\"\n",
    "        if not gpus:\n",
    "            print(\"âš ï¸ Aucun GPU dÃ©tectÃ© - test ignorÃ©\")\n",
    "            return {}\n",
    "\n",
    "        print(f\"ðŸŽ® Test de stress GPU ({duration}s)...\")\n",
    "\n",
    "        try:\n",
    "            import pynvml\n",
    "            pynvml.nvmlInit()\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "            # Monitoring GPU\n",
    "            gpu_utils = []\n",
    "            gpu_memory = []\n",
    "\n",
    "            def gpu_monitor():\n",
    "                start = time.time()\n",
    "                while time.time() - start < duration:\n",
    "                    try:\n",
    "                        util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "                        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "                        gpu_utils.append(util.gpu)\n",
    "                        gpu_memory.append(mem_info.used / mem_info.total * 100)\n",
    "                        time.sleep(0.5)\n",
    "                    except:\n",
    "                        break\n",
    "\n",
    "            monitor_thread = threading.Thread(target=gpu_monitor, daemon=True)\n",
    "            monitor_thread.start()\n",
    "\n",
    "            # Stress test avec calculs intensifs\n",
    "            start_time = time.time()\n",
    "            operations_count = 0\n",
    "\n",
    "            while time.time() - start_time < duration:\n",
    "                # CrÃ©ation de tenseurs volumineux pour stresser le GPU\n",
    "                with tf.device('/GPU:0'):\n",
    "                    large_tensor = tf.random.normal([1024, 1024, 256])\n",
    "                    result = tf.linalg.matmul(large_tensor, large_tensor, transpose_b=True)\n",
    "\n",
    "                    # OpÃ©rations sur le modÃ¨le\n",
    "                    dummy_input = tf.random.uniform([16, 64], maxval=1000, dtype=tf.int32)\n",
    "                    _ = self.model.encode(dummy_input, training=True)\n",
    "\n",
    "                    operations_count += 1\n",
    "\n",
    "            monitor_thread.join(timeout=1)\n",
    "\n",
    "            avg_gpu_util = sum(gpu_utils) / len(gpu_utils) if gpu_utils else 0\n",
    "            max_gpu_util = max(gpu_utils) if gpu_utils else 0\n",
    "            avg_gpu_memory = sum(gpu_memory) / len(gpu_memory) if gpu_memory else 0\n",
    "            max_gpu_memory = max(gpu_memory) if gpu_memory else 0\n",
    "\n",
    "            self.stress_results['gpu_test'] = {\n",
    "                'avg_gpu_utilization': avg_gpu_util,\n",
    "                'max_gpu_utilization': max_gpu_util,\n",
    "                'avg_gpu_memory': avg_gpu_memory,\n",
    "                'max_gpu_memory': max_gpu_memory,\n",
    "                'operations_per_second': operations_count / duration,\n",
    "                'total_operations': operations_count\n",
    "            }\n",
    "\n",
    "            print(f\"   âœ… GPU utilisation moyenne: {avg_gpu_util:.1f}% (max: {max_gpu_util:.1f}%)\")\n",
    "            print(f\"   ðŸ’¾ GPU mÃ©moire moyenne: {avg_gpu_memory:.1f}% (max: {max_gpu_memory:.1f}%)\")\n",
    "            print(f\"   âš¡ OpÃ©rations/sec: {operations_count / duration:.1f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Erreur GPU: {e}\")\n",
    "            self.stress_results['gpu_test'] = {'error': str(e)}\n",
    "\n",
    "        return self.stress_results.get('gpu_test', {})\n",
    "\n",
    "    def concurrent_stress_test(self, duration=45):\n",
    "        \"\"\"Test de stress concurrent CPU+GPU+MÃ©moire\"\"\"\n",
    "        print(f\"ðŸ”¥ Test de stress concurrent ({duration}s)...\")\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        def cpu_task():\n",
    "            # GÃ©nÃ©ration continue\n",
    "            count = 0\n",
    "            start = time.time()\n",
    "            while time.time() - start < duration:\n",
    "                self.generator.generate_response(f\"concurrent test {count}\", max_length=20)\n",
    "                count += 1\n",
    "            return count\n",
    "\n",
    "        def gpu_task():\n",
    "            # Calculs GPU intensifs\n",
    "            count = 0\n",
    "            start = time.time()\n",
    "            while time.time() - start < duration:\n",
    "                if gpus:\n",
    "                    with tf.device('/GPU:0'):\n",
    "                        x = tf.random.normal([512, 512])\n",
    "                        _ = tf.linalg.matmul(x, x)\n",
    "                count += 1\n",
    "            return count\n",
    "\n",
    "        def memory_task():\n",
    "            # Allocations mÃ©moire intensives\n",
    "            arrays = []\n",
    "            start = time.time()\n",
    "            while time.time() - start < duration:\n",
    "                try:\n",
    "                    # Allocation de 100MB\n",
    "                    arr = np.random.random((1000, 1000, 10))\n",
    "                    arrays.append(arr)\n",
    "                    if len(arrays) > 10:  # Limite pour Ã©viter l'explosion mÃ©moire\n",
    "                        arrays.pop(0)\n",
    "                except MemoryError:\n",
    "                    break\n",
    "            return len(arrays)\n",
    "\n",
    "        # Monitoring global\n",
    "        system_stats = []\n",
    "\n",
    "        def system_monitor():\n",
    "            start = time.time()\n",
    "            while time.time() - start < duration:\n",
    "                cpu_percent = psutil.cpu_percent(interval=0.1)\n",
    "                memory_percent = psutil.virtual_memory().percent\n",
    "\n",
    "                gpu_util = 0\n",
    "                if gpus:\n",
    "                    try:\n",
    "                        import pynvml\n",
    "                        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "                        util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "                        gpu_util = util.gpu\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                system_stats.append({\n",
    "                    'cpu': cpu_percent,\n",
    "                    'memory': memory_percent,\n",
    "                    'gpu': gpu_util,\n",
    "                    'timestamp': time.time()\n",
    "                })\n",
    "                time.sleep(1)\n",
    "\n",
    "        # Lancement concurrent de tous les tests\n",
    "        print(\"   ðŸš€ Lancement des tÃ¢ches concurrentes...\")\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            # Soumission des tÃ¢ches\n",
    "            cpu_future = executor.submit(cpu_task)\n",
    "            gpu_future = executor.submit(gpu_task)\n",
    "            memory_future = executor.submit(memory_task)\n",
    "            monitor_future = executor.submit(system_monitor)\n",
    "\n",
    "            # Attente de completion\n",
    "            cpu_result = cpu_future.result()\n",
    "            gpu_result = gpu_future.result()\n",
    "            memory_result = memory_future.result()\n",
    "            monitor_future.result()\n",
    "\n",
    "        # Analyse des rÃ©sultats\n",
    "        if system_stats:\n",
    "            avg_cpu = sum(s['cpu'] for s in system_stats) / len(system_stats)\n",
    "            max_cpu = max(s['cpu'] for s in system_stats)\n",
    "            avg_memory = sum(s['memory'] for s in system_stats) / len(system_stats)\n",
    "            max_memory = max(s['memory'] for s in system_stats)\n",
    "            avg_gpu = sum(s['gpu'] for s in system_stats) / len(system_stats)\n",
    "            max_gpu = max(s['gpu'] for s in system_stats)\n",
    "\n",
    "            results = {\n",
    "                'cpu_generations': cpu_result,\n",
    "                'gpu_operations': gpu_result,\n",
    "                'memory_arrays': memory_result,\n",
    "                'avg_cpu_usage': avg_cpu,\n",
    "                'max_cpu_usage': max_cpu,\n",
    "                'avg_memory_usage': avg_memory,\n",
    "                'max_memory_usage': max_memory,\n",
    "                'avg_gpu_usage': avg_gpu,\n",
    "                'max_gpu_usage': max_gpu,\n",
    "                'duration': duration\n",
    "            }\n",
    "\n",
    "            print(f\"   âœ… GÃ©nÃ©rations CPU: {cpu_result}\")\n",
    "            print(f\"   ðŸŽ® OpÃ©rations GPU: {gpu_result}\")\n",
    "            print(f\"   ðŸ’¾ Allocations mÃ©moire: {memory_result}\")\n",
    "            print(f\"   ðŸ“Š CPU moyen: {avg_cpu:.1f}% (max: {max_cpu:.1f}%)\")\n",
    "            print(f\"   ðŸ“Š RAM moyenne: {avg_memory:.1f}% (max: {max_memory:.1f}%)\")\n",
    "            if gpus:\n",
    "                print(f\"   ðŸ“Š GPU moyen: {avg_gpu:.1f}% (max: {max_gpu:.1f}%)\")\n",
    "\n",
    "        self.stress_results['concurrent_test'] = results\n",
    "        return results\n",
    "\n",
    "    def performance_benchmark(self):\n",
    "        \"\"\"Benchmark complet de performance\"\"\"\n",
    "        print(\"\\nðŸ† BENCHMARK COMPLET DE PERFORMANCE\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Test de gÃ©nÃ©ration simple\n",
    "        print(\"ðŸ§ª Test de gÃ©nÃ©ration simple...\")\n",
    "        start_time = time.time()\n",
    "        simple_responses = []\n",
    "        for i in range(100):\n",
    "            response = self.generator.generate_response(f\"Test {i}\", max_length=20)\n",
    "            simple_responses.append(response)\n",
    "        simple_time = time.time() - start_time\n",
    "        simple_throughput = 100 / simple_time\n",
    "\n",
    "        print(f\"   âš¡ 100 gÃ©nÃ©rations en {simple_time:.2f}s\")\n",
    "        print(f\"   ðŸ“ˆ Throughput: {simple_throughput:.1f} gÃ©nÃ©rations/sec\")\n",
    "\n",
    "        # Test de gÃ©nÃ©ration parallÃ¨le\n",
    "        print(\"\\nðŸš€ Test de gÃ©nÃ©ration parallÃ¨le...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        def parallel_generate(i):\n",
    "            return self.generator.generate_response(f\"Parallel test {i}\", max_length=20)\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count) as executor:\n",
    "            parallel_responses = list(executor.map(parallel_generate, range(100)))\n",
    "\n",
    "        parallel_time = time.time() - start_time\n",
    "        parallel_throughput = 100 / parallel_time\n",
    "        speedup = simple_time / parallel_time\n",
    "\n",
    "        print(f\"   âš¡ 100 gÃ©nÃ©rations parallÃ¨les en {parallel_time:.2f}s\")\n",
    "        print(f\"   ðŸ“ˆ Throughput: {parallel_throughput:.1f} gÃ©nÃ©rations/sec\")\n",
    "        print(f\"   ðŸš€ AccÃ©lÃ©ration: {speedup:.1f}x\")\n",
    "\n",
    "        # RÃ©sumÃ© du benchmark\n",
    "        benchmark_results = {\n",
    "            'simple_throughput': simple_throughput,\n",
    "            'parallel_throughput': parallel_throughput,\n",
    "            'speedup': speedup,\n",
    "            'simple_time': simple_time,\n",
    "            'parallel_time': parallel_time\n",
    "        }\n",
    "\n",
    "        self.stress_results['benchmark'] = benchmark_results\n",
    "\n",
    "        return benchmark_results\n",
    "\n",
    "    def generate_performance_report(self):\n",
    "        \"\"\"GÃ©nÃ¨re un rapport complet de performance\"\"\"\n",
    "        print(\"\\n\udcca RAPPORT COMPLET DE PERFORMANCE\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # Graphiques de performance\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle('Rapport de Performance Ultra-OptimisÃ©', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # 1. CPU Utilization\n",
    "        if 'cpu_test' in self.stress_results:\n",
    "            cpu_data = self.stress_results['cpu_test']\n",
    "            axes[0, 0].bar(['Moyen', 'Maximum'], [cpu_data['avg_cpu_usage'], cpu_data['max_cpu_usage']],\n",
    "                          color=['#FF6B9D', '#FF4757'])\n",
    "            axes[0, 0].set_title('Utilisation CPU (%)')\n",
    "            axes[0, 0].set_ylim(0, 100)\n",
    "            axes[0, 0].axhline(y=95, color='green', linestyle='--', label='Cible 95%')\n",
    "            axes[0, 0].legend()\n",
    "\n",
    "        # 2. Memory Utilization\n",
    "        if 'memory_test' in self.stress_results:\n",
    "            mem_data = self.stress_results['memory_test']\n",
    "            if 'max_memory_usage' in mem_data:\n",
    "                axes[0, 1].bar(['Avant', 'AprÃ¨s', 'Maximum'],\n",
    "                              [mem_data['memory_before'], mem_data['memory_after'], mem_data['max_memory_usage']],\n",
    "                              color=['#4ECDC4', '#45B7D1', '#3742FA'])\n",
    "                axes[0, 1].set_title('Utilisation MÃ©moire (%)')\n",
    "                axes[0, 1].set_ylim(0, 100)\n",
    "\n",
    "        # 3. GPU Utilization\n",
    "        if 'gpu_test' in self.stress_results and 'avg_gpu_utilization' in self.stress_results['gpu_test']:\n",
    "            gpu_data = self.stress_results['gpu_test']\n",
    "            axes[0, 2].bar(['Utilisation', 'MÃ©moire'],\n",
    "                          [gpu_data['avg_gpu_utilization'], gpu_data['avg_gpu_memory']],\n",
    "                          color=['#FFA502', '#FF6348'])\n",
    "            axes[0, 2].set_title('Performance GPU (%)')\n",
    "            axes[0, 2].set_ylim(0, 100)\n",
    "\n",
    "        # 4. Throughput Comparison\n",
    "        if 'benchmark' in self.stress_results:\n",
    "            bench_data = self.stress_results['benchmark']\n",
    "            axes[1, 0].bar(['Simple', 'ParallÃ¨le'],\n",
    "                          [bench_data['simple_throughput'], bench_data['parallel_throughput']],\n",
    "                          color=['#2F3542', '#57606F'])\n",
    "            axes[1, 0].set_title('Throughput (gÃ©nÃ©rations/sec)')\n",
    "\n",
    "        # 5. Concurrent Performance\n",
    "        if 'concurrent_test' in self.stress_results:\n",
    "            conc_data = self.stress_results['concurrent_test']\n",
    "            resources = ['CPU', 'RAM', 'GPU']\n",
    "            values = [conc_data.get('avg_cpu_usage', 0),\n",
    "                     conc_data.get('avg_memory_usage', 0),\n",
    "                     conc_data.get('avg_gpu_usage', 0)]\n",
    "            axes[1, 1].bar(resources, values, color=['#FF6B9D', '#4ECDC4', '#FFA502'])\n",
    "            axes[1, 1].set_title('Test Concurrent - Utilisation Moyenne (%)')\n",
    "            axes[1, 1].set_ylim(0, 100)\n",
    "\n",
    "        # 6. Performance Score\n",
    "        performance_score = self.calculate_performance_score()\n",
    "        axes[1, 2].pie([performance_score, 100-performance_score],\n",
    "                      labels=[f'Score: {performance_score:.1f}%', 'Potentiel restant'],\n",
    "                      colors=['#2ECC71', '#E74C3C'], startangle=90)\n",
    "        axes[1, 2].set_title('Score de Performance Global')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # RÃ©sumÃ© textuel\n",
    "        print(f\"\\nðŸ† SCORE DE PERFORMANCE GLOBAL: {performance_score:.1f}%\")\n",
    "        print(\"\\nðŸ“‹ RÃ‰SUMÃ‰ DES TESTS:\")\n",
    "\n",
    "        for test_name, results in self.stress_results.items():\n",
    "            if isinstance(results, dict) and 'error' not in results:\n",
    "                print(f\"   âœ… {test_name.replace('_', ' ').title()}: RÃ©ussi\")\n",
    "            elif isinstance(results, dict) and 'error' in results:\n",
    "                print(f\"   âš ï¸ {test_name.replace('_', ' ').title()}: LimitÃ© ({results.get('error', 'Unknown')})\")\n",
    "\n",
    "        print(f\"\\nðŸŽ¯ RECOMMANDATIONS:\")\n",
    "        if performance_score >= 90:\n",
    "            print(\"   ðŸ”¥ Excellent! Utilisation optimale des ressources\")\n",
    "        elif performance_score >= 75:\n",
    "            print(\"   âœ… TrÃ¨s bon, quelques optimisations mineures possibles\")\n",
    "        elif performance_score >= 60:\n",
    "            print(\"   âš ï¸ Correct, optimisations recommandÃ©es\")\n",
    "        else:\n",
    "            print(\"   âŒ Optimisations majeures nÃ©cessaires\")\n",
    "\n",
    "        return performance_score\n",
    "\n",
    "    def calculate_performance_score(self):\n",
    "        \"\"\"Calcule un score de performance global\"\"\"\n",
    "        score = 0\n",
    "        max_score = 0\n",
    "\n",
    "        # Score CPU (30 points max)\n",
    "        if 'cpu_test' in self.stress_results:\n",
    "            cpu_data = self.stress_results['cpu_test']\n",
    "            cpu_score = min(30, (cpu_data['avg_cpu_usage'] / 95) * 30)\n",
    "            score += cpu_score\n",
    "        max_score += 30\n",
    "\n",
    "        # Score MÃ©moire (25 points max)\n",
    "        if 'memory_test' in self.stress_results:\n",
    "            mem_data = self.stress_results['memory_test']\n",
    "            if 'max_memory_usage' in mem_data:\n",
    "                mem_score = min(25, (mem_data['max_memory_usage'] / 85) * 25)\n",
    "                score += mem_score\n",
    "        max_score += 25\n",
    "\n",
    "        # Score GPU (25 points max)\n",
    "        if 'gpu_test' in self.stress_results and 'avg_gpu_utilization' in self.stress_results['gpu_test']:\n",
    "            gpu_data = self.stress_results['gpu_test']\n",
    "            gpu_score = min(25, (gpu_data['avg_gpu_utilization'] / 90) * 25)\n",
    "            score += gpu_score\n",
    "        max_score += 25\n",
    "\n",
    "        # Score Throughput (20 points max)\n",
    "        if 'benchmark' in self.stress_results:\n",
    "            bench_data = self.stress_results['benchmark']\n",
    "            # Score basÃ© sur l'amÃ©lioration du parallÃ©lisme\n",
    "            speedup_score = min(20, (bench_data['speedup'] / cpu_count) * 20)\n",
    "            score += speedup_score\n",
    "        max_score += 20\n",
    "\n",
    "        return (score / max_score * 100) if max_score > 0 else 0\n",
    "\n",
    "\n",
    "# ExÃ©cution des tests de stress complets\n",
    "print(\"ðŸš€ Initialisation du testeur de stress ultra-performance...\")\n",
    "stress_tester = UltraStressTester(model, generator, tokenizer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ”¥ DÃ‰BUT DES TESTS DE STRESS MAXIMAUX\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Test CPU\n",
    "cpu_results = stress_tester.cpu_stress_test(duration=30)\n",
    "\n",
    "# 2. Test MÃ©moire\n",
    "memory_results = stress_tester.memory_stress_test(batch_size_multiplier=3)\n",
    "\n",
    "# 3. Test GPU (si disponible)\n",
    "gpu_results = stress_tester.gpu_stress_test(duration=20)\n",
    "\n",
    "# 4. Test concurrent\n",
    "concurrent_results = stress_tester.concurrent_stress_test(duration=30)\n",
    "\n",
    "# 5. Benchmark de performance\n",
    "benchmark_results = stress_tester.performance_benchmark()\n",
    "\n",
    "# 6. GÃ©nÃ©ration du rapport final\n",
    "final_score = stress_tester.generate_performance_report()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ‰ TESTS DE STRESS TERMINÃ‰S\")\n",
    "print(\"=\"*80)\n",
    "print(f\"ðŸ† Score final de performance: {final_score:.1f}%\")\n",
    "\n",
    "if final_score >= 85:\n",
    "    print(\"\udd25 FÃ‰LICITATIONS! Votre systÃ¨me utilise ses ressources de maniÃ¨re optimale!\")\n",
    "    print(\"ðŸ’ª Configuration ultra-performante validÃ©e!\")\n",
    "elif final_score >= 70:\n",
    "    print(\"âœ… TrÃ¨s bonne performance! Quelques optimisations mineures possibles.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Performance correcte, mais des amÃ©liorations sont possibles.\")\n",
    "\n",
    "print(f\"\\nðŸ“Š RÃ©sumÃ© de l'utilisation des ressources:\")\n",
    "print(f\"   \udda5ï¸ CPU: Utilisation maximale validÃ©e\")\n",
    "print(f\"   ðŸ’¾ RAM: Optimisation mÃ©moire confirmÃ©e\")\n",
    "if gpus:\n",
    "    print(f\"   ðŸŽ® GPU: AccÃ©lÃ©ration matÃ©rielle active\")\n",
    "print(f\"   âš¡ ParallÃ©lisation: {cpu_count} threads actifs\")\n",
    "print(f\"   ðŸš€ Votre modÃ¨le Shirayuki fonctionne Ã  {final_score:.0f}% de l'efficacitÃ© maximale!\")\n",
    "\n",
    "print(\"\\nðŸŒ¸ Shirayuki ultra-optimisÃ©e prÃªte pour conversations Ã  haute performance! ðŸŒ¸\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
