{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7a34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip wheel setuptools\n",
    "!pip install -U tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb88c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, multiprocessing as mp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# RÃ©duit le spam de logs\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "\n",
    "def setup_colab_a100():\n",
    "    \"\"\"Configure TF pour GPU A100 dans Google Colab.\"\"\"\n",
    "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "    if gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "        except Exception:\n",
    "            from keras import mixed_precision\n",
    "            mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "        tf.config.optimizer.set_jit(True)  # active XLA\n",
    "        tf.config.set_soft_device_placement(True)\n",
    "        batch_size = 256\n",
    "        print(\"âœ… GPU A100 dÃ©tectÃ© :\", gpus)\n",
    "        print(\"ðŸŸ£ Mixed precision activÃ©e (float16).\")\n",
    "        print(f\"ðŸ§  Batch size conseillÃ© : {batch_size}\")\n",
    "        return dict(use_xla=True, on_gpu=True, batch=batch_size)\n",
    "    else:\n",
    "        cores = mp.cpu_count()\n",
    "        batch_size = 32\n",
    "        tf.keras.mixed_precision.set_global_policy(\"float32\")\n",
    "        tf.config.threading.set_intra_op_parallelism_threads(cores)\n",
    "        tf.config.threading.set_inter_op_parallelism_threads(min(4, max(2, cores // 4)))\n",
    "        os.environ[\"OMP_NUM_THREADS\"] = str(cores)\n",
    "        print(\"ðŸŸ¡ Pas de GPU. Optimisation CPU (threads).\")\n",
    "        print(f\"ðŸ§  Batch size conseillÃ© : {batch_size}\")\n",
    "        return dict(use_xla=True, on_gpu=False, batch=batch_size)\n",
    "\n",
    "CONF = setup_colab_a100()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d23dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q faiss-gpu datasets pandas sentence-transformers sacrebleu tf-keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports lÃ©gers & ordonnÃ©s ===\n",
    "import math\n",
    "import random\n",
    "import pathlib\n",
    "import datetime as dt\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import callbacks as Kcb\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# (optionnel) petites infos de run\n",
    "print(\"TF\", tf.__version__)\n",
    "print(\"Devices:\", tf.config.list_physical_devices())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# DonnÃ©es\n",
    "# =========================\n",
    "def load_squad_pairs():\n",
    "    ds = load_dataset(\"squad\", split=\"train\")\n",
    "    pairs = []\n",
    "    for it in ds:\n",
    "        ctx = (it[\"context\"] or \"\").strip()\n",
    "        q = (it[\"question\"] or \"\").strip()\n",
    "        ans = it[\"answers\"][\"text\"][0].strip() if it[\"answers\"][\"text\"] else \"\"\n",
    "        if ctx and q and ans:\n",
    "            pairs.append((f\"{ctx}\\nQ: {q}\", ans))\n",
    "    print(f\"âœ… SQuAD: {len(pairs)} paires\")\n",
    "    return pairs\n",
    "\n",
    "def load_shirayuki_pairs(csv_path=\"shirayuki.csv\"):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    pairs = [(str(i).strip(), str(o).strip())\n",
    "             for i,o in zip(df[\"guy\"], df[\"girl\"])\n",
    "             if str(i).strip() and str(o).strip()]\n",
    "    print(f\"âœ… Shirayuki: {len(pairs)} paires\")\n",
    "    return pairs\n",
    "\n",
    "def split_pairs(pairs, val_ratio=0.02, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(len(pairs))\n",
    "    rng.shuffle(idx)\n",
    "    cut = max(1, int(len(pairs) * (1 - val_ratio)))\n",
    "    train_idx, val_idx = idx[:cut], idx[cut:]\n",
    "    train = [pairs[i] for i in train_idx]\n",
    "    val = [pairs[i] for i in val_idx]\n",
    "    return train, val\n",
    "\n",
    "def make_ds_from_pairs(pairs, tokenizer, max_len=96, batch_size=64, shuffle=True):\n",
    "    X = [x for x,_ in pairs]\n",
    "    Y = [f\"[START] {y} [END]\" for _,y in pairs]\n",
    "    enc = tokenizer(X)\n",
    "    out = tokenizer(Y)\n",
    "    dec_in = out[:, :-1]\n",
    "    dec_tg = out[:, 1:]\n",
    "    ds = tf.data.Dataset.from_tensor_slices(\n",
    "        ({\"encoder_input\": enc, \"decoder_input\": dec_in}, dec_tg)\n",
    "    )\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10000)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    steps = math.ceil(len(pairs) / batch_size)\n",
    "    return ds, steps\n",
    "\n",
    "def prepare_datasets(pairs, tokenizer=None, vocab_size=20000, max_len=96, batch_size=64, val_ratio=0.02):\n",
    "    train_pairs, val_pairs = split_pairs(pairs, val_ratio=val_ratio)\n",
    "    X_all = [x for x,_ in pairs]\n",
    "    Y_all = [f\"[START] {y} [END]\" for _,y in pairs]\n",
    "    if tokenizer is None:\n",
    "        tokenizer = TextVectorization(\n",
    "            max_tokens=vocab_size,\n",
    "            output_sequence_length=max_len,\n",
    "            standardize=\"lower_and_strip_punctuation\",\n",
    "            split=\"whitespace\"\n",
    "        )\n",
    "        tokenizer.adapt(X_all + Y_all)\n",
    "    train_ds, train_steps = make_ds_from_pairs(train_pairs, tokenizer, max_len, batch_size, shuffle=True)\n",
    "    val_ds, val_steps     = make_ds_from_pairs(val_pairs, tokenizer, max_len, batch_size, shuffle=False)\n",
    "    return tokenizer, train_ds, val_ds, train_steps, val_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33803d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# MÃ©moire FAISS (RAG light)\n",
    "# =========================\n",
    "import os\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "EMBED_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "EMBED_DIM = 384\n",
    "MEMORY_FILE = \"shirayuki_memory.jsonl\"\n",
    "INDEX_FILE = \"shirayuki_faiss.index\"\n",
    "index = faiss.read_index(INDEX_FILE) if os.path.exists(INDEX_FILE) else faiss.IndexFlatL2(EMBED_DIM)\n",
    "\n",
    "def _encode(text): return np.array([EMBED_MODEL.encode(text)], dtype=\"float32\")\n",
    "\n",
    "def save_to_memory(user_text, bot_text):\n",
    "    ts = datetime.datetime.now().isoformat()\n",
    "    index.add(_encode(user_text))\n",
    "    with open(MEMORY_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps({\"input\": user_text, \"response\": bot_text, \"timestamp\": ts}, ensure_ascii=False) + \"\\n\")\n",
    "    faiss.write_index(index, INDEX_FILE)\n",
    "\n",
    "def search_memory(query, top_k=3):\n",
    "    if index.ntotal == 0 or not os.path.exists(MEMORY_FILE): return []\n",
    "    D, I = index.search(_encode(query), top_k)\n",
    "    with open(MEMORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        mem = [json.loads(l) for l in f]\n",
    "    return [mem[i] for i in I[0] if 0 <= i < len(mem)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dac2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Masques (compatibles Keras MHA)\n",
    "# =========================\n",
    "PAD = 0\n",
    "def padding_mask_2d(token_ids):\n",
    "    return tf.cast(tf.not_equal(token_ids, PAD), tf.float32)   # (B,T)\n",
    "def self_attention_mask(tokens):\n",
    "    m = padding_mask_2d(tokens)                                # (B,T)\n",
    "    return tf.einsum(\"bi,bj->bij\", m, m)                       # (B,T,T)\n",
    "def look_ahead_matrix(T):\n",
    "    return tf.linalg.band_part(tf.ones((T, T), dtype=tf.float32), -1, 0)  # (T,T)\n",
    "def decoder_self_mask(dec_tokens):\n",
    "    m = padding_mask_2d(dec_tokens)                            # (B,Td)\n",
    "    pad_pair = tf.einsum(\"bi,bj->bij\", m, m)                   # (B,Td,Td)\n",
    "    la = look_ahead_matrix(tf.shape(dec_tokens)[1])            # (Td,Td)\n",
    "    return pad_pair * la                                       # (B,Td,Td)\n",
    "def cross_attention_mask(dec_tokens, enc_tokens):\n",
    "    m_dec = padding_mask_2d(dec_tokens)                        # (B,Td)\n",
    "    m_enc = padding_mask_2d(enc_tokens)                        # (B,Te)\n",
    "    return tf.einsum(\"bi,bj->bij\", m_dec, m_enc)               # (B,Td,Te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c94f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ModÃ¨le Transformer\n",
    "# =========================\n",
    "class Block(tf.keras.layers.Layer):\n",
    "    def __init__(self, d, h, ff, drop=0.1, decoder=False):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.self_att = MultiHeadAttention(num_heads=h, key_dim=d//h, dropout=drop)\n",
    "        self.ln1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.do1 = Dropout(drop)\n",
    "        if decoder:\n",
    "            self.cross = MultiHeadAttention(num_heads=h, key_dim=d//h, dropout=drop)\n",
    "            self.ln_c = LayerNormalization(epsilon=1e-6)\n",
    "            self.do_c = Dropout(drop)\n",
    "        self.ffn = tf.keras.Sequential([Dense(ff, activation=\"gelu\"), Dense(d)])\n",
    "        self.ln2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.do2 = Dropout(drop)\n",
    "    def call(self, x, enc_out=None, self_mask=None, enc_mask=None, training=False):\n",
    "        a = self.self_att(x, x, x, attention_mask=self_mask, training=training)\n",
    "        x = self.ln1(x + self.do1(a, training=training))\n",
    "        if self.decoder and enc_out is not None:\n",
    "            a2 = self.cross(x, enc_out, enc_out, attention_mask=enc_mask, training=training)\n",
    "            x = self.ln_c(x + self.do_c(a2, training=training))\n",
    "        f = self.ffn(x)\n",
    "        return self.ln2(x + self.do2(f, training=training))\n",
    "\n",
    "class Seq2Seq(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, d=256, h=8, ff=768, max_len=96, L=4, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.d, self.max_len = d, max_len\n",
    "        self.tok_emb = Embedding(vocab_size, d)\n",
    "        self.pos_emb = Embedding(max_len, d)\n",
    "        self.enc = [Block(d, h, ff, drop, decoder=False) for _ in range(L)]\n",
    "        self.dec = [Block(d, h, ff, drop, decoder=True) for _ in range(L)]\n",
    "        self.final = Dense(vocab_size)\n",
    "    def _add_pos(self, tok_ids):\n",
    "        T = tf.shape(tok_ids)[1]\n",
    "        return self.tok_emb(tok_ids) + self.pos_emb(tf.range(T)[tf.newaxis, :])\n",
    "    def encode(self, enc_tokens, training=False):\n",
    "        x = self._add_pos(enc_tokens)\n",
    "        mask = self_attention_mask(enc_tokens)                 # (B,Te,Te)\n",
    "        for blk in self.enc:\n",
    "            x = blk(x, self_mask=mask, training=training)\n",
    "        return x\n",
    "    def decode(self, dec_tokens, enc_tokens, enc_out, training=False):\n",
    "        y = self._add_pos(dec_tokens)\n",
    "        self_m = decoder_self_mask(dec_tokens)                 # (B,Td,Td)\n",
    "        cross_m = cross_attention_mask(dec_tokens, enc_tokens) # (B,Td,Te)\n",
    "        for blk in self.dec:\n",
    "            y = blk(y, enc_out=enc_out, self_mask=self_m, enc_mask=cross_m, training=training)\n",
    "        return y\n",
    "    def call(self, inputs, training=False):\n",
    "        enc_tokens = inputs[\"encoder_input\"]\n",
    "        dec_tokens = inputs[\"decoder_input\"]\n",
    "        enc_out = self.encode(enc_tokens, training=training)\n",
    "        dec_out = self.decode(dec_tokens, enc_tokens, enc_out, training=training)\n",
    "        return self.final(dec_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc245fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# GÃ©nÃ©ration\n",
    "# =========================\n",
    "def build_generation(tokenizer, model):\n",
    "    vocab = tokenizer.get_vocabulary()\n",
    "    tok2id = {t:i for i,t in enumerate(vocab)}\n",
    "    START = tok2id.get(\"[START]\", 1)\n",
    "    END = tok2id.get(\"[END]\", 2)\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def _tf_encode(enc_tokens):\n",
    "        return model.encode(enc_tokens, training=False)\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def _tf_decode(dec_tokens, enc_tokens, enc_out):\n",
    "        y = model.decode(dec_tokens, enc_tokens, enc_out, training=False)\n",
    "        return model.final(y)[:, -1, :]\n",
    "\n",
    "    def generate_response(prompt, max_new_tokens=64, temperature=0.7, top_k=None, use_memory=True, save_mem=True):\n",
    "        ctx = \"\"\n",
    "        if use_memory:\n",
    "            hits = search_memory(prompt, top_k=3)\n",
    "            if hits:\n",
    "                ctx = \"\\n\".join([f\"User: {m['input']}\\nShirayuki: {m['response']}\" for m in hits]) + \"\\n\"\n",
    "        full_inp = ctx + f\"User: {prompt}\\nShirayuki:\"\n",
    "\n",
    "        enc_tokens = tokenizer([full_inp])\n",
    "        enc_out = _tf_encode(enc_tokens)\n",
    "\n",
    "        y = tf.constant([[START]], dtype=tf.int64)\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = _tf_decode(y, enc_tokens, enc_out)\n",
    "            if temperature and temperature > 0:\n",
    "                logits = logits / temperature\n",
    "                if top_k and top_k > 0:\n",
    "                    values, indices = tf.math.top_k(logits, k=top_k)\n",
    "                    probs = tf.nn.softmax(values)\n",
    "                    next_id_rel = tf.random.categorical(tf.math.log(probs), 1)\n",
    "                    next_id = tf.gather(indices, next_id_rel, batch_dims=1)\n",
    "                    next_token = int(next_id.numpy()[0][0])\n",
    "                else:\n",
    "                    next_token = int(tf.random.categorical(logits, 1).numpy()[0][0])\n",
    "            else:\n",
    "                next_token = int(tf.argmax(logits, axis=-1).numpy()[0])\n",
    "            if next_token == END: break\n",
    "            y = tf.concat([y, tf.constant([[next_token]], dtype=tf.int64)], axis=1)\n",
    "\n",
    "        id2tok = {i:t for i,t in enumerate(vocab)}\n",
    "        toks = [id2tok.get(int(t), \"\") for t in y.numpy()[0] if int(t) not in (0, START, END)]\n",
    "        text = \" \".join(toks).strip()\n",
    "        if save_mem:\n",
    "            save_to_memory(prompt, text)\n",
    "        return text or \"[Aucune rÃ©ponse gÃ©nÃ©rÃ©e]\"\n",
    "\n",
    "    return generate_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6a6f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Callbacks\n",
    "# =========================\n",
    "def build_callbacks(run_name=\"run\"):\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = pathlib.Path(\"logs\") / f\"{run_name}-{ts}\"\n",
    "    ckpt_dir = pathlib.Path(\"ckpts\") / f\"{run_name}-{ts}\"\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # LR schedule: warmup -> cosine\n",
    "    warmup_epochs = 1\n",
    "    max_epochs = 50\n",
    "    base_lr = 1e-3\n",
    "    min_lr = 1e-5\n",
    "    def lr_schedule(epoch, lr):\n",
    "        if epoch < warmup_epochs:\n",
    "            return base_lr * (epoch + 1) / warmup_epochs\n",
    "        # cosine decay from base_lr to min_lr\n",
    "        t = (epoch - warmup_epochs) / max(1, (max_epochs - warmup_epochs))\n",
    "        return float(min_lr + 0.5*(base_lr - min_lr)*(1 + math.cos(math.pi * t)))\n",
    "\n",
    "    def make_gen_cb(gen_fn):\n",
    "        sample_prompts = [\"Hello Shirayuki\", \"How are you today?\"]\n",
    "        def _on_epoch_end(epoch, logs=None):\n",
    "            print(\"\\nðŸ§ª Samples:\")\n",
    "            for p in sample_prompts:\n",
    "                print(\" >\", p)\n",
    "                print(\" >\", gen_fn(p, temperature=0.8, top_k=40))\n",
    "        return Kcb.LambdaCallback(on_epoch_end=_on_epoch_end)\n",
    "\n",
    "    cbs = [\n",
    "        Kcb.TensorBoard(log_dir=str(log_dir), histogram_freq=0, write_graph=True),\n",
    "        Kcb.BackupAndRestore(backup_dir=str(log_dir / \"backup\")),\n",
    "        Kcb.ModelCheckpoint(\n",
    "            filepath=str(ckpt_dir / \"{epoch:02d}-{val_loss:.3f}.weights.h5\"),\n",
    "            save_weights_only=True, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1\n",
    "        ),\n",
    "        Kcb.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1),\n",
    "        Kcb.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5, verbose=1),\n",
    "        Kcb.LearningRateScheduler(lr_schedule, verbose=0),\n",
    "        Kcb.CSVLogger(str(log_dir / \"training.csv\"), append=False),\n",
    "        Kcb.TerminateOnNaN(),\n",
    "    ]\n",
    "    return cbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e217ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Imports utiles =====\n",
    "import math, datetime, pathlib, random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "Kcb = tf.keras.callbacks\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1) Warmup + Cosine schedule (sur les *steps*)\n",
    "# -------------------------------------------------\n",
    "class WarmupCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, base_lr, min_lr, warmup_steps, total_steps):\n",
    "        self.base_lr = float(base_lr)\n",
    "        self.min_lr = float(min_lr)\n",
    "        self.warmup_steps = int(warmup_steps)\n",
    "        self.total_steps = int(total_steps)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        warm = tf.cond(step < self.warmup_steps,\n",
    "            lambda: self.base_lr * (step + 1.0) / tf.maximum(1.0, float(self.warmup_steps)),\n",
    "            lambda: self.min_lr + 0.5*(self.base_lr - self.min_lr) *\n",
    "                    (1.0 + tf.cos(math.pi * tf.minimum(1.0,\n",
    "                       (step - self.warmup_steps) / tf.maximum(1.0, float(self.total_steps - self.warmup_steps))))))\n",
    "        return warm\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) BLEU-4 et ROUGE-L (implÃ©mentation light)\n",
    "# -------------------------------------------------\n",
    "def _tokenize(s): return s.strip().split()\n",
    "\n",
    "def _ngrams(toks, n):\n",
    "    return [tuple(toks[i:i+n]) for i in range(len(toks)-n+1)]\n",
    "\n",
    "def bleu4(ref, hyp, smooth=1.0):\n",
    "    # ref/hyp: strings\n",
    "    r = _tokenize(ref); h = _tokenize(hyp)\n",
    "    if len(h) == 0: return 0.0\n",
    "    precisions = []\n",
    "    for n in range(1, 5):\n",
    "        R = Counter(_ngrams(r, n)); H = Counter(_ngrams(h, n))\n",
    "        overlap = sum((R & H).values())\n",
    "        total = max(sum(H.values()), 1)\n",
    "        precisions.append((overlap + smooth) / (total + smooth))\n",
    "    bp = math.exp(1 - len(r)/max(len(h), 1)) if len(h) < len(r) else 1.0\n",
    "    return float(bp * math.exp(sum(map(math.log, precisions)) / 4.0))\n",
    "\n",
    "def _lcs_len(a, b):\n",
    "    # a, b: list of tokens\n",
    "    m, n = len(a), len(b)\n",
    "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    for i in range(m):\n",
    "        ai = a[i]\n",
    "        row = dp[i]\n",
    "        row1 = dp[i+1]\n",
    "        for j in range(n):\n",
    "            row1[j+1] = row[j] + 1 if ai == b[j] else max(row1[j], row[j+1])\n",
    "    return dp[m][n]\n",
    "\n",
    "def rouge_l_f1(ref, hyp, beta=1.2):\n",
    "    r = _tokenize(ref); h = _tokenize(hyp)\n",
    "    if len(r) == 0 or len(h) == 0: return 0.0\n",
    "    L = _lcs_len(r, h)\n",
    "    p = L / len(h); rc = L / len(r)\n",
    "    if p + rc == 0: return 0.0\n",
    "    b2 = beta * beta\n",
    "    return float((1 + b2) * p * rc / (rc + b2 * p))\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) Callback dâ€™Ã©val NLG (BLEU/ROUGE/PPL + TB)\n",
    "# -------------------------------------------------\n",
    "class EvalNLG(Kcb.Callback):\n",
    "    def __init__(self, gen_fn, eval_pairs, log_dir, every=1, name_prefix=\"val\"):\n",
    "        super().__init__()\n",
    "        self.gen = gen_fn\n",
    "        self.pairs = eval_pairs\n",
    "        self.every = int(every)\n",
    "        self.name = name_prefix\n",
    "        self.tb = tf.summary.create_file_writer(str(log_dir))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.every != 0: return\n",
    "        logs = logs or {}\n",
    "        preds, refs = [], []\n",
    "        for q, a in self.pairs:\n",
    "            # greedy pour des mÃ©triques stables\n",
    "            hyp = self.gen(q, temperature=0.0)  # top_k=None par dÃ©faut\n",
    "            preds.append(hyp); refs.append(a)\n",
    "        bleu = float(np.mean([bleu4(r, h) for r, h in zip(refs, preds)]))\n",
    "        rouge = float(np.mean([rouge_l_f1(r, h) for r, h in zip(refs, preds)]))\n",
    "        ppl = float(np.exp(logs['val_loss'])) if 'val_loss' in logs else float('nan')\n",
    "\n",
    "        # injecter dans logs -> utilisable par EarlyStopping/Checkpoint\n",
    "        logs[f'{self.name}_bleu'] = bleu\n",
    "        logs[f'{self.name}_rougeL'] = rouge\n",
    "        logs[f'{self.name}_perplexity'] = ppl\n",
    "\n",
    "        # log TensorBoard\n",
    "        lr = self._current_lr()\n",
    "        with self.tb.as_default():\n",
    "            tf.summary.scalar(f'{self.name}_bleu', bleu, step=epoch)\n",
    "            tf.summary.scalar(f'{self.name}_rougeL', rouge, step=epoch)\n",
    "            tf.summary.scalar(f'{self.name}_perplexity', ppl, step=epoch)\n",
    "            if lr is not None:\n",
    "                tf.summary.scalar('lr', lr, step=epoch)\n",
    "\n",
    "        # console\n",
    "        print(f\"\\nðŸ“Š {self.name.upper()} â€” BLEU: {bleu:.3f} | ROUGE-L: {rouge:.3f} | PPL: {ppl:.1f} | LR: {lr:.2e if lr else np.nan}\")\n",
    "\n",
    "        # petit aperÃ§u\n",
    "        for p in [\"Hello Shirayuki\", \"How are you today?\"]:\n",
    "            print(\" >\", p)\n",
    "            print(\" >\", self.gen(p, temperature=0.8, top_k=40))\n",
    "\n",
    "    def _current_lr(self):\n",
    "        lr = getattr(self.model.optimizer, 'learning_rate', None)\n",
    "        if lr is None: return None\n",
    "        try:\n",
    "            # schedule -> callable\n",
    "            return float(lr(self.model.optimizer.iterations))\n",
    "        except TypeError:\n",
    "            # constant -> variable\n",
    "            return float(tf.keras.backend.get_value(lr))\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4) Callbacks pack (cohÃ©rent et centrÃ© ROUGE-L)\n",
    "# -------------------------------------------------\n",
    "def build_callbacks_optim(run_name, gen_fn, eval_pairs, max_epochs, steps_per_epoch):\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = pathlib.Path(\"logs\") / f\"{run_name}-{ts}\"\n",
    "    ckpt_dir = pathlib.Path(\"ckpts\") / f\"{run_name}-{ts}\"\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    eval_cb = EvalNLG(gen_fn, eval_pairs, log_dir, every=1, name_prefix=\"val\")\n",
    "\n",
    "    cbs = [\n",
    "        # IMPORTANT: EvalNLG AVANT Checkpoint/EarlyStopping (il insÃ¨re 'val_rougeL' dans logs)\n",
    "        eval_cb,\n",
    "        Kcb.TensorBoard(log_dir=str(log_dir), histogram_freq=0, write_graph=True, profile_batch=(10, 20)),\n",
    "        Kcb.BackupAndRestore(backup_dir=str(log_dir / \"backup\")),\n",
    "        # meilleur modÃ¨le par ROUGE-L\n",
    "        Kcb.ModelCheckpoint(\n",
    "            filepath=str(ckpt_dir / \"best-rouge-{epoch:02d}-{val_rougeL:.3f}.weights.h5\"),\n",
    "            save_weights_only=True, monitor=\"val_rougeL\", mode=\"max\", save_best_only=True, verbose=1\n",
    "        ),\n",
    "        # on garde aussi le meilleur par val_loss (utile pour perplexity)\n",
    "        Kcb.ModelCheckpoint(\n",
    "            filepath=str(ckpt_dir / \"best-loss-{epoch:02d}-{val_loss:.3f}.weights.h5\"),\n",
    "            save_weights_only=True, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1\n",
    "        ),\n",
    "        # early stop sur la vraie fitness\n",
    "        Kcb.EarlyStopping(monitor=\"val_rougeL\", patience=4, mode=\"max\", restore_best_weights=True, verbose=1),\n",
    "        Kcb.CSVLogger(str(log_dir / \"training.csv\"), append=False),\n",
    "        Kcb.TerminateOnNaN(),\n",
    "    ]\n",
    "    return cbs, log_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690f24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# DonnÃ©es + modÃ¨le (Colab A100 ready)\n",
    "# =========================\n",
    "import os, random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from tensorflow.keras.layers import Embedding\n",
    "# MultiHeadAttention\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Dense\n",
    "import json\n",
    "\n",
    "# (Optionnel) un peu plus de reproductibilitÃ©\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Charge les paires\n",
    "squad_pairs = load_squad_pairs()\n",
    "\n",
    "# Batch dynamique selon GPU/CPU\n",
    "tokenizer, squad_train, squad_val, squad_steps, squad_val_steps = prepare_datasets(\n",
    "    squad_pairs, vocab_size=20000, max_len=96, batch_size=CONF[\"batch\"], val_ratio=0.02\n",
    ")\n",
    "\n",
    "# (RecommandÃ©) Ã©carter le padding (id=0) de la loss/accuracy via sample_weight\n",
    "PAD_ID = 0\n",
    "def add_mask(x, y):\n",
    "    # sw: (B, T) float32 ; 1 pour tokens non-pad, 0 pour pad\n",
    "    sw = tf.cast(tf.not_equal(y, PAD_ID), tf.float32)\n",
    "    return x, y, sw\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "# + parallel map et cache (utile si les paires tiennent en RAM)\n",
    "squad_train = (squad_train\n",
    "               .map(add_mask, num_parallel_calls=AUTOTUNE)\n",
    "               .prefetch(AUTOTUNE))\n",
    "squad_val   = (squad_val\n",
    "               .map(add_mask, num_parallel_calls=AUTOTUNE)\n",
    "               .prefetch(AUTOTUNE))\n",
    "\n",
    "# Scheduler de LR sur les steps (on scale linÃ©airement si batch changÃ©)\n",
    "max_epochs   = 50\n",
    "scale        = float(CONF[\"batch\"]) / 64.0\n",
    "base_lr      = 3e-4 * scale\n",
    "min_lr       = 1e-5\n",
    "total_steps  = int(squad_steps * max_epochs)\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "lr_sched = WarmupCosine(base_lr=base_lr, min_lr=min_lr,\n",
    "                        warmup_steps=warmup_steps, total_steps=total_steps)\n",
    "\n",
    "# Optimizer = AdamW + clipnorm (compat Keras/TF)\n",
    "# Essaye TF-Keras 2.x, puis Keras 3, puis Adam fallback\n",
    "opt = None\n",
    "# TF >= 2.11\n",
    "try:\n",
    "    opt = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=lr_sched, weight_decay=1e-4, clipnorm=1.0\n",
    "    )\n",
    "except Exception:\n",
    "    pass\n",
    "# Keras 3 (backend-agnostic)\n",
    "if opt is None:\n",
    "    try:\n",
    "        import keras\n",
    "        opt = keras.optimizers.AdamW(\n",
    "            learning_rate=lr_sched, weight_decay=1e-4, clipnorm=1.0\n",
    "        )\n",
    "    except Exception:\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=lr_sched, clipnorm=1.0)\n",
    "\n",
    "# ModÃ¨le\n",
    "# TextVectorization a bien tokenizer.vocabulary_size()\n",
    "vocab_size = tokenizer.vocabulary_size() if hasattr(tokenizer, \"vocabulary_size\") else \\\n",
    "             (getattr(tokenizer, \"num_words\", None) or getattr(tokenizer, \"vocab_size\", None) or 20000)\n",
    "\n",
    "model = Seq2Seq(\n",
    "    vocab_size=vocab_size, d=256, h=8, ff=768,\n",
    "    max_len=96, L=4, drop=0.1\n",
    ")\n",
    "\n",
    "# XLA: sur Apple/Metal, Ã©vite XLA GPU. On lâ€™active seulement sâ€™il nâ€™y a PAS de GPU logique.\n",
    "use_xla_cpu_only = bool(CONF.get(\"use_xla\")) and (len(tf.config.list_logical_devices('GPU')) == 0)\n",
    "\n",
    "\n",
    "class SparseCEFromLogitsFP32(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        return tf.cast(self.loss_fn(y_true, tf.cast(y_pred, tf.float32)), tf.float32)\n",
    "\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss=SparseCEFromLogitsFP32(),                   # stable avec mixed precision\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"tok_acc\")],\n",
    "    jit_compile=use_xla_cpu_only\n",
    ")\n",
    "\n",
    "# GÃ©nÃ©ration pour les callbacks\n",
    "generate_response = build_generation(tokenizer, model)\n",
    "\n",
    "# Petit set d'Ã©val texte (256 QA alÃ©atoires mais fixes)\n",
    "random.seed(SEED)\n",
    "eval_pairs = random.sample(squad_pairs, k=min(256, len(squad_pairs)))\n",
    "\n",
    "cbs, log_dir = build_callbacks_optim(\n",
    "    \"pretrain_squad\", generate_response, eval_pairs, max_epochs, squad_steps\n",
    ")\n",
    "\n",
    "model.build(input_shape={\n",
    "    \"encoder_input\": (None, None),   # (batch, seq_len)\n",
    "    \"decoder_input\": (None, None)\n",
    "})\n",
    "\n",
    "print(\"ðŸš€ PrÃ©-entraÃ®nement sur SQuAD (optimisÃ© M3 â€” GPU si dispo)â€¦\")\n",
    "\n",
    "history = model.fit(\n",
    "    squad_train,\n",
    "    validation_data=squad_val,\n",
    "    epochs=max_epochs,\n",
    "    steps_per_epoch=int(squad_steps),\n",
    "    validation_steps=int(squad_val_steps),\n",
    "    callbacks=cbs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaffb72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    # model.fit(\n",
    "    #     squad_train,\n",
    "    #     validation_data=squad_val,\n",
    "    #     epochs=5,\n",
    "    #     steps_per_epoch=squad_steps,\n",
    "    #     validation_steps=squad_val_steps,\n",
    "    #     callbacks=cbs_pre,\n",
    "    #     verbose=1\n",
    "    # )\n",
    "\n",
    "    shirayuki_pairs = load_shirayuki_pairs(\"/content/conversation_dataset_ShirayukiV3.csv\")   # <-- uploader le fichier dans Colab\n",
    "    _, sh_train, sh_val, sh_steps, sh_val_steps = prepare_datasets(\n",
    "        shirayuki_pairs, tokenizer=tokenizer, max_len=96, batch_size=64, val_ratio=0.05\n",
    "    )\n",
    "    cbs_ft = build_callbacks(\"finetune_shirayuki\")\n",
    "    def _on_epoch_end_ft(epoch, logs=None):\n",
    "        print(\"\\nðŸ§ª FT Samples:\")\n",
    "        for p in [\"Hello Shirayuki\", \"Peux-tu m'aider Ã  planifier ma journÃ©e ?\"]:\n",
    "            print(\" >\", p)\n",
    "            print(\" >\", generate_response(p, temperature=0.8, top_k=40))\n",
    "    cbs_ft.append(Kcb.LambdaCallback(on_epoch_end=_on_epoch_end_ft))\n",
    "\n",
    "    print(\"ðŸ”„ Fine-tuning sur Shirayuki...\")\n",
    "    model.fit(\n",
    "        sh_train,\n",
    "        validation_data=sh_val,\n",
    "        epochs=10,\n",
    "        steps_per_epoch=sh_steps,\n",
    "        validation_steps=sh_val_steps,\n",
    "        callbacks=cbs_ft,\n",
    "        verbose=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3106cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# DÃ©mo rapide post-entraÃ®nement\n",
    "# =========================\n",
    "tests = [\n",
    "    \"Hello Shirayuki\",\n",
    "    \"How are you today?\",\n",
    "    \"What's your favorite music?\",\n",
    "    \"Peux-tu m'aider Ã  planifier ma journÃ©e ?\"\n",
    "]\n",
    "for t in tests:\n",
    "    print(\"\\n> ðŸ’¬\", t)\n",
    "    print(\"> ðŸ¤–\", generate_response(t, temperature=0.8, top_k=40))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}